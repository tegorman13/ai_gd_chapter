---
title: "AI and Group Decision Making: An Information Processing Perspective"
author:
- name: Thomas E. Gorman
  affiliations: 
  - name: Communication and Cognition Lab, Purdue University, USA
    affiliation-url: https://web.ics.purdue.edu/~treimer/
  - name: College of Liberal Arts Research Academy
    affiliation-url: https://cla.purdue.edu/about/college-initiatives/research-academy/
  url: https://tegorman13.github.io/
  email: tegorman@purdue.edu
  orcid: "0000-0001-5366-5442"
- name: Torsten Reimer
  affiliation: Communication and Cognition Lab, Purdue University, USA
  affiliation-url: https://cla.purdue.edu/communication/
  url: https://web.ics.purdue.edu/~treimer/
  email: treimer@purdue.edu
  orcid: 0000-0002-7419-0076
format:
  html: 
    date: today
    toc: true
    lightbox: true
  gfm: default
  hikmah-manuscript-docx: default
  hikmah-manuscript-pdf:
    echo: false
    output-file: "gd_chapter_v1.2.pdf"
    mainfont: "EB Garamond"
    mainfontoptions:
        - "Numbers=Proportional"
        - "Numbers=OldStyle"
    mathfont: "Libertinus Math"
    include-in-header:
      text: |
          \newcommand*\NewPage{\newpage\null\thispagestyle{empty}\newpage}
          \usepackage{mathtools}
          \everydisplay\expandafter{\the\everydisplay\small}
          \usepackage{float}
          \floatplacement{figure}{H}
          \floatplacement{table}{H}
          \usepackage{setspace}
          \pagenumbering{arabic}
          \usepackage{titlesec}
#   pdf: 
#     documentclass: article
#     papersize: letter
#     toc: false
#     fontsize: 11pt
#     linestretch: 1.5
  docx:
    prefer-html: true
    output-file: "gd_ai_chapter_v1.2.docx"
    toc: false
---





## Introduction


Artificial Intelligence (AI) is becoming a central component of group decision-making processes across a range of domains. From healthcare to finance, education to policymaking, AI systems are being integrated into group decision-making processes, offering new avenues for enhancing efficiency, accuracy, and innovation [@burtonHowLargeLanguage2024; @carterIntegratingArtificialIntelligence2024;@banihaniAIDecisionmakingProcess2024]. This growing collaboration between humans and AI brings forth both significant opportunities and pressing challenges. On one hand, AI systems offer the potential to enhance information processing efficiency, improve decision accuracy, and streamline communication within teams. On the other hand, the complexities inherent in human-AI interactions—such as issues of trust and over-reliance, susceptibility to cognitive biases, erosion of critical thinking skills, lack of transparency in AI algorithms, and ethical concerns regarding accountability and fairness.
  

The use of AI in group settings has evolved from basic decision-support tools to more sophisticated roles, such as collaborative partners capable of generating novel insights. Large language models (LLMs), for instance, can facilitate collective intelligence by synthesizing information, generating alternative solutions, and even mediating group discussions​​. However, the extent to which AI enhances group performance remains context-dependent. Recent meta-analyses reveal that human-AI collaboration can lead to either augmentation of individual performance or to performance decrements [@vaccaroWhenCombinationsHumans2024], depending on the task and interaction design​​.

To navigate these complexities, this chapter adopts the information processing framework as a lens for examining AI-assisted group decision-making [@hinszEmergingConceptualizationGroups1997]. This framework provides a structured method to analyze how AI systems interact with human cognitive processes at each stage of decision-making. By dissecting the inputs (information acquisition and sharing), the processing mechanisms (interpretation and integration of information), and the outputs (decisions and actions), we can gain insights into the opportunities and challenges presented by AI integration.

Key questions we will seek to address within this framework::

- **Inputs**: How does AI influence the way groups search for, gather, and share information? For example, AI can augment information search through advanced data retrieval but may also introduce biases based on the algorithms' training data.

- **Processing**: In what ways do AI systems affect the interpretation and integration of information within the group? AI can facilitate complex data analysis but might obscure the reasoning process through opaque algorithms, impacting the group's shared understanding.

- **Outputs**: How do AI recommendations influence the group's final decisions and actions? The reliance on AI outputs raises questions about trust, accountability, and the potential diminishment of human agency.

This analysis is particularly timely given the rapid advancement of AI capabilities....

## Inputs

..

### Group Member Roles


Deciding how best to assign team members to roles is crucial in group decision-making, particularly when learning who is best suited for what role within a team. @marjiehTaskAllocationTeams2024 explore how humans allocate tasks within teams comprising both human and AI agents to maximize overall performance. The central theme of their research is understanding the mechanisms by which individuals discern and act upon their own strengths and those of their team members in a dynamic task allocation setting. In their experimental paradigm, participants had to repeatedly allocate three different types of tasks (visual, auditory, and lexical tasks) between themselves and two AI agents. Unbeknownst to participants, each AI agent was configured to have high competence (70% success rate) in one task type but low competence (15% success rate) in others. 

Building upon this, @mcneeseSteppingOutShadow2023 argue that human-autonomy teams (HATs) should be recognized as distinct from traditional human teams. They emphasize that HATs should not strive to replicate human-human team dynamics but instead should leverage the unique capabilities of AI agents. The authors propose several research trajectories to advance our understanding of HATs, including exploring diverse teaming models, redefining roles for AI teammates, expanding communication modalities, focusing on AI behavior design, developing specialized training, and emphasizing teamwork in AI design. These insights highlight the necessity of adjusting our approaches to team composition and role assignment when AI agents are involved, ensuring that both human and AI strengths are optimized in the decision-making process.





---

Recent advances in large language models have dramatically expanded the potential roles of AI in group decision-making, enabling AI agents to move beyond simple advisory functions to serve as mediators, devil's advocates, and active discussion participants

@chiangEnhancingAIAssistedGroup2024 investigated the potential of Large Language Models (LLMs) to act as devil's advocates in AI-assisted group decision-making - in the hopes of fostering more critical engagement with AI assistance. In their experimental task, participants were first individually trained on the relationship between defendant profiles and recidivism. For each defendant, participants were also shown the prediction of a recommendation AI model (RiskComp). Participants were then sorted into groups of three, where they reviewed and discussed novel defendant profiles, before making a group recidivism assessment. In the group stage, the recommendations from the RiskComp model were biased against a subset of the defendants (black defendants with low prior crime counts). Of interest was whether the inclusion of an LLM-based devil's advocate in the group discussions could help mitigate the bias introduced by the RiskComp AI model (note that the LLM devils advocate and RiskComp AI are separate AI models). The experimental manipulation consisted of four variants of an LLM-based devil's advocate using, varying both the target of objection (challenging either RiskComp recommendations or majority group opinions) and the level of interactivity (static one-time comments versus dynamic engagement throughout the discussions). Their findings revealed that the dynamic devil's advocate led to higher decision accuracy and improved discernment of when to trust the RiskComp model's advice.

- [@kumarAssessingImpactDiffering2024]
- [@luMixMatchCharacterizing2024]
- [@mcneeseSteppingOutShadow2023]

## Information Processing


### Information Search

The information search stage of decision-making, once reliant on human capacity to locate and synthesize data, has been transformed by the advent of artificial intelligence (AI), particularly Large Language Models (LLMs). This section explores how AI reshapes information search, augmenting both data retrieval and synthesis, and fostering idea generation and creative discovery.

#### AI-Assisted Data Retrieval and Synthesis

LLMs significantly enhance the efficiency and comprehensiveness of information gathering, enabling access to a broader knowledge base and deeper insights [@bouscheryAugmentingHumanInnovation2023]. These models process vast datasets, identifying connections and patterns beyond human capacity. Furthermore, individual differences, such as computational thinking skills, influence how users interact with LLMs, with those possessing higher creativity and algorithmic thinking more effectively leveraging AI-generated content for deeper engagement within a specific information landscape [@floresInformationForagingHumanChatGPT2024]. Programmers, for example, navigate between traditional web search and generative AI tools, strategically selecting between them based on factors like task familiarity and goal clarity, demonstrating the synergistic use of both resources [@yenSearchGenExploring2024]. DiscipLink, for instance, uses LLMs to generate exploratory questions across disciplines, automatically expand queries with field-specific terminology, and extract themes from retrieved papers, effectively bridging knowledge gaps in interdisciplinary research [@zhengDiscipLinkUnfoldingInterdisciplinary2024]. Moreover, AI facilitates advanced techniques like retrieval-augmented generation (RAG), allowing LLMs to access and process real-time information, enhancing the accuracy and relevance of their output [@siCanLLMsGenerate2024; @wangSocialRAGRetrievingGroup2024]. This capability empowers decision-makers with synthesized insights from diverse sources, crucial for informed choices across various fields, from scientific research to policy analysis [@burtonHowLargeLanguage2024].

LLM-based search tools offer natural language interfaces, streamlining complex queries and providing detailed responses, often leading to increased efficiency and user satisfaction [@spathariotiComparingTraditionalLLMbased2023]. However, this ease of use can also lead to overreliance on potentially inaccurate information and decreased critical evaluation, particularly when presented conversationally [@anderlConversationalPresentationMode2024]. This can contribute to confirmation bias and the formation of "generative echo chambers," limiting exposure to diverse perspectives [@sharmaGenerativeEchoChamber2024]. Furthermore, while LLMs can reduce cognitive load during information seeking, this may come at the cost of deeper learning and engagement with the material, leading to less sophisticated reasoning and argumentation [@stadlerCognitiveEaseCost2024]. Therefore, careful design and implementation are crucial to mitigate these risks and leverage the full potential of LLMs for enhanced information retrieval and synthesis.

#### AI in Idea Generation and Creative Discovery

AI’s role extends beyond data retrieval to fostering creative discovery. LLMs act as catalysts, offering alternative perspectives, challenging assumptions, and proposing unexpected connections [@bouscheryAugmentingHumanInnovation2023]. In structured tasks like semantic search, AI agents enhance group performance by selectively sharing information, amplifying collective intelligence [@ueshimaDiscoveringNovelSocial2024]. Studies comparing human and AI-generated ideas reveal a nuanced picture: while LLMs excel at generating ideas with higher average quality (e.g., purchase intent) and even surpassing human experts in novelty [@joostenComparingIdeationQuality2024; @meinckeUsingLargeLanguage2024; @siCanLLMsGenerate2024], they may exhibit lower feasibility [@joostenComparingIdeationQuality2024] and reduced diversity [@meinckeUsingLargeLanguage2024]. This highlights the importance of strategic prompt engineering, as demonstrated by @boussiouxCrowdlessFutureGenerative2024, who found that human-guided prompts—specifically differentiated search (i.e., prompts designed to encourage diverse and varied responses) — enhanced the novelty of LLM-generated solutions while maintaining high value.

The type of AI interaction also significantly influences human creativity. @ashkinazeHowAIIdeas2024 found that exposure to AI-generated ideas increased the diversity of collective ideas without affecting individual creativity. In contrast, @kumarHumanCreativityAge2024 observed that while providing direct answers had minimal negative impact, exposure to LLM-generated strategies decreased both originality and creative flexibility in subsequent unassisted tasks. 




### Communication; information sharing

Transactive memory systems (TMS) represent a critical aspect of group cognition, referring to the shared understanding within a group regarding the distribution of knowledge and expertise among its members [@wegnerTransactiveMemoryContemporary1987; @yanCommunicationTransactiveMemory2021]. A well-functioning TMS enables team members not only to know who possesses specific knowledge but also to access and share this distributed expertise efficiently.

@bienefeldHumanAITeamingLeveraging2023 conducted an observational study to examine the role of transactive memory systems and speaking-up behaviors in human-AI teams within an intensive care unit (ICU) setting. In this study, ICU physicians and nurses, divided into groups of four, who collaborated with an AI agent named "Autovent." Autovent is an auto-adaptive ventilator system that autonomously manages patient ventilation by processing continuous, individualized data streams. Participants, all with a minimum of six months' experience using Autovent, engaged in simulated clinical scenarios that required diagnosing and treating critically ill patients. Using behavioral coding of video recordings, the researchers analyzed how team members accessed information from both human teammates and the AI system, investigating how these human-human and human-ai interactions related to subsequent behaviors like hypothesis generation and speaking up with concerns. The researchers found that in higher-performing teams, accessing knowledge from the AI agent was positively correlated with developing new hypotheses and increased speaking-up behavior. Conversely, accessing information from human team members was negatively associated with these behaviors, regardless of team performance. These results suggest that AI systems may serve as unique knowledge repositories that help teams overcome some of the social barriers that typically inhibit information sharing and voice behaviors in purely human teams.


@bastolaLLMbasedSmartReply2024 further explored the potential of AI-mediated communication by examining how an LLM-based Smart Reply (LSR) system could impact collaborative performance in professional settings. They developed a system utilizing ChatGPT to generate context-aware, personalized responses during workplace interactions, aiming to reduce the cognitive effort required for message composition in multitasking scenarios. In their study, participants engaged in a cognitively demanding Dual N-back task while managing scheduling activities via Google Calendar and responding to simulated co-workers on Slack. The findings indicated that the use of the LSR system not only improved work performance—evidenced by higher accuracy in the N-back task—but also increased messaging efficiency and reduced cognitive load, as participants could more readily focus on primary tasks without the distraction of composing responses. However, it is important to note that participants expressed concerns about the appropriateness and accuracy of AI-generated messages, as well as issues related to trust and privacy. Thus, while AI-mediated communication tools like the LSR system may facilitate information sharing and alleviate cognitive demands in collaborative work, these benefits must be balanced against potential user experience challenges to fully realize their potential advantages.


- [@yangTalk2CareLLMbasedVoice2024]
- [@maHumanAIDeliberationDesign2024]
- [@radivojevicLLMsUsGenerative2024]
- [@sidjiHumanAICollaborationCooperative2024]
- [@nishidaConversationalAgentDynamics2024]
- [@chuangWisdomPartisanCrowds2024]

### Shared Mental Models

- [@collinsBuildingMachinesThat2024a]

...


### Cognitive Load

@bucincaTrustThinkCognitive2021 examined how interface design might influence cognitive engagement with AI recommendations through what they term "cognitive forcing functions." Drawing on dual-process theory, they implemented three distinct interface interventions (e.g., requiring explicit requests for AI input, mandating initial independent decisions, introducing temporal delays) designed to disrupt automatic processing and promote more analytical engagement with AI suggestions. Their findings demonstrated that while these interventions successfully reduced overreliance on incorrect AI recommendations, they also increased perceived cognitive load and decreased user satisfaction. Of particular methodological interest was their systematic investigation of individual differences in cognitive motivation: participants with high Need for Cognition (NFC) showed substantially greater benefits from these interventions, suggesting that the effectiveness of such cognitive load manipulations may be moderated by individual differences in information processing preferences.


## Decision-Making Output

..


### Consensus Formation

@tesslerAICanHelp2024 investigated the potential of AI in facilitating consensus formation through their development of the "Habermas Machine" (HM), an LLM-based system fine-tuned to mediate human deliberation. The HM system receives input statements from individual participants, and attempts to generate consensus statements which will maximize group endorsement. The findings revealed that the AI-generated group statements were consistently preferred over comparison statements written by human mediators. Participants rated the AI-mediated statements higher in terms of informativeness, clarity, and lack of bias. This suggests that AI can effectively capture the collective sentiment of a group and articulate it in a way that resonates with its members. Notably, the researchers also verified that the HM system reliably incorporated minority opinions into the consensus statements, preventing dominance by majority perspectives. These results were replicated in a virtual citizens' assembly with a demographically representative sample of the UK population. The AI-mediated process again resulted in high-quality group statements and facilitated consensus among participants on contentious issues.


### Decision Accuracy and Complementarity

The integration of AI into group decision-making holds significant promise for leveraging the complementary strengths of humans and machines. @rastogiTaxonomyHumanML2023 proposed a taxonomy to characterize differences in human and machine decision-making, providing a framework for understanding how to combine their unique capabilities optimally. This taxonomy highlights areas where AI can augment human decision processes, such as handling large data sets or identifying patterns beyond human perceptual abilities. @beckerBoostingHumanDecisionmaking2022 demonstrated that AI-generated decision aids, when presented as interpretable procedural instructions, can significantly improve human decision-making by promoting more resource-rational strategies. In complex domains, @shinSuperhumanArtificialIntelligence2023 showed that exposure to superhuman AI, as in the game of Go, can enhance human decision-making by encouraging the exploration of novel strategies, thereby increasing overall performance and innovation. However, the effectiveness of human-AI collaboration depends on the dynamics of the interaction.

Recent research has revealed complex trade-offs in human-AI team performance that depend heavily on task structure and collaboration dynamics. @bennettHumanPerformanceCompetitive2023 found that while both human-human and human-AI teams experienced performance costs relative to theoretical benchmarks, human-human teams showed particular advantages in collaborative versus competitive conditions—an effect that diminished when humans worked with AI partners. This aligns with findings from @liangAdaptingAlgorithmHow2022 showing that humans can learn to selectively rely on AI assistance based on task difficulty, but often require explicit feedback and training to optimize this collaboration. The reduced collaborative advantage in human-AI teams appears to stem from difficulties in developing shared mental models and coordinating actions effectively, suggesting that current AI systems may lack crucial capabilities for fluid team interaction.


## Trust, Risk and Reliance 

### Trust in AI

Trust and confidence are crucial determinants of how individuals and groups interact with AI systems during decision-making processes. Research indicates that individuals’ confidence levels significantly influence their propensity to seek and utilize AI-generated advice. @pescetelliRoleDecisionConfidence2021  found that people are more likely to seek advice when their confidence in their own decisions is low; however, they often deviate from optimal Bayesian integration when incorporating this advice, sometimes relying on heuristic strategies instead. @carlebachFlexibleUseConfidence2023 further demonstrated that the relationship between confidence and advice-seeking is context-dependent. When the reliability of an AI advisor is unknown, individuals may paradoxically seek advice even when they are confident, using their own confidence as a feedback mechanism to learn about the advisor’s quality. @liangAdaptingAlgorithmHow2022 explored how explicit performance comparisons between humans and AI affect reliance on decision aids, revealing that individuals adapt their use of AI recommendations based on task difficulty and perceived accuracy differences. @steyversCalibrationGapModel2024 demonstrated that humans tend to overestimate AI system accuracy when presented with default explanations, particularly when those explanations are lengthy. However, this miscalibration can be mitigated by explicitly communicating the AI's uncertainty levels. 

In group settings, trust in AI systems significantly influences how teams interact and make decisions. @cuiAIenhancedCollectiveIntelligence2024 emphasize that trust between humans and AI is crucial for achieving collective intelligence in teams. They argue that factors such as the perceived competence, benevolence, and integrity of AI systems shape this trust, mirroring the dynamics of trust in human relationships. The level of anthropomorphism in AI agents can also affect trust; while human-like features may initially enhance trust, this effect can wane if the AI’s performance does not meet team expectations. @zvelebilovaCollectiveAttentionHumanAI2024 further demonstrate that even when teams do not fully trust an AI assistant or consider it a genuine team member, the AI can still significantly influence team discourse and collective attention. Their study found that teams adopted terminology introduced by the AI, indicating an automatic integration of AI input despite doubts about its reliability. This suggests that AI systems can impact group cognition and coordination regardless of explicit trust levels. These findings highlight the complexity of trust in AI within team environments, where both the design of AI agents and their subtle influences on team dynamics must be carefully managed to enhance collaborative outcomes.



### Utilization

Recent work by @bucincaTrustThinkCognitive2021 presents an innovative approach to addressing overreliance on AI systems through interface design rather than explanation quality. Their study evaluated three "cognitive forcing functions" - interface elements designed to disrupt quick, heuristic processing of AI recommendations. Although these interventions significantly reduced overreliance on incorrect AI recommendations, an important trade-off emerged: interfaces that most effectively prevented overreliance were also rated as most complex and least preferred by users. Moreover, their analysis revealed potential equity concerns, as the interventions provided substantially greater benefits to individuals with high Need for Cognition. These findings suggest that while interface design can effectively modulate AI utilization patterns, careful consideration must be given to both user experience and potential intervention-generated inequalities.

### Responsibility

- [@narayananHowDoesValue2023]

Recent work has begun examining how people attribute responsibility in human-AI collaborative contexts where control is shared and actions are interdependent [@tsirtsisComputationalModelResponsibility2024]. Their study employs a stylized semi-autonomous driving simulation where participants observe how a 'human agent' and an 'AI agent' collaborate to reach a destination within a time limit. In their setup, the human and AI agents shared control of a vehicle, with each agent having partial and differing knowledge of the environment (i.e., the AI knew about traffic conditions but not road closures, while humans knew about closures but not traffic). Participants observe illustrated simulations of a variety of commute scenarios, and then make judgements about how responsible each agent was for the commute outcome (reaching the destination on time, or not). The study reveals that participants' responsibility judgments are influenced by factors such as the unexpectedness of an agent's action, counterfactual simulations of alternative actions, and the actual contribution of each agent to the task outcome.  






### Risk
- [@bhatiaExploringVariabilityRisk2024]
- [@zhuLanguageModelsTrained2024]







## References
::: {#refs}
:::

