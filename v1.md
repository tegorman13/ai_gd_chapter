---
title: "AI and Group Decision Making: An Information Processing Perspective"
author:
- name: Thomas E. Gorman
  affiliations: 
  - name:  Purdue University, USA
    affiliation-url: https://web.ics.purdue.edu/~treimer/
  url: https://tegorman13.github.io/
  email: tegorman@purdue.edu
  orcid: "0000-0001-5366-5442"
- name: Torsten Reimer
  affiliation: Brian Lamb School of Communication, Purdue University
  affiliation-url: https://cla.purdue.edu/communication/
  url: https://web.ics.purdue.edu/~treimer/
  email: treimer@purdue.edu
  orcid: 0000-0002-7419-0076
format:
  html: 
    date: today
    toc: true
    lightbox: true
  gfm: default
  hikmah-manuscript-pdf:
    echo: false
    output-file: "gd_chapter_v1.2.pdf"
    mainfont: "Linux Libertine O"
    mainfontoptions:
        - "Numbers=Proportional"
        - "Numbers=OldStyle"
    mathfont: "Libertinus Math"
#   pdf: 
#     documentclass: article
#     papersize: letter
#     toc: false
#     fontsize: 11pt
#     linestretch: 1.5
  docx:
    prefer-html: true
    output-file: "gd_ai_chapter_v1.2.docx"
    toc: false
---





## Introduction


- [@steyversThreeChallengesAIAssisted2024]
- [@laiScienceHumanAIDecision2023]
- [@burtonHowLargeLanguage2024]
- [@rastogiTaxonomyHumanML2023]

....

## Inputs

..

### Group Member Roles


Deciding how best to assign team members to roles is crucial in group decision-making, particularly when learning who is best suited for what role within a team. @marjiehTaskAllocationTeams2024 explore how humans allocate tasks within teams comprising both human and AI agents to maximize overall performance. The central theme of their research is understanding the mechanisms by which individuals discern and act upon their own strengths and those of their team members in a dynamic task allocation setting. In their experimental paradigm, participants had to repeatedly allocate three different types of tasks (visual, auditory, and lexical tasks) between themselves and two AI agents. Unbeknownst to participants, each AI agent was configured to have high competence (70% success rate) in one task type but low competence (15% success rate) in others.





---

Recent advances in large language models have dramatically expanded the potential roles of AI in group decision-making, enabling AI agents to move beyond simple advisory functions to serve as mediators, devil's advocates, and active discussion participants

@chiangEnhancingAIAssistedGroup2024 investigated the potential of Large Language Models (LLMs) to act as devil's advocates in AI-assisted group decision-making - in the hopes of fostering more critical engagement with AI assistance. In their experimental task, participants were first individually trained on the relationship between defendant profiles and recidivism. For each defendant, participants were also shown the prediction of a reccomendation AI model (RiskComp). Participants were then sorted into groups of three, where they reviewed and discussed novel defendant profiles, before making a group recidivism assessment. In the group stage, the reccomendations from the RiskComp model were biased against a subset of the defendants (black defendants with low prior crime counts). Of interest was whether the inclusion of an LLM-based devil's advocate in the group discussions could help mitigate the bias introduced by the RiskComp AI model (note that the LLM devils advocate and RiskComp AI are separate AI models). The experimental manipulation consisted of four variants of an LLM-based devil's advocate using, varying both the target of objection (challenging either RiskComp recommendations or majority group opinions) and the level of interactivity (static one-time comments versus dynamic engagement throughout the discussions). Their findings revealed that the dynamic devil's advocate led to higher decision accuracy and improved discernment of when to trust the RiskComp model's advice.

- [@marjiehTaskAllocationTeams2024]
- [@kumarAssessingImpactDiffering2024]
- [@luMixMatchCharacterizing2024]
- [@mcneeseSteppingOutShadow2023]

## Information Processing

### Information Search

- [@gaoMemorySharingLarge2024]

...

### Communication; information sharing

Transactive memory systems (TMS) represent a critical aspect of group cognition, referring to the shared understanding within a group regarding the distribution of knowledge and expertise among its members [@wegnerTransactiveMemoryContemporary1987]. A well-functioning TMS enables team members not only to know who possesses specific knowledge but also to access and share this distributed expertise efficiently.

@bienefeldHumanAITeamingLeveraging2023 conducted an observational study to examine the role of transactive memory systems and speaking-up behaviors in human-AI teams within an intensive care unit (ICU) setting. In this study, ICU physicians and nurses, divded into groups of four, who collaborated with an AI agent named "Autovent." Autovent is an auto-adaptive ventilator system that autonomously manages patient ventilation by processing continuous, individualized data streams. Participants, all with a minimum of six months' experience using Autovent, engaged in simulated clinical scenarios that required diagnosing and treating critically ill patients. Using behavioral coding of video recordings, the researchers analyzed how team members accessed information from both human teammates and the AI system, investigating how these human-human and human-ai interactions related to subsequent behaviors like hypothesis generation and speaking up with concerns. The researchers found that in higher-performing teams, accessing knowledge from the AI agent was positively correlated with developing new hypotheses and increased speaking-up behavior. Conversely, accessing information from human team members was negatively associated with these behaviors, regardless of team performance. These results suggest that AI systems may serve as unique knowledge repositories that help teams overcome some of the social barriers that typically inhibit information sharing and voice behaviors in purely human teams.


- [@yangTalk2CareLLMbasedVoice2024]
- [@maHumanAIDeliberationDesign2024]
- [@radivojevicLLMsUsGenerative2024]
- [@sidjiHumanAICollaborationCooperative2024]
- [@nishidaConversationalAgentDynamics2024]
- [@chuangWisdomPartisanCrowds2024]

### Shared Mental Models

- [@collinsBuildingMachinesThat2024a]

...


### Cognitive Load

@bucincaTrustThinkCognitive2021 examined how interface design might influence cognitive engagement with AI recommendations through what they term "cognitive forcing functions." Drawing on dual-process theory, they implemented three distinct interface interventions (e.g., requiring explicit requests for AI input, mandating initial independent decisions, introducing temporal delays) designed to disrupt automatic processing and promote more analytical engagement with AI suggestions. Their findings demonstrated that while these interventions successfully reduced overreliance on incorrect AI recommendations, they also increased perceived cognitive load and decreased user satisfaction. Of particular methodological interest was their systematic investigation of individual differences in cognitive motivation: participants with high Need for Cognition (NFC) showed substantially greater benefits from these interventions, suggesting that the effectiveness of such cognitive load manipulations may be moderated by individual differences in information processing preferences.





## Decision-Making Output

..


### Consensus Formation

@tesslerAICanHelp2024 investigated the potential of AI in facilitating consensus formation through their development of the "Habermas Machine" (HM), an LLM-based system fine-tuned to mediate human deliberation. The HM system receives input statements from individual participants, and attempts to generate consensus statements which will maximize group endorsement. The findings revealed that the AI-generated group statements were consistently preferred over comparison statements written by human mediators. Participants rated the AI-mediated statements higher in terms of informativeness, clarity, and lack of bias. This suggests that AI can effectively capture the collective sentiment of a group and articulate it in a way that resonates with its members. Notably, the researchers also verified that the HM system reliably incorporated minority opinions into the consensus statements, preventing dominance by majority perspectives. These results were replicated in a virtual citizens' assembly with a demographically representative sample of the UK population. The AI-mediated process again resulted in high-quality group statements and facilitated consensus among participants on contentious issues.


### Decision Accuracy and Confidence

- [@beckerBoostingHumanDecisionmaking2022]
- 
...


## Trust, Risk and Reliance 

### Trust in AI

- @westphalDecisionControlExplanations2023
- [@koehlMeasuringLatentTrust2023]
- [@banerjeeLearningGuideHuman2024]

### Reliance

- [@narayananHowDoesValue2023]


Recent work has begun examining how people attribute responsibility in human-AI collaborative contexts where control is shared and actions are interdependent [@tsirtsisComputationalModelResponsibility2024]. Their study employs a stylized semi-autonomous driving simulation where participants observe how a 'human agent' and an 'AI agent' collaborate to reach a destination within a time limit. In their setup, the human and AI agents shared control of a vehicle, with each agent having partial and differing knowledge of the environment (i.e., the AI knew about traffic conditions but not road closures, while humans knew about closures but not traffic). Participants observe illustrated simulations of a variety of commute scenarios, and then make judgements about how responsible each agent was for the commute outcome (reaching the destination on time, or not). The study reveals that participants' responsibility judgments are influenced by factors such as the unexpectedness of an agent's action, counterfactual simulations of alternative actions, and the actual contribution of each agent to the task outcome.  


### Utilization

Recent work by @bucincaTrustThinkCognitive2021 presents an innovative approach to addressing overreliance on AI systems through interface design rather than explanation quality. Their study evaluated three "cognitive forcing functions" - interface elements designed to disrupt quick, heuristic processing of AI recommendations. Although these interventions significantly reduced overreliance on incorrect AI recommendations, an important trade-off emerged: interfaces that most effectively prevented overreliance were also rated as most complex and least preferred by users. Moreover, their analysis revealed potential equity concerns, as the interventions provided substantially greater benefits to individuals with high Need for Cognition. These findings suggest that while interface design can effectively modulate AI utilization patterns, careful consideration must be given to both user experience and potential intervention-generated inequalities.

- [@cuiAIenhancedCollectiveIntelligence2024]
- [@stadlerCognitiveEaseCost2024]


### Risk
- [@bhatiaExploringVariabilityRisk2024]
- [@zhuLanguageModelsTrained2024]



{{< pagebreak >}}


## Tables 

::: {.column-page-inset-right}

{{< embed tab.qmd >}}

:::



{{< pagebreak >}}




## References
::: {#refs}
:::

