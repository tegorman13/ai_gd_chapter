# AI and Group Decision Making: An Information Processing Perspective


## Introduction



Artificial Intelligence (AI) is becoming a central component of group decision-making processes across a range of domains. From healthcare to finance, education to policymaking, AI systems are being integrated into group decision-making processes, offering new avenues for enhancing efficiency, accuracy, and innovation (BaniHani et al., 2024; Burton et al., 2024; Carter & Wynne, 2024). This growing collaboration between humans and AI brings forth both significant opportunities and pressing challenges. On one hand, AI systems offer the potential to enhance information processing efficiency, improve decision accuracy, and streamline communication within teams. On the other hand, the complexities inherent in human-AI interactions are challenging—such as issues of trust and over-reliance, susceptibility to cognitive biases, erosion of critical thinking skills, lack of transparency in AI algorithms, and ethical concerns regarding accountability and fairness.

The use of AI in group settings has evolved from basic decision-support tools to more sophisticated roles, such as collaborative partners capable of generating novel insights. Large language models (LLMs), for instance, can facilitate collective intelligence by synthesizing information, generating alternative solutions, and even mediating group discussions (Reference). However, the extent to which AI enhances group performance remains context-dependent. Recent meta-analyses reveal that human-AI collaboration can lead to either augmentation of individual performance or to performance decrements (Vaccaro et al., 2024), depending on the task and interaction design.

To navigate these complexities, this chapter adopts an information processing framework as a lens for examining human-AI decision making in groups (Hinsz et al., 1997; Reimer et al., 2017; van Swol et al., 2023). This framework provides a structure to systematically describe how AI systems interact with human cognitive processes at each stage of decision making. Information processing can be represented and conceptualized in many different ways. We use the information processing framework displayed in Figure 1 to summarize, integrate and highlight some research findings on AI-human decision making in groups and to outline questions for future research. 
 


Figure 1 illustrates the conceptual model underpinning out review. The input rectangle represents the factors that feed into the group decision-making process at the beginning (human member characterisitics, and AI System attributes).  The AI-Group communication rectangle represents the central process where the humans and AI interact (e.g., in the process of searching and exchanging information, or via natural language communication). The third rectangle represents the outcomes of the group decision (e.g., actual decisions made by the group, along with associated metrics like decision quality, trust). 


By dissecting the inputs (human member characteristics and AI systems attributes), the communication phase (information search and exchange and communicative roles of AI), and the decision mechanisms (human-AI decision strategies and decision outcomes), we can gain insights into the opportunities and challenges presented by AI integration.

Key questions we will seek to address within this framework are as follows:

•	Input: How does AI influence the way groups search for, gather, and share information? For example, AI can augment information search through advanced data retrieval but may also introduce biases based on the algorithms’ training data.
•	AI-Group Communication: In what ways do AI systems affect the interpretation and integration of information within the group? AI can facilitate complex data analysis but might obscure the reasoning process through opaque algorithms, impacting the group’s shared understanding.
•	Group Decision: How do AI recommendations influence the group’s final decisions? The reliance on AI recommendations raises questions about trust, accountability, and the potential diminishment of human agency.
By examining these aspects through the lens of the information processing framework, we can better understand the complex interplay between humans and AI in group decision-making contexts and identify strategies for increasing the benefits of AI-assisted collaboration while mitigating its risks.


Box 1: Glossary of Terms
Complementarity. In the context of human-AI teams, complementarity refers to the synergistic integration of human and artificial intelligence, leveraging the unique strengths of each to achieve performance outcomes that exceed those attainable by either humans or AI systems operating in isolation (Steyvers et al., 2022). Effective complementarity involves a balanced division of labor and mutual enhancement of capabilities.
Information Processing Frameworks. Frameworks in cognitive psychology that conceptualizes the human mind, and by extension, groups, as systems that acquire, process, store, retrieve, and transmit information, akin to computational systems (Hinsz et al., 1997). This framework provides a structured lens for analyzing decision making as a sequence of stages, from inputs to outputs.
Large Language Model (LLM). A sophisticated type of artificial intelligence algorithm characterized by its use of deep learning techniques and training on massive datasets to enable the understanding, generation, and prediction of human language. LLMs, such as GPT-4, are foundational to many contemporary AI applications in decision support and communication.
Transactive Memory Systems (TMS). A system of distributed knowledge within a group, where members develop specialized knowledge and rely on each other for access to that knowledge. Like LLMs, transactive memory systems include members’ assumptions and knowledge about what other members know and do not know.
Over- and under-reliance. Excessive dependence on and trust in AI, often leading to the uncritical acceptance and adoption of AI recommendations, even when they are flawed or suboptimal. Overreliance can undermine human vigilance and critical evaluation in AI-assisted decision-making contexts.



## Input

The input stage is characterized by a multifaceted array of resources and factors, each contributing uniquely to the subsequent processing and ultimate decision making. In human-AI systems, key inputs relate to the characteristics of human group members and characteristics of the AI system. 

### Human Member Characteristics 

The decision-making process can be greatly influenced by the composition of human teams, particularly in terms of AI-related expertise, cognitive styles, and trust in AI, plays a critical role in shaping decision-making processes. Individual differences in familiarity with AI, digital literacy, and prior experience influence both the adoption and effectiveness of AI-assisted decision-making (Diebel et al., 2025; Gerlich, 2025). For instance, users with greater AI knowledge may experience competence-based self-esteem shifts when working with proactive AI, while those with lower AI familiarity may exhibit overreliance or skepticism (Diebel et al., 2025). Additionally, baseline trust in AI is not uniform across individuals and significantly impacts reliance and the overall effectiveness of human-AI collaboration (Roesler et al., 2024). 


Cognitive diversity in AI perceptions also impacts group dynamics. Some individuals engage with AI as a collaborative partner, while others view it as a mere tool, affecting communication patterns and willingness to integrate AI-generated insights (Baines et al., 2024). The interpretability and transparency of AI recommendations further moderate these interactions—users are more likely to trust AI when explanations align with their cognitive models and expertise level (Herzog & Franklin, 2024).

Additionally, cultural and contextual factors shape AI acceptance and perceived efficacy in decision-making teams. Attitudes toward authority, automation, and technology influence the degree to which AI is integrated into group deliberation (Chugunova & Sele, 2022). In interdisciplinary teams, differences in domain expertise and AI literacy can create asymmetries in how AI outputs are evaluated and acted upon (Berretta et al., 2023).



#### Demographics and Individual Differences. 

Demographic factors such as age, gender, and education, as well as individual differences in personality traits, digital affinity, and cultural background, can moderate human-AI interaction (Diebel et al., 2025; Gerlich, 2025; Roesler et al., 2024). For instance, younger individuals or those with higher digital affinity may be more comfortable integrating AI into their decision-making processes. Diebel et al. (2025) found that individuals with higher AI knowledge experienced a greater loss of competence-based self-esteem when interacting with proactive AI, indicating that prior experience with AI can shape user perceptions. Cultural background can also play a role, influencing attitudes towards authority, technology, and collaboration (Chugunova & Sele, 2022).


- Employees with high confidence in their abilities are less likely to seek and follow AI advice (Burton et al. 2020; Logg et al. 2019)


## Human Member Characteristics 

The composition of human teams, in terms of expertise, cognitive styles, and diversity, represents a foundational input layer. For instance, the expertise and knowledge that individual members bring to a team are crucial determinants of the quality of information available for processing (Aggarwal et al., 2023). Furthermore, cognitive diversity, encompassing varied approaches to problem-solving and information processing, can enrich the group’s cognitive resources, potentially enhancing its ability to tackle complex problems (Aggarwal et al., 2023). However, the benefits of diversity are not unqualified, as factors such as team longevity and task complexity can moderate the diversity-performance relationship (Wallrich et al., 2024). In addition to expertise and cognitive styles, the roles assigned to team members, particularly in hierarchical structures, shape how information is accessed, shared, and utilized within the group (Marjieh, Sucholutsky, et al., 2024; S. Narayanan et al., 2023). Moreover, demographic diversity, while having a statistically significant but practically small overall effect on team performance, can interact with contextual factors to influence group dynamics and outcomes (Wallrich et al., 2024). These varied human characteristics collectively form a crucial input layer, setting the stage for how groups interact with and leverage AI in decision-making processes.



#### AI System Attributes


Not only do human group members differ in their knowledge and values, there are also differences in AI systems. The capabilities and functionalities of AI systems, such as their proficiency in natural language processing (e.g., generating coherenat summaries; matching language style to user needs), reasoning, and idea generation, directly determine the types of tasks and roles they can effectively assume within a group (Burton et al., 2024; Tawashy, 2024). For instance, an LLM proficient in text summarization can streamline information integration for a team, while an AI adept at complex calculations might enhance the quantitative aspects of decision-making (Bastola et al., 2024; Banerjee et al., 2024).

**Communication Style and Modality.** The communication style of the AI, whether proactive or reactive, formal or informal, also shapes user experience and team dynamics (Bastola et al., 2024; Argyle et al., 2023). Proactive communication from AI, for example, can enhance team coordination and situation awareness, but might also lead to lower user satisfaction if perceived as intrusive or undermining human competence (Diebel et al., 2025; Zhang et al., 2023). Argyle et al. (2023) demonstrated that AI's ability to generate nuanced, context-aware language promotes more respectful and understanding interactions, enhancing communication effectiveness. Further, Bastola et al. (2024) found that perceived effectiveness was influenced by AI’s communication style and that adapting communication styles to different team member personas enhances group dynamics. 

**Transparency and Explainability.** Another critical attribute is the degree to which an AI system's decision-making processes are transparent and explainable (Bao et al. 2023; Beretta et al. 2023). AI systems may range from "black box" models, where the reasoning is obscure, to transparent systems that provide clear explanations for their recommendations. The way uncertainty is communicated in LLM outputs also affects human-AI decision-making. Steyvers, Tejeda, Kumar, et al. (2024) found that miscalibration between LLMs’ internal confidence and human perception of that confidence impacted decision quality.

This is particularly relevant for high-stakes decisions, where black-box models may undermine human trust and reduce the likelihood of AI-assisted decisions being effectively adopted (Herzog & Franklin, 2024)…..


### Information Search and Exchange 

Information search, once reliant on human capacity to locate and synthesize data, has been transformed by the advent of artificial intelligence (AI). This section highlights research that explored how AI reshapes information search, augmenting both data retrieval and synthesis, and fostering idea generation and creative discovery.

LLMs significantly enhance the efficiency and comprehensiveness of information gathering, enabling access to a broader knowledge base and deeper insights (Bouschery et al., 2023). These models process vast datasets, identifying connections and patterns beyond human capacity. Furthermore, individual differences, such as computational thinking skills, influence how users interact with LLMs, with those possessing higher creativity and algorithmic thinking more effectively leveraging AI-generated content for deeper engagement within a specific information landscape (Flores et al., 2024). Programmers, for example, navigate between traditional web search and generative AI tools, strategically selecting between them based on factors like task familiarity and goal clarity, demonstrating the synergistic use of both resources (Yen et al., 2024). DiscipLink, for instance, uses LLMs to generate exploratory questions across disciplines, automatically expand queries with field-specific terminology, and extract themes from retrieved papers, effectively bridging knowledge gaps in interdisciplinary research (Zheng et al., 2024). Moreover, AI facilitates advanced techniques like retrieval-augmented generation (RAG), allowing LLMs to access and process real-time information, enhancing the accuracy and relevance of their output (Si et al., 2024; Wang et al., 2024). This capability empowers decision-makers with synthesized insights from diverse sources, crucial for informed choices across various fields, from scientific research to policy analysis (Burton et al., 2024).

LLM-based search tools offer natural language interfaces, streamlining complex queries and providing detailed responses, often leading to increased efficiency and user satisfaction (Spatharioti et al., 2023). However, this ease of use can also lead to overreliance on potentially inaccurate information and decreased critical evaluation, particularly when presented conversationally (Anderl et al., 2024). This can contribute to confirmation bias and the formation of “generative echo chambers,” limiting exposure to diverse perspectives (Sharma et al., 2024). Furthermore, while LLMs can reduce cognitive load during information seeking, this may come at the cost of deeper learning and engagement with the material, leading to less sophisticated reasoning and argumentation (Stadler et al., 2024). Therefore, careful design and implementation are crucial to mitigate these risks and leverage the full potential of LLMs for enhanced information retrieval and synthesis.

AI systems, such as LLM-based Smart Reply (LSR) systems, can mediate communication and information sharing within human-AI teams (Bastola et al., 2024; Fortunati & Edwards, 2021; Gomez Caballero et al., 2024). The key is to use AI to ensure more efficient information exchange between team members, and also to reduce cognitive load during repetitive tasks. However, AI systems need to be carefully designed to maintain user experience and be respectful of human biases, preferences and agency.
Bastola et al. (2024) further explored the potential of AI-mediated communication by examining how an LLM-based Smart Reply (LSR) system could impact collaborative performance in professional settings. They developed a system utilizing ChatGPT to generate context-aware, personalized responses during workplace interactions, aiming to reduce the cognitive effort required for message composition in multitasking scenarios. In their study, participants engaged in a cognitively demanding Dual N-back task while managing scheduling activities via Google Calendar and responding to simulated co-workers on Slack. The findings indicated that the use of the LSR system not only improved work performance—evidenced by higher accuracy in the N-back task—but also increased messaging efficiency and reduced cognitive load, as participants could more readily focus on primary tasks without the distraction of composing responses. However, it is important to note that participants expressed concerns about the appropriateness and accuracy of AI-generated messages, as well as issues related to trust and privacy. Thus, while AI-mediated communication tools like the LSR system may facilitate information sharing and alleviate cognitive demands in collaborative work, these benefits must be balanced against potential user experience challenges to fully realize their potential advantages.

### Communicative Roles of AI. 

The way in which AI agents communicate, including whether they are proactive, sociable, responsive and clear, can affect the flow of information in the group and the trust between team members (Bennett et al., 2023; Duan et al., 2025; Nishida et al., 2024). Different communication styles may be preferable in different contexts, and the human perception of AI communication strategies play a critical role in shaping human expectations for AI teammates (D. Zhang et al., 2023).
The roles and functionality assigned to AI systems significantly impact group dynamics and decision-making processes (Bennett et al., 2023; Berretta et al., 2023; Carter & Wynne, 2024; Duan et al., 2025; Guo et al., 2024; Nomura et al., 2024). AI can act as an advisor, providing recommendations and insights; a peer collaborator, actively participating in discussions; a devil’s advocate, challenging the group’s assumptions; a mediator, facilitating consensus formation; or even a manager, coordinating tasks and assigning roles. The role of the AI will affect how humans interact with it and how much responsibility is assigned to the AI in the decision-making process.

For instance, an AI acting as an advisor might provide information and suggestions, which group members then evaluate and integrate into their decision-making process. In contrast, an AI acting as a peer collaborator might actively engage in discussions, contributing its own opinions and analyses. A devil’s advocate AI could challenge the group’s consensus, promoting critical evaluation and potentially reducing groupthink. A mediator AI might help to synthesize diverse perspectives and facilitate agreement, as demonstrated by the Habermas Machine (Tessler et al., 2024). The proactivity or reactiveness of the AI also influences its role and impact (Diebel et al., 2025). A proactive AI might initiate suggestions or interventions, while a reactive AI would only respond to user prompts. Proactive AI can enhance efficiency but may also reduce user control and satisfaction, particularly if the AI’s actions are perceived as intrusive or misaligned with user needs.

Recent advances in large language models have dramatically expanded the potential roles of AI in group decision-making, enabling AI agents to move beyond simple advisory functions to serve as mediators, devil’s advocates, and active discussion participants. Chiang et al. (2024) investigated the potential of Large Language Models (LLMs) to act as devil’s advocates in AI-assisted group decision-making - in the hopes of fostering more critical engagement with AI assistance. In their experimental task, participants were first individually trained on the relationship between defendant profiles and recidivism. For each defendant, participants were also shown the prediction of a recommendation AI model (RiskComp). Participants were then sorted into groups of three, where they reviewed and discussed novel defendant profiles, before making a group recidivism assessment. In the group stage, the recommendations from the RiskComp model were biased against a subset of the defendants (black defendants with low prior crime counts). Of interest was whether the inclusion of an LLM-based devil’s advocate in the group discussions could help mitigate the bias introduced by the RiskComp AI model (note that the LLM devils advocate and RiskComp AI are separate AI models). The experimental manipulation consisted of four variants of an LLM-based devil’s advocate using, varying both the target of objection (challenging either RiskComp recommendations or majority group opinions) and the level of interactivity (static one-time comments versus dynamic engagement throughout the discussions). Their findings revealed that the dynamic devil’s advocate led to higher decision accuracy and improved discernment of when to trust the RiskComp model’s advice.
Tessler et al. (2024) investigated the potential of AI in facilitating consensus formation through their development of the “Habermas Machine” (HM), an LLM-based system fine-tuned to mediate human deliberation. The HM system receives input statements from individual participants, and attempts to generate consensus statements which will maximize group endorsement. The findings revealed that the AI-generated group statements were consistently preferred over comparison statements written by human mediators. Participants rated the AI-mediated statements higher in terms of informativeness, clarity, and lack of bias. This suggests that AI can effectively capture the collective sentiment of a group and articulate it in a way that resonates with its members. Notably, the researchers also verified that the HM system reliably incorporated minority opinions into the consensus statements, preventing dominance by majority perspectives. These results were replicated in a virtual citizens’ assembly with a demographically representative sample of the UK population. The AI-mediated process again resulted in high-quality group statements and facilitated consensus among participants on contentious issues.

## Group Decision

Communication affects the decision process. How to integrate opinions/preferences/information.
One of the primary goals of integrating AI into group decision-making is to enhance decision quality, and research has shown promising results in this area. AI augmentation has been demonstrated to improve human decision-making across various tasks (Becker et al., 2022). In complex domains, such as medical diagnostics, human-AI collectives have been shown to produce more accurate differential diagnoses than either human-only or AI-only groups, highlighting the potential for synergistic gains (Zöller et al., 2024). The exposure to superhuman AI, as in the game of Go, can also enhance human decision-making by encouraging the exploration of novel strategies, thereby increasing overall performance and innovation (Shin et al., 2023).

Human-AI Decision Strategies  
Complementarity. Research indicates that human-AI teams, under certain conditions, can achieve superior decision quality compared to either human-only or AI-only groups, demonstrating the potential for human-AI complementarity (Steyvers et al., 2022; Zöller et al., 2024). This complementarity arises from leveraging the distinct strengths of humans and machines: AI systems excel at processing large datasets and identifying patterns, while humans contribute contextual understanding, ethical considerations, and creative problem-solving abilities (Canonico et al., 2019; Carter & Wynne, 2024). For instance, in medical diagnostics, hybrid human-AI collectives have shown greater accuracy in differential diagnoses due to the diverse error profiles of humans and LLMs (Zöller et al., 2024). However, achieving this complementarity requires careful consideration of task allocation and team structure (Marjieh, Sucholutsky, et al., 2024; Q. Zhang et al., 2022). Bayesian modeling approaches have been developed to formally analyze and optimize human-AI complementarity, demonstrating that hybrid combinations can outperform either component alone under specific conditions (Lemus et al., 2022; Steyvers et al., 2022).
The integration of AI into group decision-making holds significant promise for leveraging the complementary strengths of humans and machines. Rastogi et al. (2023) proposed a taxonomy to characterize differences in human and machine decision-making, providing a framework for understanding how to combine their unique capabilities optimally. This taxonomy highlights areas where AI can augment human decision processes, such as handling large data sets or identifying patterns beyond human perceptual abilities. Becker et al. (2022) demonstrated that AI-generated decision aids, when presented as interpretable procedural instructions, can significantly improve human decision-making by promoting more resource-rational strategies. In complex domains, Shin et al. (2023) showed that exposure to superhuman AI, as in the game of Go, can enhance human decision-making by encouraging the exploration of novel strategies, thereby increasing overall performance and innovation. However, the effectiveness of human-AI collaboration depends on the dynamics of the interaction.
Recent research has revealed complex trade-offs in human-AI team performance that depend heavily on task structure and collaboration dynamics. Bennett et al. (2023) found that while both human-human and human-AI teams experienced performance costs relative to theoretical benchmarks, human-human teams showed particular advantages in collaborative versus competitive conditions—an effect that diminished when humans worked with AI partners. This aligns with findings from Liang et al. (2022) showing that humans can learn to selectively rely on AI assistance based on task difficulty, but often require explicit feedback and training to optimize this collaboration. The reduced collaborative advantage in human-AI teams appears to stem from difficulties in developing shared mental models and coordinating actions effectively, suggesting that current AI systems may lack crucial capabilities for fluid team interaction.


## Decision Outcomes  

Several factors influence the quality of decisions in AI-assisted contexts. One critical factor is the accuracy of the AI’s recommendations (Yin et al., 2019). However, accuracy alone is not sufficient; the way AI confidence is communicated also plays a crucial role. Humans do not simply accept or reject AI advice but rather adjust their reliance based on a complex interplay of factors, including perceived AI accuracy, human confidence, and decision context (Steyvers et al., 2022). The alignment of confidence between team members also influences how individuals process and weight the information they have at hand and make decisions based on that information (Li et al., 2025). The alignment of confidence could also impact polarization, as individuals with high self-confidence might reject AI recommendations, while those with lower self-confidence might over-rely, potentially leading to inconsistent reliance and team performance (Li et al., 2025). AI systems optimized for teamwork, rather than just accuracy, can lead to better overall performance (Bansal et al., 2021). This optimization includes factors like explainability and adaptability, which directly relate to the concepts of complementarity and reliance. The confidence level displayed by an AI and the provided explanation impact the trust level of human decision-makers (Y. Zhang et al., 2020). When people do not understand the basis of AI recommendations (or believe the AI to be a black box), they are less likely to trust it and therefore less likely to rely on the AI even when it would improve decision quality (Westphal et al., 2023). In this way, lack of explainability presents a risk to decision quality.
Satisfaction, Acceptance, and Group Dynamics. Beyond objective measures of decision quality, the subjective experiences of group members, including satisfaction, acceptance, and shifts in group dynamics, are crucial outputs in AI-assisted decision-making. User satisfaction with AI systems is significantly influenced by factors such as interface design, perceived cognitive load, and the type of AI assistance provided (Buçinca et al., 2021; Rebholz et al., 2024). For instance, while cognitive forcing functions can improve decision accuracy, they may also decrease user satisfaction due to increased perceived complexity (Buçinca et al., 2021). Acceptance of AI in group settings is contingent on various factors, including the perceived usefulness of AI, trust in its recommendations, and alignment with human values (S. Narayanan et al., 2023). Furthermore, AI integration can reshape group dynamics, potentially influencing group polarization and the emergence of echo chambers or polarization phenomena (Cheung et al., 2024). While AI can facilitate consensus formation and bridge diverse viewpoints (Tessler et al., 2024) it also carries the risk of amplifying existing biases and limiting exposure to diverse perspectives (Cheung et al., 2024; Sharma et al., 2024).
Recent work by Buçinca et al. (2021) presents an innovative approach to addressing overreliance on AI systems through interface design rather than explanation quality. Their study evaluated three “cognitive forcing functions” - interface elements designed to disrupt quick, heuristic processing of AI recommendations. Although these interventions significantly reduced overreliance on incorrect AI recommendations, an important trade-off emerged: interfaces that most effectively prevented overreliance were also rated as most complex and least preferred by users. Moreover, their analysis revealed potential equity concerns, as the interventions provided substantially greater benefits to individuals with high Need for Cognition. These findings suggest that while interface design can effectively modulate AI utilization patterns, careful consideration must be given to both user experience and potential intervention-generated inequalities.


### Transactive Memory Systems/Large Language Models

#### Mutual Trust

Trust and confidence in AI are dynamic and multifaceted, as highlighted by Hancock et al. (2011), who conducted a meta-analysis showing that trust in automation is influenced by a variety of human, robot, and environmental factors. Li et al. (2025) further explored the dynamic nature of trust, revealing that human self-confidence tends to align with AI confidence, which can affect decision-making strategies. Vodrahalli et al. (2022) emphasized the importance of calibrated trust, where humans accurately assess the reliability of AI advice. The calibration of trust is also seen as critical, as shown by Cecil et al. (2024), who found that explanations did not mitigate the negative impacts of incorrect AI advice, suggesting a complex relationship between trust, reliance, and the perceived reliability of AI.
•	Trust Baselines: Pre-existing attitudes toward AI, perceived reliability, anthropomorphism (Cui & Yasseri, 2024)
•	Bias and Training Data: Potential for AI to introduce or amplify biases from training corpora (Bhatia, 2024; Cecil et al., 2024)
Trust and confidence are crucial determinants of how individuals and groups interact with AI systems during decision-making processes. Research indicates that individuals’ confidence levels significantly influence their propensity to seek and utilize AI-generated advice. Pescetelli and Yeung (2021) found that people are more likely to seek advice when their confidence in their own decisions is low; however, they often deviate from optimal Bayesian integration when incorporating this advice, sometimes relying on heuristic strategies instead. Carlebach and Yeung (2023) further demonstrated that the relationship between confidence and advice-seeking is context-dependent. When the reliability of an AI advisor is unknown, individuals may paradoxically seek advice even when they are confident, using their own confidence as a feedback mechanism to learn about the advisor’s quality. Liang et al. (2022) explored how explicit performance comparisons between humans and AI affect reliance on decision aids, revealing that individuals adapt their use of AI recommendations based on task difficulty and perceived accuracy differences. Steyvers et al. (2024) demonstrated that humans tend to overestimate AI system accuracy when presented with default explanations, particularly when those explanations are lengthy. However, this miscalibration can be mitigated by explicitly communicating the AI’s uncertainty levels.
In group settings, trust in AI systems significantly influences how teams interact and make decisions. Cui and Yasseri (2024) emphasize that trust between humans and AI is crucial for achieving collective intelligence in teams. They argue that factors such as the perceived competence, benevolence, and integrity of AI systems shape this trust, mirroring the dynamics of trust in human relationships. The level of anthropomorphism in AI agents can also affect trust; while human-like features may initially enhance trust, this effect can wane if the AI’s performance does not meet team expectations. Zvelebilova et al. (2024) further demonstrate that even when teams do not fully trust an AI assistant or consider it a genuine team member, the AI can still significantly influence team discourse and collective attention. Their study found that teams adopted terminology introduced by the AI, indicating an automatic integration of AI input despite doubts about its reliability. This suggests that AI systems can impact group cognition and coordination regardless of explicit trust levels. These findings highlight the complexity of trust in AI within team environments, where both the design of AI agents and their subtle influences on team dynamics must be carefully managed to enhance collaborative outcomes.
•	Contagion Effects of (Dis)Trust: Spread of distrust or overconfidence within human–AI teams (Duan et al., 2025)
•	Uncalibrated vs. Well-Calibrated AI: Conditions under which uncalibrated AI might paradoxically enhance outcomes (Vodrahalli et al., 2022)

#### Shared Mental Models

AI can facilitate transactive memory systems in human-AI teams, acting as knowledge repositories and influencing information access and sharing (Bienefeld et al., 2023; Gao et al., 2024; Yan et al., 2021). This is not unlike how human transactive memory systems work (Bienefeld et al., 2023), but the dynamics and limitations of human-AI TMSs are yet to be fully understood, especially in cases where AI may not fully understand the full context or task environment.

The formation and maintenance of shared mental models (SMMs) in human-AI teams are significantly impacted by team structure (Collins et al., 2024; R. Narayanan & Feigh, 2024). In hierarchical human-AI teams, for example, SMMs are often distributed across the team, rather than held by all members, which requires more sophisticated approaches to communication. A key to successful human-AI teams is ensuring there is a common understanding of team member roles, goals and limitations, and in particular the role of any AI systems.
Transactive memory systems (TMS) represent a critical aspect of group cognition, referring to the shared understanding within a group regarding the distribution of knowledge and expertise among its members (Wegner, 1987; Yan et al., 2021). A well-functioning TMS enables team members not only to know who possesses specific knowledge but also to access and share this distributed expertise efficiently.
Bienefeld et al. (2023) conducted an observational study to examine the role of transactive memory systems and speaking-up behaviors in human-AI teams within an intensive care unit (ICU) setting. In this study, ICU physicians and nurses, divided into groups of four, who collaborated with an AI agent named “Autovent.” Autovent is an auto-adaptive ventilator system that autonomously manages patient ventilation by processing continuous, individualized data streams. Participants, all with a minimum of six months’ experience using Autovent, engaged in simulated clinical scenarios that required diagnosing and treating critically ill patients. Using behavioral coding of video recordings, the researchers analyzed how team members accessed information from both human teammates and the AI system, investigating how these human-human and human-ai interactions related to subsequent behaviors like hypothesis generation and speaking up with concerns. The researchers found that in higher-performing teams, accessing knowledge from the AI agent was positively correlated with developing new hypotheses and increased speaking-up behavior. Conversely, accessing information from human team members was negatively associated with these behaviors, regardless of team performance. These results suggest that AI systems may serve as unique knowledge repositories that help teams overcome some of the social barriers that typically inhibit information sharing and voice behaviors in purely human teams.

CONTEXT

Task Characteristics 

The nature of the decision-making task itself and the broader context within which it is embedded form another essential layer of inputs. Task complexity, for instance, significantly influences the type of information processing required and the potential benefits of AI assistance (Eigner & Händler, 2024; Hamada et al., 2020). Complex tasks may necessitate more sophisticated AI support to manage information overload and enhance analytical capabilities. The decision environment, encompassing factors such as time pressure and risk levels, also shapes the input requirements and the dynamics of human-AI interaction (R. Zhang et al., 2023). For example, time pressure may alter reliance on AI assistance, as individuals adapt their decision-making strategies to balance speed and accuracy (Swaroop et al., 2024). Moreover, the specific decision-making setting, whether in healthcare, finance, or policy, introduces unique contextual factors that influence the relevance and effectiveness of AI inputs (S. Narayanan et al., 2023). These task and contextual factors, therefore, represent a critical input layer, moderating the interplay between human and AI contributions.

### Cognitive Load

Cognitive load impacts human decision-making and the way in which they engage with AI tools (Buçinca et al., 2021; Gerlich, 2025; Westphal et al., 2023). AI systems may reduce cognitive load, but can also impair critical thinking skills, and must be carefully designed to support users. Cognitive forcing functions (Buçinca et al., 2021), or interactions designed to encourage analytical thinking and deeper processing, can promote more effective human-AI interactions. However, there can be trade-offs between effectiveness and usability (Buçinca et al., 2024; Westby & Riedl, 2023) that must be considered.
Buçinca et al. (2021) examined how interface design might influence cognitive engagement with AI recommendations through what they term “cognitive forcing functions.” Drawing on dual-process theory, they implemented three distinct interface interventions (e.g., requiring explicit requests for AI input, mandating initial independent decisions, introducing temporal delays) designed to disrupt automatic processing and promote more analytical engagement with AI suggestions. Their findings demonstrated that while these interventions successfully reduced overreliance on incorrect AI recommendations, they also increased perceived cognitive load and decreased user satisfaction. Of particular methodological interest was their systematic investigation of individual differences in cognitive motivation: participants with high Need for Cognition (NFC) showed substantially greater benefits from these interventions, suggesting that the effectiveness of such cognitive load manipulations may be moderated by individual differences in information processing preferences.


### Responsibility

•	(S. Narayanan et al., 2023)
Recent work has begun examining how people attribute responsibility in human-AI collaborative contexts where control is shared and actions are interdependent (Tsirtsis et al., 2024). Their study employs a stylized semi-autonomous driving simulation where participants observe how a ‘human agent’ and an ‘AI agent’ collaborate to reach a destination within a time limit. In their setup, the human and AI agents shared control of a vehicle, with each agent having partial and differing knowledge of the environment (i.e., the AI knew about traffic conditions but not road closures, while humans knew about closures but not traffic). Participants observe illustrated simulations of a variety of commute scenarios, and then make judgements about how responsible each agent was for the commute outcome (reaching the destination on time, or not). The study reveals that participants’ responsibility judgments are influenced by factors such as the unexpectedness of an agent’s action, counterfactual simulations of alternative actions, and the actual contribution of each agent to the task outcome.


