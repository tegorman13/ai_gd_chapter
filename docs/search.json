[
  {
    "objectID": "tables.html",
    "href": "tables.html",
    "title": "Tables",
    "section": "",
    "text": "Here are 5 potential table ideas for the chapter:\nHere’s a detailed version of Table 1 (Types of AI Roles), which I think would be most valuable:\nTable 1: AI Roles in Group Decision-Making\nThis table would help readers understand the different ways AI can be integrated into group decision-making, while highlighting key considerations for each role type."
  },
  {
    "objectID": "tables.html#table-1-ai-systems-in-group-decision-making-research",
    "href": "tables.html#table-1-ai-systems-in-group-decision-making-research",
    "title": "Tables",
    "section": "Table 1: AI Systems in Group Decision-Making Research",
    "text": "Table 1: AI Systems in Group Decision-Making Research\n\n\n\n\n\n\n\n\n\n\n\n\nAI Type\nCore Capabilities\nKey Examples\nPrimary Applications\nLimitations\nRepresentative Studies\nPerformance Metrics\n\n\n\n\nLarge Language Models\n- Natural language processing- Context-aware reasoning- Dynamic conversation- Knowledge synthesis\n- GPT-4- Claude- LLaMA\n- Group facilitation- Consensus building- Devil’s advocacy- Information synthesis\n- Hallucinations- Inconsistency- Limited domain expertise- Opaque reasoning\n- Tessler et al. (2024)- Chiang et al. (2024)- Collins et al. (2024)\n- Consensus quality- Discussion depth- Information integration\n\n\nDomain-Specific AI\n- Specialized analysis- Pattern recognition- Optimization- Prediction\n- Medical diagnosis systems- Risk assessment tools- Resource allocation algorithms\n- Clinical decision support- Task allocation- Risk analysis- Performance optimization\n- Limited flexibility- Narrow scope- Rigid decision rules- Poor generalization\n- Bienefeld et al. (2023)- Marjieh et al. (2024)- Swaroop et al. (2024)\n- Task accuracy- Efficiency gains- Error reduction\n\n\nHybrid Systems\n- Multi-modal processing- Integrated reasoning- Adaptive responses- Cross-domain learning\n- LLM+expert systems- Multi-agent frameworks- Integrated decision platforms\n- Complex problem solving- Multi-stakeholder decisions- Dynamic environments\n- Integration challenges- System complexity- Coordination issues\n- Gao et al. (2024)- Burton et al. (2024)- Ma et al. (2024)\n- Solution quality- Adaptation speed- Integration effectiveness\n\n\nInteractive Agents\n- Real-time response- Behavioral adaptation- User modeling- Interface management\n- Conversational agents- Adaptive interfaces- Personalized assistants\n- Team coordination- Information sharing- Process guidance\n- Limited social awareness- Interface constraints- Adaptation lag\n- Yang et al. (2024)- Kumar et al. (2024)- Lu et al. (2024)\n- User satisfaction- Interaction quality- Adaptation accuracy"
  },
  {
    "objectID": "tables.html#table-2-cognitive-biases-in-human-ai-decision-making-2",
    "href": "tables.html#table-2-cognitive-biases-in-human-ai-decision-making-2",
    "title": "Tables",
    "section": "Table 2: Cognitive Biases in Human-AI Decision Making",
    "text": "Table 2: Cognitive Biases in Human-AI Decision Making\n\n\n\n\n\n\n\n\n\n\n\n\nBias Category\nSpecific Biases\nDescription\nManifestation in Human-AI Teams\nMitigation Strategies\nKey Studies\nImpact on Decision Quality\n\n\n\n\nInformation Processing\n- Confirmation bias- Availability bias- Anchoring effect\nSystematic errors in how information is gathered and weighted\n- Over-reliance on AI-provided information- Selective attention to AI suggestions\n- Structured information review- Multiple perspective consideration- Cognitive forcing functions\n- Cheung et al. (2024)- Hagendorff et al. (2023)- Bucinca et al. (2021)\nHigh - Affects core decision processes\n\n\nAI-Specific\n- Automation bias- Algorithm aversion- AI authority bias\nBiases specific to human interaction with AI systems\n- Uncritical acceptance of AI suggestions- Systematic distrust of AI\n- Training in AI capabilities- Transparency in AI reasoning- Balanced reliance strategies\n- Westphal et al. (2023)- Narayanan et al. (2023)- Tsirtsis et al. (2024)\nHigh - Directly impacts AI utilization\n\n\nSocial-Cognitive\n- Group polarization- Conformity bias- Status quo bias\nBiases emerging from social dynamics\n- Amplification of group biases by AI- Resistance to AI-suggested changes\n- Diverse perspective inclusion- Structured disagreement- Devil’s advocate approaches\n- Chuang et al. (2024)- Nishida et al. (2024)- Tessler et al. (2024)\nMedium - Affects group dynamics\n\n\nDecision Context\n- Time pressure bias- Complexity aversion- Risk perception bias\nBiases related to decision environment\n- Rushed integration of AI input- Oversimplification of AI suggestions\n- Time management strategies- Complexity reduction tools- Risk assessment frameworks\n- Swaroop et al. (2024)- Stadler et al. (2024)- Bhatia et al. (2024)\nMedium - Context dependent"
  },
  {
    "objectID": "tables.html#table-3-research-methods-in-human-ai-group-decision-making",
    "href": "tables.html#table-3-research-methods-in-human-ai-group-decision-making",
    "title": "Tables",
    "section": "Table 3: Research Methods in Human-AI Group Decision Making",
    "text": "Table 3: Research Methods in Human-AI Group Decision Making\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMethod Type\nStudy Design\nSample Characteristics\nTask Types\nKey Measures\nAnalysis Approaches\nStrengths\nLimitations\nRepresentative Studies\n\n\n\n\nLaboratory Experiments\n- Controlled conditions- Between/within subjects- Factorial designs\n- Small groups (2-5)- Student/general population- N=100-300 typically\n- Classification- Problem solving- Resource allocation- Group consensus\n- Decision accuracy- Response times- Process measures- User satisfaction\n- ANOVA/MANOVA- Regression analysis- Path modeling- Mediation analysis\n- High internal validity- Precise measurement- Causal inference- Process tracking\n- Limited ecological validity- Simplified tasks- Short timeframes- Selection bias\n- Bucinca et al. (2021)- Chiang et al. (2024)- Kumar et al. (2024)\n\n\nField Studies\n- Natural settings- Longitudinal designs- Case studies\n- Professional teams- Varied group sizes- Domain experts\n- Medical decisions- Project management- Emergency response- Strategic planning\n- Team performance- System usage- Communication patterns- Organizational outcomes\n- Mixed methods- Time series analysis- Network analysis- Content analysis\n- High external validity- Real outcomes- Rich context- Long-term effects\n- Less control- Multiple confounds- Small samples- Resource intensive\n- Bienefeld et al. (2023)- Yang et al. (2024)- Ma et al. (2024)\n\n\nOnline Experiments\n- Web-based platforms- Remote participation- Large-scale designs\n- Large samples- Diverse demographics- N&gt;500 typical\n- Text analysis- Opinion formation- Collective intelligence- Decision aggregation\n- Group agreement- Information flow- Participant behavior- System interaction\n- Computational modeling- Text analytics- Network analysis- Machine learning\n- Large samples- Ecological validity- Diverse participants- Cost effective\n- Less control- Attention issues- Technical constraints- Participation quality\n- Tessler et al. (2024)- Chuang et al. (2024)- Radivojevic et al. (2024)\n\n\nSimulation Studies\n- Agent-based models- System dynamics- Monte Carlo methods\n- Computational agents- Scalable populations- Parameter spaces\n- Strategic interaction- Resource distribution- Network evolution- System optimization\n- Emergent patterns- System efficiency- Optimization metrics- Stability measures\n- Agent-based modeling- System dynamics- Network simulation- Optimization analysis\n- Systematic variation- Large scale testing- Parameter control- Theory development\n- Simplified behavior- Limited realism- Parameter sensitivity- Validation challenges\n- Marjieh et al. (2024)- Gao et al. (2024)- Lu et al. (2024)"
  },
  {
    "objectID": "tables.html#table-1-ai-types-in-decision-making-tasks",
    "href": "tables.html#table-1-ai-types-in-decision-making-tasks",
    "title": "Tables",
    "section": "Table 1: AI Types in Decision-Making Tasks",
    "text": "Table 1: AI Types in Decision-Making Tasks\n\n\n\n\n\n\n\n\n\n\nAI Type\nKey Capabilities\nLimitations\nBest Application Contexts\nExample Studies\n\n\n\n\nRule-Based Systems\nExplicitly programmed logic, predictable behavior, high transparency\nBrittle, difficult to adapt to novel situations, limited learning capacity\nWell-defined tasks with clear rules, automated decision-making in stable environments\n[Find examples if applicable - potentially older systems]\n\n\nMachine Learning (ML)\nLearns patterns from data, adaptable, can handle complex relationships\nRequires large datasets, potential for bias, can be opaque (“black box”)\nPrediction, classification, personalized recommendations, tasks with complex patterns\n[Many examples throughout the chapter; could list a few representative ones from different domains]\n\n\nDeep Learning (DL)\nLearns hierarchical representations, excels in complex pattern recognition\nComputationally intensive, requires even larger datasets, interpretability challenges\nImage recognition, natural language processing, complex data analysis\n[Examples of DL models used in specific studies in the chapter]\n\n\nLarge Language Models (LLMs)\nNatural language understanding and generation, context-aware reasoning\nProne to hallucinations, bias amplification, can be computationally expensive\nFacilitating discussions, summarizing information, creative writing, generating explanations, text-based interactions\nChiang et al. (2024); Tessler et al. (2024); Burton et al. (2024); many others\n\n\nReinforcement Learning (RL)\nLearns through trial and error, optimizes for long-term goals, adaptable to dynamic environments\nCan be unstable, requires careful reward design, exploration-exploitation trade-off\nAutonomous agents, game playing, robotics, personalized learning, sequential decision-making tasks\nCallaway et al. (2022); Marjieh et al. (2024) [Focus on RL-specific studies if any are discussed]\n\n\nHybrid Approaches\nCombines strengths of different AI types (e.g., LLMs with rule-based systems or DL)\nIntegration complexity, potential for conflicts between different AI components\nMulti-faceted decision-making tasks requiring both adaptability and precision, complex tasks requiring multiple AI capabilities\nWu et al. (2024), Gao et al. (2024) [Focus on those that clearly combine multiple AI approaches]"
  },
  {
    "objectID": "tables.html#table-2-cognitive-biases-in-human-ai-decision-making-4",
    "href": "tables.html#table-2-cognitive-biases-in-human-ai-decision-making-4",
    "title": "Tables",
    "section": "Table 2: Cognitive Biases in Human-AI Decision Making",
    "text": "Table 2: Cognitive Biases in Human-AI Decision Making\n\n\n\n\n\n\n\n\n\n\nBias\nDescription\nImpact on Decision-Making\nMitigation Strategies\nExample Studies\n\n\n\n\nConfirmation Bias\nFavoring information that confirms existing beliefs, potentially ignoring contradictory evidence.\nReinforces pre-existing beliefs, leading to suboptimal decisions.\nActively seeking out diverse perspectives, Devil’s advocate techniques, structured analytical techniques.\n[General examples from social psychology literature; perhaps connect to specific AI contexts if relevant in your chapter]\n\n\nAnchoring Bias\nOver-reliance on the first piece of information encountered (the “anchor”), even if irrelevant.\nSkews estimates and judgments toward the anchor.\nProviding multiple anchors, encouraging critical evaluation of initial information, delayed feedback.\nHagendorff et al. (2023); Nguyen (2024)\n\n\nAvailability Heuristic\nOverestimating the likelihood of events that are easily recalled (e.g., due to recent exposure).\nLeads to biased risk assessments and prioritization of readily available information.\nEncouraging systematic information search, providing base-rate information, structured risk assessment frameworks.\nMacmillan-Scott & Musolesi (2024); Suri et al. (2024)\n\n\nHindsight Bias\nBelieving, after an event, that one would have accurately predicted it.\nDistorts evaluation of past decisions and learning from experience.\nProspective documentation of predictions, counterfactual reasoning exercises.\n[Examples from decision-making literature; perhaps discuss implications for AI evaluation or training]\n\n\nFraming Effect\nBeing influenced by the way information is presented (e.g., gains vs. losses).\nLeads to inconsistent choices depending on framing, even if outcomes are equivalent.\nPresenting information in multiple frames, highlighting underlying probabilities and outcomes.\nSuri et al. (2024); Nguyen (2024)\n\n\nOverreliance on AI\nExcessive dependence on AI recommendations, even when incorrect or inappropriate.\nDiminishes human critical thinking and oversight.\nCognitive forcing functions (Buçinca et al., 2021), transparency and explainability, calibrated trust.\nBuçinca et al. (2021); Many studies on trust and reliance in the chapter\n\n\nAlgorithm Aversion\nRejecting AI recommendations due to distrust or lack of understanding.\nLeads to underutilization of potentially beneficial AI assistance.\nIncreased transparency, user control over AI, demonstrably successful AI performance.\n[Studies on algorithm aversion; discuss if mentioned in your chapter]\n\n\nAutomation Bias\nOver-trusting automated systems and neglecting to critically evaluate their outputs.\nIncreases the likelihood of accepting incorrect or incomplete AI recommendations.\nTraining on limitations of AI, clear communication of uncertainty, human-in-the-loop systems.\n[Studies on automation bias in different domains]"
  },
  {
    "objectID": "tables.html#table-3-research-methods-in-human-ai-group-decision-making-1",
    "href": "tables.html#table-3-research-methods-in-human-ai-group-decision-making-1",
    "title": "Tables",
    "section": "Table 3: Research Methods in Human-AI Group Decision-Making",
    "text": "Table 3: Research Methods in Human-AI Group Decision-Making\n\n\n\n\n\n\n\n\n\n\n\n\nResearch Method\nSample Characteristics\nTask Types\nKey Measures\nExample Studies\nStrengths\nLimitations\n\n\n\n\nLaboratory Experiments\nSmall groups (2-4), often student participants; typical N = 100-300\nSimplified tasks (e.g., classification, judgment, resource allocation)\nDecision accuracy, response time, trust ratings, reliance on AI, eye-tracking\nChiang et al. (2024); Buçinca et al. (2021)\nHigh control, causal inference, precise measurement of behavior\nLimited ecological validity, often simplified tasks, potential for demand characteristics\n\n\nField Studies\nProfessional teams, real organizational settings, varied group sizes\nComplex, real-world tasks (e.g., medical diagnosis, emergency response, project planning)\nTeam performance, communication patterns, system adoption, qualitative observations\nBienefeld et al. (2023); Yang et al. (2024)\nHigh ecological validity, real-world outcomes\nLess experimental control, smaller sample sizes, difficult to isolate specific AI effects\n\n\nOnline Experiments\nLarge participant pools, diverse demographics, often N &gt; 500\nVaried tasks (e.g., text analysis, opinion formation, consensus building, games)\nGroup agreement, information sharing, bias measures, survey responses\nTessler et al. (2024); Chuang et al. (2024); Sidji et al. (2024)\nLarge samples, cost-effective, access to diverse populations\nLess control over participation, limited interaction depth, potential for self-selection bias\n\n\nSimulation Studies\nComputational agents, mixed human-AI teams, scalable N\nSimulated environments, strategic games, resource distribution, network formation\nEmergent patterns, system optimization, efficiency metrics, agent behavior\nMarjieh et al. (2024); Gao & Zhang (2024)\nSystematic variation, large-scale testing, exploration of complex dynamics\nMay oversimplify human behavior, limited external validity\n\n\nObservational Studies\nNatural groups, longitudinal data, varied contexts\nReal-world collaborative activities (e.g., collaborative writing, decision support usage, team coordination)\nUsage patterns, adaptation over time, natural behaviors, communication analysis\nRadivojevic et al. (2024); Ma et al. (2024)\nNatural behavior, temporal dynamics, rich qualitative data\nNo experimental manipulation, selection effects, difficulty in establishing causality\n\n\nCase Studies\nIn-depth examination of specific instances or examples.\nVaried, often complex and real-world scenarios.\nQualitative data, detailed descriptions of processes and outcomes.\nMany examples throughout chapter; select a few representative cases.\nRich contextual information, exploration of unique phenomena\nLimited generalizability, potential for researcher bias"
  },
  {
    "objectID": "tables.html#table-1-ai-types-in-decision-making-tasks-1",
    "href": "tables.html#table-1-ai-types-in-decision-making-tasks-1",
    "title": "Tables",
    "section": "Table 1: AI Types in Decision-Making Tasks",
    "text": "Table 1: AI Types in Decision-Making Tasks\n\n\n\n\n\n\n\n\n\n\nAI Type\nKey Capabilities\nLimitations\nBest Application Contexts\nExample Studies\n\n\n\n\nRule-Based Systems\nExplicitly programmed logic, predictable behavior, high transparency, computationally efficient\nBrittle, difficult to adapt to novel situations, limited learning capacity\nWell-defined tasks with clear rules, automated decision-making in stable environments, situations where explainability is paramount\n[Provide specific examples if possible; classic expert systems might fit here]\n\n\nMachine Learning (ML)\nLearns patterns from data, adaptable, can handle complex relationships, can discover non-obvious patterns\nRequires large datasets, potential for bias, can be opaque (“black box”), can require significant computational resources depending on complexity\nPrediction, classification, personalized recommendations, tasks with complex patterns where data is abundant\n[Many examples throughout the chapter; could list a few representative ones from different domains, e.g., medical diagnosis, fraud detection]\n\n\nDeep Learning (DL)\nLearns hierarchical representations, excels in complex pattern recognition, can process unstructured data (images, text, audio)\nComputationally intensive, requires even larger datasets, interpretability challenges, “black box” nature can be problematic\nImage recognition, natural language processing, complex data analysis, tasks with high dimensionality\n[Examples of DL models used in specific studies in the chapter, e.g., image-based medical diagnosis systems]\n\n\nLarge Language Models (LLMs)\nNatural language understanding and generation, context-aware reasoning, knowledge retrieval, creative text generation\nProne to hallucinations, bias amplification, can be computationally expensive, limited reasoning abilities in some contexts, can be difficult to control\nFacilitating discussions, summarizing information, creative writing, generating explanations, text-based interactions, tasks requiring natural language understanding\nChiang et al. (2024); Tessler et al. (2024); Burton et al. (2024); many others\n\n\nReinforcement Learning (RL)\nLearns through trial and error, optimizes for long-term goals, adaptable to dynamic environments, can learn optimal strategies in complex situations\nCan be unstable, requires careful reward design, exploration-exploitation trade-off, computationally expensive, can be difficult to interpret\nAutonomous agents, game playing, robotics, personalized learning, sequential decision-making tasks, resource allocation problems\nCallaway et al. (2022); Marjieh et al. (2024) [Focus on RL-specific studies if any are discussed]\n\n\nHybrid Approaches\nCombines strengths of different AI types (e.g., LLMs with rule-based systems or DL), can leverage specialized AI modules for specific sub-tasks\nIntegration complexity, potential for conflicts between different AI components, requires careful design and coordination\nMulti-faceted decision-making tasks requiring both adaptability and precision, complex tasks requiring multiple AI capabilities, tasks where different AI strengths can be combined synergistically\nWu et al. (2024), Gao et al. (2024) [Focus on those that clearly combine multiple AI approaches]"
  },
  {
    "objectID": "tables.html#table-2-cognitive-biases-in-human-ai-decision-making-5",
    "href": "tables.html#table-2-cognitive-biases-in-human-ai-decision-making-5",
    "title": "Tables",
    "section": "Table 2: Cognitive Biases in Human-AI Decision Making",
    "text": "Table 2: Cognitive Biases in Human-AI Decision Making\n\n\n\n\n\n\n\n\n\n\nBias\nDescription\nImpact on Decision-Making\nMitigation Strategies\nExample Studies\n\n\n\n\nConfirmation Bias\nFavoring information that confirms existing beliefs, potentially ignoring contradictory evidence.\nReinforces pre-existing beliefs, leading to suboptimal or biased decisions.\nActively seeking out diverse perspectives, Devil’s advocate techniques, structured analytical techniques, critical thinking training\nNickerson (1998) [Provide specific examples in AI contexts from your chapter, if any]\n\n\nAnchoring Bias\nOver-reliance on the first piece of information encountered (the “anchor”), even if irrelevant.\nSkews estimates and judgments toward the anchor, hindering objective evaluation.\nProviding multiple anchors, encouraging critical evaluation of initial information, delayed feedback, using ranges instead of point estimates\nHagendorff et al. (2023); Nguyen (2024)\n\n\nAvailability Heuristic\nOverestimating the likelihood of events that are easily recalled (e.g., due to recent exposure or vividness).\nLeads to biased risk assessments and prioritization of readily available information, even if not representative.\nEncouraging systematic information search, providing base-rate information, considering less salient but relevant information, structured risk assessment frameworks\nTversky & Kahneman (1973); [Connect to AI contexts if relevant in your chapter]\n\n\nHindsight Bias\nBelieving, after an event, that one would have accurately predicted it.\nDistorts evaluation of past decisions, hinders learning from experience, leads to overconfidence.\nProspective documentation of predictions, counterfactual reasoning exercises, reflecting on the limitations of one’s own knowledge\nFischhoff (1975); [Discuss implications for AI evaluation or training if relevant]\n\n\nFraming Effect\nBeing influenced by the way information is presented (e.g., gains vs. losses).\nLeads to inconsistent choices depending on framing, even if outcomes are logically equivalent.\nPresenting information in multiple frames, highlighting underlying probabilities and outcomes, focusing on objective consequences rather than subjective framing\nTversky & Kahneman (1981); Suri et al. (2024)\n\n\nOverreliance on AI\nExcessive dependence on AI recommendations, even when incorrect or inappropriate.\nDiminishes human critical thinking and oversight, can lead to errors and propagation of AI biases.\nCognitive forcing functions (Buçinca et al., 2021), transparency and explainability, calibrated trust, training on AI limitations, clear communication of uncertainty\nBuçinca et al. (2021); Many studies on trust and reliance in the chapter\n\n\nAlgorithm Aversion\nRejecting AI recommendations due to distrust or lack of understanding, even when AI performance is demonstrably better.\nLeads to underutilization of potentially beneficial AI assistance, hinders optimal human-AI collaboration.\nIncreased transparency and explainability, user control over AI, demonstrably successful AI performance, education about AI capabilities and limitations\nDietvorst et al. (2015); [Studies on algorithm aversion; discuss if mentioned in your chapter]\n\n\nAutomation Bias\nOver-trusting automated systems and neglecting to critically evaluate their outputs.\nIncreases the likelihood of accepting incorrect or incomplete AI recommendations, especially in complex or time-pressured situations.\nTraining on limitations of AI, clear communication of uncertainty, human-in-the-loop systems, redundant checks and balances, promoting vigilance and critical evaluation\n[Studies on automation bias in different domains, e.g., Parasuraman & Manzey (2010)]"
  },
  {
    "objectID": "tables.html#table-3-research-methods-in-human-ai-group-decision-making-2",
    "href": "tables.html#table-3-research-methods-in-human-ai-group-decision-making-2",
    "title": "Tables",
    "section": "Table 3: Research Methods in Human-AI Group Decision-Making",
    "text": "Table 3: Research Methods in Human-AI Group Decision-Making\n\n\n\n\n\n\n\n\n\n\n\n\nResearch Method\nSample Characteristics\nTask Types\nKey Measures\nExample Studies\nStrengths\nLimitations\n\n\n\n\nLaboratory Experiments\nSmall groups (2-4), often student participants; typical N = 100-300\nSimplified, controlled tasks (e.g., classification, judgment, resource allocation, logic puzzles)\nDecision accuracy, response time, trust ratings, reliance on AI, eye-tracking, think-aloud protocols, physiological measures\nChiang et al. (2024); Buçinca et al. (2021); Swaroop et al. (2024)\nHigh control, causal inference, precise measurement of behavior\nLimited ecological validity, often simplified tasks, potential for demand characteristics, limited generalizability\n\n\nField Studies\nProfessional teams, real organizational settings, varied group sizes\nComplex, real-world tasks (e.g., medical diagnosis, emergency response, project planning, incident response)\nTeam performance, communication patterns, system adoption, qualitative observations (interviews, focus groups), workflow analysis\nBienefeld et al. (2023); Yang et al. (2024)\nHigh ecological validity, real-world outcomes\nLess experimental control, smaller sample sizes, difficult to isolate specific AI effects, potential for confounding variables\n\n\nOnline Experiments\nLarge participant pools, diverse demographics, often N &gt; 500\nVaried tasks (e.g., text analysis, opinion formation, consensus building, games, surveys, simulated work tasks)\nGroup agreement, information sharing, bias measures, survey responses, behavioral logs, communication content analysis\nTessler et al. (2024); Chuang et al. (2024); Sidji et al. (2024); Nishida et al. (2024)\nLarge samples, cost-effective, access to diverse populations, can study large-scale dynamics\nLess control over participation, limited interaction depth, potential for self-selection bias, difficulty ensuring data quality\n\n\nSimulation Studies\nComputational agents, mixed human-AI teams, scalable N\nSimulated environments, strategic games (e.g., negotiation games), resource distribution, network formation\nEmergent patterns, system optimization, efficiency metrics, agent behavior, communication patterns, social dynamics\nMarjieh et al. (2024); Gao et al. (2024); Abdelnabi et al. (2023)\nSystematic variation, large-scale testing, exploration of complex dynamics, can isolate specific mechanisms\nMay oversimplify human behavior, limited external validity, assumptions of the simulation model can influence results\n\n\nObservational Studies\nNatural groups, longitudinal data, varied contexts\nReal-world collaborative activities (e.g., collaborative writing, decision support usage, team coordination, online discussions)\nUsage patterns, adaptation over time, natural behaviors, communication analysis, qualitative observations\nRadivojevic et al. (2024); Ma et al. (2024)\nNatural behavior, temporal dynamics, rich qualitative data, can study evolving interactions\nNo experimental manipulation, selection effects, difficulty in establishing causality, observer bias\n\n\nCase Studies\nIn-depth examination of specific instances or examples.\nVaried, often complex and real-world scenarios.\nQualitative data, detailed descriptions of processes and outcomes, analysis of specific events or interactions\nMany examples throughout chapter; select a few representative cases.\nRich contextual information, exploration of unique phenomena, can generate hypotheses\nLimited generalizability, potential for researcher bias, difficult to replicate"
  },
  {
    "objectID": "v1.html",
    "href": "v1.html",
    "title": "AI and Group Decision Making: An Information Processing Perspective",
    "section": "",
    "text": "Artificial Intelligence (AI) is becoming a central component of group decision-making processes across a range of domains. From healthcare to finance, education to policymaking, AI systems are being integrated into group decision-making processes, offering new avenues for enhancing efficiency, accuracy, and innovation (BaniHani et al., 2024; Burton et al., 2024; Carter & Wynne, 2024). This growing collaboration between humans and AI brings forth both significant opportunities and pressing challenges. On one hand, AI systems offer the potential to enhance information processing efficiency, improve decision accuracy, and streamline communication within teams. On the other hand, the complexities inherent in human-AI interactions—such as issues of trust and over-reliance, susceptibility to cognitive biases, erosion of critical thinking skills, lack of transparency in AI algorithms, and ethical concerns regarding accountability and fairness.\nThe use of AI in group settings has evolved from basic decision-support tools to more sophisticated roles, such as collaborative partners capable of generating novel insights. Large language models (LLMs), for instance, can facilitate collective intelligence by synthesizing information, generating alternative solutions, and even mediating group discussions​​. However, the extent to which AI enhances group performance remains context-dependent. Recent meta-analyses reveal that human-AI collaboration can lead to either augmentation of individual performance or to performance decrements (Vaccaro et al., 2024), depending on the task and interaction design​​.\nTo navigate these complexities, this chapter adopts the information processing framework as a lens for examining AI-assisted group decision-making (Hinsz et al., 1997). This framework provides a structured method to analyze how AI systems interact with human cognitive processes at each stage of decision-making. By dissecting the inputs (information acquisition and sharing), the processing mechanisms (interpretation and integration of information), and the outputs (decisions and actions), we can gain insights into the opportunities and challenges presented by AI integration.\nKey questions we will seek to address within this framework::\n\nInputs: How does AI influence the way groups search for, gather, and share information? For example, AI can augment information search through advanced data retrieval but may also introduce biases based on the algorithms’ training data.\nProcessing: In what ways do AI systems affect the interpretation and integration of information within the group? AI can facilitate complex data analysis but might obscure the reasoning process through opaque algorithms, impacting the group’s shared understanding.\nOutputs: How do AI recommendations influence the group’s final decisions and actions? The reliance on AI outputs raises questions about trust, accountability, and the potential diminishment of human agency.\n\nThis analysis is particularly timely given the rapid advancement of AI capabilities…."
  },
  {
    "objectID": "v1.html#introduction",
    "href": "v1.html#introduction",
    "title": "AI and Group Decision Making: An Information Processing Perspective",
    "section": "",
    "text": "Artificial Intelligence (AI) is becoming a central component of group decision-making processes across a range of domains. From healthcare to finance, education to policymaking, AI systems are being integrated into group decision-making processes, offering new avenues for enhancing efficiency, accuracy, and innovation (BaniHani et al., 2024; Burton et al., 2024; Carter & Wynne, 2024). This growing collaboration between humans and AI brings forth both significant opportunities and pressing challenges. On one hand, AI systems offer the potential to enhance information processing efficiency, improve decision accuracy, and streamline communication within teams. On the other hand, the complexities inherent in human-AI interactions—such as issues of trust and over-reliance, susceptibility to cognitive biases, erosion of critical thinking skills, lack of transparency in AI algorithms, and ethical concerns regarding accountability and fairness.\nThe use of AI in group settings has evolved from basic decision-support tools to more sophisticated roles, such as collaborative partners capable of generating novel insights. Large language models (LLMs), for instance, can facilitate collective intelligence by synthesizing information, generating alternative solutions, and even mediating group discussions​​. However, the extent to which AI enhances group performance remains context-dependent. Recent meta-analyses reveal that human-AI collaboration can lead to either augmentation of individual performance or to performance decrements (Vaccaro et al., 2024), depending on the task and interaction design​​.\nTo navigate these complexities, this chapter adopts the information processing framework as a lens for examining AI-assisted group decision-making (Hinsz et al., 1997). This framework provides a structured method to analyze how AI systems interact with human cognitive processes at each stage of decision-making. By dissecting the inputs (information acquisition and sharing), the processing mechanisms (interpretation and integration of information), and the outputs (decisions and actions), we can gain insights into the opportunities and challenges presented by AI integration.\nKey questions we will seek to address within this framework::\n\nInputs: How does AI influence the way groups search for, gather, and share information? For example, AI can augment information search through advanced data retrieval but may also introduce biases based on the algorithms’ training data.\nProcessing: In what ways do AI systems affect the interpretation and integration of information within the group? AI can facilitate complex data analysis but might obscure the reasoning process through opaque algorithms, impacting the group’s shared understanding.\nOutputs: How do AI recommendations influence the group’s final decisions and actions? The reliance on AI outputs raises questions about trust, accountability, and the potential diminishment of human agency.\n\nThis analysis is particularly timely given the rapid advancement of AI capabilities…."
  },
  {
    "objectID": "v1.html#inputs",
    "href": "v1.html#inputs",
    "title": "AI and Group Decision Making: An Information Processing Perspective",
    "section": "Inputs",
    "text": "Inputs\n..\n\nGroup Member Roles\nDeciding how best to assign team members to roles is crucial in group decision-making, particularly when learning who is best suited for what role within a team. Marjieh et al. (2024) explore how humans allocate tasks within teams comprising both human and AI agents to maximize overall performance. The central theme of their research is understanding the mechanisms by which individuals discern and act upon their own strengths and those of their team members in a dynamic task allocation setting. In their experimental paradigm, participants had to repeatedly allocate three different types of tasks (visual, auditory, and lexical tasks) between themselves and two AI agents. Unbeknownst to participants, each AI agent was configured to have high competence (70% success rate) in one task type but low competence (15% success rate) in others.\nBuilding upon this, McNeese et al. (2023) argue that human-autonomy teams (HATs) should be recognized as distinct from traditional human teams. They emphasize that HATs should not strive to replicate human-human team dynamics but instead should leverage the unique capabilities of AI agents. The authors propose several research trajectories to advance our understanding of HATs, including exploring diverse teaming models, redefining roles for AI teammates, expanding communication modalities, focusing on AI behavior design, developing specialized training, and emphasizing teamwork in AI design. These insights highlight the necessity of adjusting our approaches to team composition and role assignment when AI agents are involved, ensuring that both human and AI strengths are optimized in the decision-making process.\n\nRecent advances in large language models have dramatically expanded the potential roles of AI in group decision-making, enabling AI agents to move beyond simple advisory functions to serve as mediators, devil’s advocates, and active discussion participants\nChiang et al. (2024) investigated the potential of Large Language Models (LLMs) to act as devil’s advocates in AI-assisted group decision-making - in the hopes of fostering more critical engagement with AI assistance. In their experimental task, participants were first individually trained on the relationship between defendant profiles and recidivism. For each defendant, participants were also shown the prediction of a recommendation AI model (RiskComp). Participants were then sorted into groups of three, where they reviewed and discussed novel defendant profiles, before making a group recidivism assessment. In the group stage, the recommendations from the RiskComp model were biased against a subset of the defendants (black defendants with low prior crime counts). Of interest was whether the inclusion of an LLM-based devil’s advocate in the group discussions could help mitigate the bias introduced by the RiskComp AI model (note that the LLM devils advocate and RiskComp AI are separate AI models). The experimental manipulation consisted of four variants of an LLM-based devil’s advocate using, varying both the target of objection (challenging either RiskComp recommendations or majority group opinions) and the level of interactivity (static one-time comments versus dynamic engagement throughout the discussions). Their findings revealed that the dynamic devil’s advocate led to higher decision accuracy and improved discernment of when to trust the RiskComp model’s advice.\n\n(A. Kumar et al., 2024)\n(Lu et al., 2024)\n(McNeese et al., 2023)"
  },
  {
    "objectID": "v1.html#information-processing",
    "href": "v1.html#information-processing",
    "title": "AI and Group Decision Making: An Information Processing Perspective",
    "section": "Information Processing",
    "text": "Information Processing\n\nInformation Search\nThe information search stage of decision-making, once reliant on human capacity to locate and synthesize data, has been transformed by the advent of artificial intelligence (AI), particularly Large Language Models (LLMs). This section explores how AI reshapes information search, augmenting both data retrieval and synthesis, and fostering idea generation and creative discovery.\n\nAI-Assisted Data Retrieval and Synthesis\nLLMs significantly enhance the efficiency and comprehensiveness of information gathering, enabling access to a broader knowledge base and deeper insights (Bouschery et al., 2023). These models process vast datasets, identifying connections and patterns beyond human capacity. Furthermore, individual differences, such as computational thinking skills, influence how users interact with LLMs, with those possessing higher creativity and algorithmic thinking more effectively leveraging AI-generated content for deeper engagement within a specific information landscape (Flores et al., 2024). Programmers, for example, navigate between traditional web search and generative AI tools, strategically selecting between them based on factors like task familiarity and goal clarity, demonstrating the synergistic use of both resources (Yen et al., 2024). DiscipLink, for instance, uses LLMs to generate exploratory questions across disciplines, automatically expand queries with field-specific terminology, and extract themes from retrieved papers, effectively bridging knowledge gaps in interdisciplinary research (Zheng et al., 2024). Moreover, AI facilitates advanced techniques like retrieval-augmented generation (RAG), allowing LLMs to access and process real-time information, enhancing the accuracy and relevance of their output (Si et al., 2024; Wang et al., 2024). This capability empowers decision-makers with synthesized insights from diverse sources, crucial for informed choices across various fields, from scientific research to policy analysis (Burton et al., 2024).\nLLM-based search tools offer natural language interfaces, streamlining complex queries and providing detailed responses, often leading to increased efficiency and user satisfaction (Spatharioti et al., 2023). However, this ease of use can also lead to overreliance on potentially inaccurate information and decreased critical evaluation, particularly when presented conversationally (Anderl et al., 2024). This can contribute to confirmation bias and the formation of “generative echo chambers,” limiting exposure to diverse perspectives (Sharma et al., 2024). Furthermore, while LLMs can reduce cognitive load during information seeking, this may come at the cost of deeper learning and engagement with the material, leading to less sophisticated reasoning and argumentation (Stadler et al., 2024). Therefore, careful design and implementation are crucial to mitigate these risks and leverage the full potential of LLMs for enhanced information retrieval and synthesis.\n\n\nAI in Idea Generation and Creative Discovery\nAI’s role extends beyond data retrieval to fostering creative discovery. LLMs act as catalysts, offering alternative perspectives, challenging assumptions, and proposing unexpected connections (Bouschery et al., 2023). In structured tasks like semantic search, AI agents enhance group performance by selectively sharing information, amplifying collective intelligence (Ueshima & Takikawa, 2024). Studies comparing human and AI-generated ideas reveal a nuanced picture: while LLMs excel at generating ideas with higher average quality (e.g., purchase intent) and even surpassing human experts in novelty (Joosten et al., 2024; Meincke et al., 2024; Si et al., 2024), they may exhibit lower feasibility (Joosten et al., 2024) and reduced diversity (Meincke et al., 2024). This highlights the importance of strategic prompt engineering, as demonstrated by Boussioux et al. (2024), who found that human-guided prompts—specifically differentiated search (i.e., prompts designed to encourage diverse and varied responses) — enhanced the novelty of LLM-generated solutions while maintaining high value.\nThe type of AI interaction also significantly influences human creativity. Ashkinaze et al. (2024) found that exposure to AI-generated ideas increased the diversity of collective ideas without affecting individual creativity. In contrast, H. Kumar et al. (2024) observed that while providing direct answers had minimal negative impact, exposure to LLM-generated strategies decreased both originality and creative flexibility in subsequent unassisted tasks.\n\n\n\nCommunication; information sharing\nTransactive memory systems (TMS) represent a critical aspect of group cognition, referring to the shared understanding within a group regarding the distribution of knowledge and expertise among its members (Wegner, 1987; Yan et al., 2021). A well-functioning TMS enables team members not only to know who possesses specific knowledge but also to access and share this distributed expertise efficiently.\nBienefeld et al. (2023) conducted an observational study to examine the role of transactive memory systems and speaking-up behaviors in human-AI teams within an intensive care unit (ICU) setting. In this study, ICU physicians and nurses, divided into groups of four, who collaborated with an AI agent named “Autovent.” Autovent is an auto-adaptive ventilator system that autonomously manages patient ventilation by processing continuous, individualized data streams. Participants, all with a minimum of six months’ experience using Autovent, engaged in simulated clinical scenarios that required diagnosing and treating critically ill patients. Using behavioral coding of video recordings, the researchers analyzed how team members accessed information from both human teammates and the AI system, investigating how these human-human and human-ai interactions related to subsequent behaviors like hypothesis generation and speaking up with concerns. The researchers found that in higher-performing teams, accessing knowledge from the AI agent was positively correlated with developing new hypotheses and increased speaking-up behavior. Conversely, accessing information from human team members was negatively associated with these behaviors, regardless of team performance. These results suggest that AI systems may serve as unique knowledge repositories that help teams overcome some of the social barriers that typically inhibit information sharing and voice behaviors in purely human teams.\nBastola et al. (2024) further explored the potential of AI-mediated communication by examining how an LLM-based Smart Reply (LSR) system could impact collaborative performance in professional settings. They developed a system utilizing ChatGPT to generate context-aware, personalized responses during workplace interactions, aiming to reduce the cognitive effort required for message composition in multitasking scenarios. In their study, participants engaged in a cognitively demanding Dual N-back task while managing scheduling activities via Google Calendar and responding to simulated co-workers on Slack. The findings indicated that the use of the LSR system not only improved work performance—evidenced by higher accuracy in the N-back task—but also increased messaging efficiency and reduced cognitive load, as participants could more readily focus on primary tasks without the distraction of composing responses. However, it is important to note that participants expressed concerns about the appropriateness and accuracy of AI-generated messages, as well as issues related to trust and privacy. Thus, while AI-mediated communication tools like the LSR system may facilitate information sharing and alleviate cognitive demands in collaborative work, these benefits must be balanced against potential user experience challenges to fully realize their potential advantages.\n\n(Yang et al., 2024)\n(Ma et al., 2024)\n(Radivojevic et al., 2024)\n(Sidji et al., 2024)\n(Nishida et al., 2024)\n(Chuang et al., 2024)\n\n\n\nShared Mental Models\n\n(Collins et al., 2024)\n\n…\n\n\nCognitive Load\nBuçinca et al. (2021) examined how interface design might influence cognitive engagement with AI recommendations through what they term “cognitive forcing functions.” Drawing on dual-process theory, they implemented three distinct interface interventions (e.g., requiring explicit requests for AI input, mandating initial independent decisions, introducing temporal delays) designed to disrupt automatic processing and promote more analytical engagement with AI suggestions. Their findings demonstrated that while these interventions successfully reduced overreliance on incorrect AI recommendations, they also increased perceived cognitive load and decreased user satisfaction. Of particular methodological interest was their systematic investigation of individual differences in cognitive motivation: participants with high Need for Cognition (NFC) showed substantially greater benefits from these interventions, suggesting that the effectiveness of such cognitive load manipulations may be moderated by individual differences in information processing preferences."
  },
  {
    "objectID": "v1.html#decision-making-output",
    "href": "v1.html#decision-making-output",
    "title": "AI and Group Decision Making: An Information Processing Perspective",
    "section": "Decision-Making Output",
    "text": "Decision-Making Output\n..\n\nConsensus Formation\nTessler et al. (2024) investigated the potential of AI in facilitating consensus formation through their development of the “Habermas Machine” (HM), an LLM-based system fine-tuned to mediate human deliberation. The HM system receives input statements from individual participants, and attempts to generate consensus statements which will maximize group endorsement. The findings revealed that the AI-generated group statements were consistently preferred over comparison statements written by human mediators. Participants rated the AI-mediated statements higher in terms of informativeness, clarity, and lack of bias. This suggests that AI can effectively capture the collective sentiment of a group and articulate it in a way that resonates with its members. Notably, the researchers also verified that the HM system reliably incorporated minority opinions into the consensus statements, preventing dominance by majority perspectives. These results were replicated in a virtual citizens’ assembly with a demographically representative sample of the UK population. The AI-mediated process again resulted in high-quality group statements and facilitated consensus among participants on contentious issues.\n\n\nDecision Accuracy and Complementarity\nThe integration of AI into group decision-making holds significant promise for leveraging the complementary strengths of humans and machines. Rastogi et al. (2023) proposed a taxonomy to characterize differences in human and machine decision-making, providing a framework for understanding how to combine their unique capabilities optimally. This taxonomy highlights areas where AI can augment human decision processes, such as handling large data sets or identifying patterns beyond human perceptual abilities. Becker et al. (2022) demonstrated that AI-generated decision aids, when presented as interpretable procedural instructions, can significantly improve human decision-making by promoting more resource-rational strategies. In complex domains, Shin et al. (2023) showed that exposure to superhuman AI, as in the game of Go, can enhance human decision-making by encouraging the exploration of novel strategies, thereby increasing overall performance and innovation. However, the effectiveness of human-AI collaboration depends on the dynamics of the interaction.\nRecent research has revealed complex trade-offs in human-AI team performance that depend heavily on task structure and collaboration dynamics. Bennett et al. (2023) found that while both human-human and human-AI teams experienced performance costs relative to theoretical benchmarks, human-human teams showed particular advantages in collaborative versus competitive conditions—an effect that diminished when humans worked with AI partners. This aligns with findings from Liang et al. (2022) showing that humans can learn to selectively rely on AI assistance based on task difficulty, but often require explicit feedback and training to optimize this collaboration. The reduced collaborative advantage in human-AI teams appears to stem from difficulties in developing shared mental models and coordinating actions effectively, suggesting that current AI systems may lack crucial capabilities for fluid team interaction."
  },
  {
    "objectID": "v1.html#trust-risk-and-reliance",
    "href": "v1.html#trust-risk-and-reliance",
    "title": "AI and Group Decision Making: An Information Processing Perspective",
    "section": "Trust, Risk and Reliance",
    "text": "Trust, Risk and Reliance\n\nTrust in AI\nTrust and confidence are crucial determinants of how individuals and groups interact with AI systems during decision-making processes. Research indicates that individuals’ confidence levels significantly influence their propensity to seek and utilize AI-generated advice. Pescetelli & Yeung (2021) found that people are more likely to seek advice when their confidence in their own decisions is low; however, they often deviate from optimal Bayesian integration when incorporating this advice, sometimes relying on heuristic strategies instead. Carlebach & Yeung (2023) further demonstrated that the relationship between confidence and advice-seeking is context-dependent. When the reliability of an AI advisor is unknown, individuals may paradoxically seek advice even when they are confident, using their own confidence as a feedback mechanism to learn about the advisor’s quality. Liang et al. (2022) explored how explicit performance comparisons between humans and AI affect reliance on decision aids, revealing that individuals adapt their use of AI recommendations based on task difficulty and perceived accuracy differences. Steyvers et al. (2024) demonstrated that humans tend to overestimate AI system accuracy when presented with default explanations, particularly when those explanations are lengthy. However, this miscalibration can be mitigated by explicitly communicating the AI’s uncertainty levels.\nIn group settings, trust in AI systems significantly influences how teams interact and make decisions. Cui & Yasseri (2024) emphasize that trust between humans and AI is crucial for achieving collective intelligence in teams. They argue that factors such as the perceived competence, benevolence, and integrity of AI systems shape this trust, mirroring the dynamics of trust in human relationships. The level of anthropomorphism in AI agents can also affect trust; while human-like features may initially enhance trust, this effect can wane if the AI’s performance does not meet team expectations. Zvelebilova et al. (2024) further demonstrate that even when teams do not fully trust an AI assistant or consider it a genuine team member, the AI can still significantly influence team discourse and collective attention. Their study found that teams adopted terminology introduced by the AI, indicating an automatic integration of AI input despite doubts about its reliability. This suggests that AI systems can impact group cognition and coordination regardless of explicit trust levels. These findings highlight the complexity of trust in AI within team environments, where both the design of AI agents and their subtle influences on team dynamics must be carefully managed to enhance collaborative outcomes.\n\n\nUtilization\nRecent work by Buçinca et al. (2021) presents an innovative approach to addressing overreliance on AI systems through interface design rather than explanation quality. Their study evaluated three “cognitive forcing functions” - interface elements designed to disrupt quick, heuristic processing of AI recommendations. Although these interventions significantly reduced overreliance on incorrect AI recommendations, an important trade-off emerged: interfaces that most effectively prevented overreliance were also rated as most complex and least preferred by users. Moreover, their analysis revealed potential equity concerns, as the interventions provided substantially greater benefits to individuals with high Need for Cognition. These findings suggest that while interface design can effectively modulate AI utilization patterns, careful consideration must be given to both user experience and potential intervention-generated inequalities.\n\n\nResponsibility\n\n(Narayanan et al., 2023)\n\nRecent work has begun examining how people attribute responsibility in human-AI collaborative contexts where control is shared and actions are interdependent (Tsirtsis et al., 2024). Their study employs a stylized semi-autonomous driving simulation where participants observe how a ‘human agent’ and an ‘AI agent’ collaborate to reach a destination within a time limit. In their setup, the human and AI agents shared control of a vehicle, with each agent having partial and differing knowledge of the environment (i.e., the AI knew about traffic conditions but not road closures, while humans knew about closures but not traffic). Participants observe illustrated simulations of a variety of commute scenarios, and then make judgements about how responsible each agent was for the commute outcome (reaching the destination on time, or not). The study reveals that participants’ responsibility judgments are influenced by factors such as the unexpectedness of an agent’s action, counterfactual simulations of alternative actions, and the actual contribution of each agent to the task outcome.\n\n\nRisk\n\n(Bhatia, 2024)\n(Zhu et al., 2024)"
  },
  {
    "objectID": "v1.html#tables",
    "href": "v1.html#tables",
    "title": "AI and Group Decision Making: An Information Processing Perspective",
    "section": "Tables",
    "text": "Tables\n\n\n\n\n\nTable 1: Research methods in human-AI decision making\n\n\n\n\n\n\n\n\n\n\n\nResearch Method\nTask Types\nKey Measures\nExample Studies\n\n\n\n\nLaboratory Experiments\nSimplified, controlled tasks (e.g., classification, judgment, resource allocation, logic puzzles)\nDecision accuracy, response time, trust ratings, reliance on AI, eye-tracking, think-aloud protocols, physiological measures\nChiang et al. (2024); Buçinca et al. (2021); Swaroop et al. (2024)\n\n\nField Studies\nComplex, real-world tasks (e.g., medical diagnosis, emergency response, project planning, incident response)\nTeam performance, communication patterns, system adoption, qualitative observations (interviews, focus groups), workflow analysis\nBienefeld et al. (2023); Yang et al. (2024)\n\n\nOnline Experiments\nVaried tasks (e.g., text analysis, opinion formation, consensus building, games, surveys, simulated work tasks)\nGroup agreement, information sharing, bias measures, survey responses, behavioral logs, communication content analysis\nTessler et al. (2024); Chuang et al. (2024); Sidji et al. (2024); Nishida et al. (2024)\n\n\nObservational Studies\nReal-world collaborative activities (e.g., collaborative writing, decision support usage, team coordination, online discussions)\nUsage patterns, adaptation over time, natural behaviors, communication analysis, qualitative observations\nRadivojevic et al. (2024); Ma et al. (2024)\n\n\n\n\n\n\n\n\n\nTable 2: Types of AI used in decision-making tasks\n\n\n\n\n\n\n\n\n\n\n\nAI Type\nKey Capabilities\nPrimary Applications\nRepresentative Studies\n\n\n\n\nLarge Language Models\nNatural language understanding; Open-ended dialogue; Context adaptation; Knowledge synthesis; Abstract reasoning\nGroup facilitation; Information synthesis; Creative ideation; Deliberation support\nTessler et al. (2024); Chiang et al. (2024); Collins et al. (2024); Du et al. (2024)\n\n\nDomain-Specific AI\nExpert knowledge; Precise calculations; Consistent performance; Real-time processing; Specialized optimization\nMedical diagnosis; Resource allocation; Risk assessment; Pattern detection\nBienefeld et al. (2023); Marjieh et al. (2024); Yang et al. (2024)\n\n\nHybrid Systems\nCombined generalist/specialist capabilities; Integrated expertise; Adaptive reasoning; Multi-modal processing\nComplex decision-making; Strategic planning; Knowledge management\nBurton et al. (2024); Ma et al. (2024); Gao et al. (2024)\n\n\nAutonomous Agents\nIndependent operation; Goal-directed behavior; Environmental interaction; Continuous learning\nTeam collaboration; Process automation; Distributed tasks\nMcNeese et al. (2023); Guo et al. (2024); Zhang et al. (2024)\n\n\nRecommender Systems\nPersonalization; Pattern recognition; Preference learning; Information filtering\nInformation curation; Decision support; Resource suggestions\nStadler et al. (2024); Kumar et al. (2024); Swaroop et al. (2024)"
  },
  {
    "objectID": "v1.html#references",
    "href": "v1.html#references",
    "title": "AI and Group Decision Making: An Information Processing Perspective",
    "section": "References",
    "text": "References\n\n\nAnderl, C., Klein, S. H., Sarigül, B., Schneider, F. M., Han, J., Fiedler, P. L., & Utz, S. (2024). Conversational presentation mode increases credibility judgements during information search with ChatGPT. Scientific Reports, 14(1), 17127. https://doi.org/10.1038/s41598-024-67829-6\n\n\nAshkinaze, J., Mendelsohn, J., Qiwei, L., Budak, C., & Gilbert, E. (2024). How AI Ideas Affect the Creativity, Diversity, and Evolution of Human Ideas: Evidence From a Large, Dynamic Experiment (arXiv:2401.13481). arXiv. https://doi.org/10.48550/arXiv.2401.13481\n\n\nBaniHani, I., Alawadi, S., & Elmrayyan, N. (2024). AI and the decision-making process: A literature review in healthcare, financial, and technology sectors. Journal of Decision Systems, 1–11. https://doi.org/10.1080/12460125.2024.2349425\n\n\nBastola, A., Wang, H., Hembree, J., Yadav, P., Gong, Z., Dixon, E., Razi, A., & McNeese, N. (2024). LLM-based Smart Reply (LSR): Enhancing Collaborative Performance with ChatGPT-mediated Smart Reply System (arXiv:2306.11980). arXiv. https://arxiv.org/abs/2306.11980\n\n\nBecker, F., Skirzyński, J., van Opheusden, B., & Lieder, F. (2022). Boosting Human Decision-making with AI-Generated Decision Aids. Computational Brain & Behavior, 5(4), 467–490. https://doi.org/10.1007/s42113-022-00149-y\n\n\nBennett, M. S., Hedley, L., Love, J., Houpt, J. W., Brown, S. D., & Eidels, A. (2023). Human Performance in Competitive and Collaborative Human–Machine Teams. Topics in Cognitive Science, 1–25. https://doi.org/10.1111/tops.12683\n\n\nBhatia, S. (2024). Exploring variability in risk taking with large language models. Journal of Experimental Psychology: General, 153(7), 1838–1860. https://doi.org/10.1037/xge0001607\n\n\nBienefeld, N., Kolbe, M., Camen, G., Huser, D., & Buehler, P. K. (2023). Human-AI teaming: Leveraging transactive memory and speaking up for enhanced team effectiveness. Frontiers in Psychology, 14. https://doi.org/10.3389/fpsyg.2023.1208019\n\n\nBouschery, S. G., Blazevic, V., & Piller, F. T. (2023). Augmenting human innovation teams with artificial intelligence: Exploring transformer-based language models. Journal of Product Innovation Management, 40(2), 139–153. https://doi.org/10.1111/jpim.12656\n\n\nBoussioux, L., Lane, J. N., Zhang, M., Jacimovic, V., & Lakhani, K. R. (2024). The Crowdless Future? Generative AI and Creative Problem-Solving. Organization Science, 35(5), 1589–1607. https://doi.org/10.1287/orsc.2023.18430\n\n\nBuçinca, Z., Malaya, M. B., & Gajos, K. Z. (2021). To Trust or to Think: Cognitive Forcing Functions Can Reduce Overreliance on AI in AI-assisted Decision-making. Proceedings of the ACM on Human-Computer Interaction, 5(CSCW1), 1–21. https://doi.org/10.1145/3449287\n\n\nBurton, J. W., Lopez-Lopez, E., Hechtlinger, S., Rahwan, Z., Aeschbach, S., Bakker, M. A., Becker, J. A., Berditchevskaia, A., Berger, J., Brinkmann, L., Flek, L., Herzog, S. M., Huang, S., Kapoor, S., Narayanan, A., Nussberger, A.-M., Yasseri, T., Nickl, P., Almaatouq, A., … Hertwig, R. (2024). How large language models can reshape collective intelligence. Nature Human Behaviour, 1–13. https://doi.org/10.1038/s41562-024-01959-9\n\n\nCarlebach, N., & Yeung, N. (2023). Flexible use of confidence to guide advice requests. Cognition, 230, 105264. https://doi.org/10.1016/j.cognition.2022.105264\n\n\nCarter, W., & Wynne, K. T. (2024). Integrating artificial intelligence into team decision-making: Toward a theory of AI–human team effectiveness. European Management Review. https://doi.org/10.1111/emre.12685\n\n\nChiang, C.-W., Lu, Z., Li, Z., & Yin, M. (2024). Enhancing AI-Assisted Group Decision Making through LLM-Powered Devil’s Advocate. Proceedings of the 29th International Conference on Intelligent User Interfaces, 103–119. https://doi.org/10.1145/3640543.3645199\n\n\nChuang, Y.-S., Harlalka, N., Suresh, S., Goyal, A., Hawkins, R., Yang, S., Shah, D., Hu, J., & Rogers, T. T. (2024). The Wisdom of Partisan Crowds: Comparing Collective Intelligence in Humans and LLM-based Agents.\n\n\nCollins, K. M., Sucholutsky, I., Bhatt, U., Chandra, K., Wong, L., Lee, M., Zhang, C. E., Zhi-Xuan, T., Ho, M., Mansinghka, V., Weller, A., Tenenbaum, J. B., & Griffiths, T. L. (2024). Building machines that learn and think with people. Nature Human Behaviour, 8(10), 1851–1863. https://doi.org/10.1038/s41562-024-01991-9\n\n\nCui, H., & Yasseri, T. (2024). AI-enhanced collective intelligence. Patterns, 5(11), 101074. https://doi.org/10.1016/j.patter.2024.101074\n\n\nFlores, P., Rong, G., & Cowley, B. (2024). Information foraging in human-ChatGPT interactions: Factors of computational thinking dissociate exploration and exploitation. Proceedings of the Annual Meeting of the Cognitive Science Society, 46.\n\n\nHinsz, V. B., Tindale, R. S., & Vollrath, D. A. (1997). The emerging conceptualization of groups as information processors. Psychological Bulletin, 121(1), 43–64. https://doi.org/10.1037/0033-2909.121.1.43\n\n\nJoosten, J., Bilgram, V., Hahn, A., & Totzek, D. (2024). Comparing the Ideation Quality of Humans With Generative Artificial Intelligence. IEEE Engineering Management Review, 52(2), 153–164. https://doi.org/10.1109/EMR.2024.3353338\n\n\nKumar, A., Tham, R.-H. M., & Steyvers, M. (2024). Assessing the Impact of Differing Perspectives in Advice-Taking Behavior. https://doi.org/10.31234/osf.io/seqjr\n\n\nKumar, H., Vincentius, J., Jordan, E., & Anderson, A. (2024). Human Creativity in the Age of LLMs: Randomized Experiments on Divergent and Convergent Thinking (arXiv:2410.03703). arXiv. https://arxiv.org/abs/2410.03703\n\n\nLiang, G., Sloane, J. F., Donkin, C., & Newell, B. R. (2022). Adapting to the algorithm: How accuracy comparisons promote the use of a decision aid. Cognitive Research: Principles and Implications, 7(1), 14. https://doi.org/10.1186/s41235-022-00364-y\n\n\nLu, Z., Amin Mahmoo, S. H., Li, Z., & Yin, M. (2024). Mix and Match: Characterizing Heterogeneous Human Behavior in AI-assisted Decision Making. Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, 12, 95–104. https://doi.org/10.1609/hcomp.v12i1.31604\n\n\nMa, S., Chen, Q., Wang, X., Zheng, C., Peng, Z., Yin, M., & Ma, X. (2024). Towards Human-AI Deliberation: Design and Evaluation of LLM-Empowered Deliberative AI for AI-Assisted Decision-Making (arXiv:2403.16812). arXiv. https://arxiv.org/abs/2403.16812\n\n\nMarjieh, R., Gokhale, A., Bullo, F., & Griffiths, T. L. (2024). Task Allocation in Teams as a Multi-Armed Bandit.\n\n\nMcNeese, N. J., Flathmann, C., O’Neill, T. A., & Salas, E. (2023). Stepping out of the shadow of human-human teaming: Crafting a unique identity for human-autonomy teams. Computers in Human Behavior, 148, 107874. https://doi.org/10.1016/j.chb.2023.107874\n\n\nMeincke, L., Girotra, K., Nave, G., Terwiesch, C., & Ulrich, K. T. (2024). Using Large Language Models for Idea Generation in Innovation. https://doi.org/Meincke, Lennart and Girotra, Karan and Nave, Gideon and Terwiesch, Christian and Ulrich, Karl T., Using Large Language Models for Idea Generation in Innovation (September 07, 2024). The Wharton School Research Paper Forthcoming, Available at SSRN: https://ssrn.com/abstract=4526071 or http://dx.doi.org/10.2139/ssrn.4526071\n\n\nNarayanan, S., Yu, G., Ho, C.-J., & Yin, M. (2023). How does Value Similarity affect Human Reliance in AI-Assisted Ethical Decision Making? Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society, 49–57. https://doi.org/10.1145/3600211.3604709\n\n\nNishida, Y., Shimojo, S., & Hayashi, Y. (2024). Conversational Agent Dynamics with Minority Opinion and Cognitive Conflict in Small-Group Decision-Making. Japanese Psychological Research. https://doi.org/10.1111/jpr.12552\n\n\nPescetelli, N., & Yeung, N. (2021). The role of decision confidence in advice-taking and trust formation. Journal of Experimental Psychology: General, 150(3), 507–526. https://doi.org/10.1037/xge0000960\n\n\nRadivojevic, K., Clark, N., & Brenner, P. (2024). LLMs Among Us: Generative AI Participating in Digital Discourse. Proceedings of the AAAI Symposium Series, 3(1), 209–218. https://doi.org/10.1609/aaaiss.v3i1.31202\n\n\nRastogi, C., Leqi, L., Holstein, K., & Heidari, H. (2023). A Taxonomy of Human and ML Strengths in Decision-Making to Investigate Human-ML Complementarity. Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, 11, 127–139. https://doi.org/10.1609/hcomp.v11i1.27554\n\n\nSharma, N., Liao, Q. V., & Xiao, Z. (2024). Generative Echo Chamber? Effect of LLM-Powered Search Systems on Diverse Information Seeking. Proceedings of the CHI Conference on Human Factors in Computing Systems, 1–17. https://doi.org/10.1145/3613904.3642459\n\n\nShin, M., Kim, J., van Opheusden, B., & Griffiths, T. L. (2023). Superhuman artificial intelligence can improve human decision-making by increasing novelty. Proceedings of the National Academy of Sciences, 120(12), e2214840120. https://doi.org/10.1073/pnas.2214840120\n\n\nSi, C., Yang, D., & Hashimoto, T. (2024). Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers (arXiv:2409.04109). arXiv. https://doi.org/10.48550/arXiv.2409.04109\n\n\nSidji, M., Smith, W., & Rogerson, M. J. (2024). Human-AI Collaboration in Cooperative Games: A Study of Playing Codenames with an LLM Assistant. Proc. ACM Hum.-Comput. Interact., 8(CHI PLAY), 316:1–316:25. https://doi.org/10.1145/3677081\n\n\nSpatharioti, S. E., Rothschild, D. M., Goldstein, D. G., & Hofman, J. M. (2023). Comparing Traditional and LLM-based Search for Consumer Choice: A Randomized Experiment (arXiv:2307.03744). arXiv. https://doi.org/10.48550/arXiv.2307.03744\n\n\nStadler, M., Bannert, M., & Sailer, M. (2024). Cognitive ease at a cost: LLMs reduce mental effort but compromise depth in student scientific inquiry. Computers in Human Behavior, 160, 108386. https://doi.org/10.1016/j.chb.2024.108386\n\n\nSteyvers, M., Tejeda, H., Kumar, A., Belem, C., Karny, S., Hu, X., Mayer, L., & Smyth, P. (2024). The Calibration Gap between Model and Human Confidence in Large Language Models (arXiv:2401.13835). arXiv. https://arxiv.org/abs/2401.13835\n\n\nTessler, M. H., Bakker, M. A., Jarrett, D., Sheahan, H., Chadwick, M. J., Koster, R., Evans, G., Campbell-Gillingham, L., Collins, T., Parkes, D. C., Botvinick, M., & Summerfield, C. (2024). AI can help humans find common ground in democratic deliberation. Science, 386(6719), eadq2852. https://doi.org/10.1126/science.adq2852\n\n\nTsirtsis, S., Rodriguez, M. G., & Gerstenberg, T. (2024). Towards a computational model of responsibility judgments in sequential human-AI collaboration. https://doi.org/10.31234/osf.io/m4yad\n\n\nUeshima, A., & Takikawa, H. (2024). Discovering Novel Social Preferences Using Simple Artificial Neural Networks. Collabra: Psychology, 10(1), 121234. https://doi.org/10.1525/collabra.121234\n\n\nVaccaro, M., Almaatouq, A., & Malone, T. (2024). When combinations of humans and AI are useful: A systematic review and meta-analysis. Nature Human Behaviour, 1–11. https://doi.org/10.1038/s41562-024-02024-1\n\n\nWang, R., Zhou, X., Qiu, L., Chang, J. C., Bragg, J., & Zhang, A. X. (2024). Social-RAG: Retrieving from Group Interactions to Socially Ground Proactive AI Generation to Group Preferences (arXiv:2411.02353). arXiv. https://doi.org/10.48550/arXiv.2411.02353\n\n\nWegner, D. M. (1987). Transactive Memory: A Contemporary Analysis of the Group Mind. In B. Mullen & G. R. Goethals (Eds.), Theories of Group Behavior (pp. 185–208). Springer. https://doi.org/10.1007/978-1-4612-4634-3_9\n\n\nYan, B., Hollingshead, A. B., Alexander, K. S., Cruz, I., & Shaikh, S. J. (2021). Communication in Transactive Memory Systems: A Review and Multidimensional Network Perspective. Small Group Research, 52(1), 3–32. https://doi.org/10.1177/1046496420967764\n\n\nYang, Z., Xu, X., Yao, B., Rogers, E., Zhang, S., Intille, S., Shara, N., Gao, G. G., & Wang, D. (2024). Talk2Care: An LLM-based Voice Assistant for Communication between Healthcare Providers and Older Adults. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 8(2), 1–35. https://doi.org/10.1145/3659625\n\n\nYen, R., Sultanum, N., & Zhao, J. (2024). To Search or To Gen? Exploring the Synergy between Generative AI and Web Search in Programming. Extended Abstracts of the CHI Conference on Human Factors in Computing Systems, 1–8. https://doi.org/10.1145/3613905.3650867\n\n\nZheng, C., Zhang, Y., Huang, Z., Shi, C., Xu, M., & Ma, X. (2024). DiscipLink: Unfolding Interdisciplinary Information Seeking Process via Human-AI Co-Exploration. Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology, 1–20. https://doi.org/10.1145/3654777.3676366\n\n\nZhu, J.-Q., Yan, H., & Griffiths, T. L. (2024). Language Models Trained to do Arithmetic Predict Human Risky and Intertemporal Choice (arXiv:2405.19313). arXiv. https://arxiv.org/abs/2405.19313\n\n\nZvelebilova, J., Savage, S., & Riedl, C. (2024). Collective Attention in Human-AI Teams (arXiv:2407.17489). arXiv. https://doi.org/10.48550/arXiv.2407.17489"
  }
]