[
  {
    "objectID": "tables.html",
    "href": "tables.html",
    "title": "Tables",
    "section": "",
    "text": "Here are 5 potential table ideas for the chapter:\nHere’s a detailed version of Table 1 (Types of AI Roles), which I think would be most valuable:\nTable 1: AI Roles in Group Decision-Making\nThis table would help readers understand the different ways AI can be integrated into group decision-making, while highlighting key considerations for each role type."
  },
  {
    "objectID": "tables.html#table-1-ai-systems-in-group-decision-making-research",
    "href": "tables.html#table-1-ai-systems-in-group-decision-making-research",
    "title": "Tables",
    "section": "Table 1: AI Systems in Group Decision-Making Research",
    "text": "Table 1: AI Systems in Group Decision-Making Research\n\n\n\n\n\n\n\n\n\n\n\n\nAI Type\nCore Capabilities\nKey Examples\nPrimary Applications\nLimitations\nRepresentative Studies\nPerformance Metrics\n\n\n\n\nLarge Language Models\n- Natural language processing- Context-aware reasoning- Dynamic conversation- Knowledge synthesis\n- GPT-4- Claude- LLaMA\n- Group facilitation- Consensus building- Devil’s advocacy- Information synthesis\n- Hallucinations- Inconsistency- Limited domain expertise- Opaque reasoning\n- Tessler et al. (2024)- Chiang et al. (2024)- Collins et al. (2024)\n- Consensus quality- Discussion depth- Information integration\n\n\nDomain-Specific AI\n- Specialized analysis- Pattern recognition- Optimization- Prediction\n- Medical diagnosis systems- Risk assessment tools- Resource allocation algorithms\n- Clinical decision support- Task allocation- Risk analysis- Performance optimization\n- Limited flexibility- Narrow scope- Rigid decision rules- Poor generalization\n- Bienefeld et al. (2023)- Marjieh et al. (2024)- Swaroop et al. (2024)\n- Task accuracy- Efficiency gains- Error reduction\n\n\nHybrid Systems\n- Multi-modal processing- Integrated reasoning- Adaptive responses- Cross-domain learning\n- LLM+expert systems- Multi-agent frameworks- Integrated decision platforms\n- Complex problem solving- Multi-stakeholder decisions- Dynamic environments\n- Integration challenges- System complexity- Coordination issues\n- Gao et al. (2024)- Burton et al. (2024)- Ma et al. (2024)\n- Solution quality- Adaptation speed- Integration effectiveness\n\n\nInteractive Agents\n- Real-time response- Behavioral adaptation- User modeling- Interface management\n- Conversational agents- Adaptive interfaces- Personalized assistants\n- Team coordination- Information sharing- Process guidance\n- Limited social awareness- Interface constraints- Adaptation lag\n- Yang et al. (2024)- Kumar et al. (2024)- Lu et al. (2024)\n- User satisfaction- Interaction quality- Adaptation accuracy"
  },
  {
    "objectID": "tables.html#table-2-cognitive-biases-in-human-ai-decision-making-2",
    "href": "tables.html#table-2-cognitive-biases-in-human-ai-decision-making-2",
    "title": "Tables",
    "section": "Table 2: Cognitive Biases in Human-AI Decision Making",
    "text": "Table 2: Cognitive Biases in Human-AI Decision Making\n\n\n\n\n\n\n\n\n\n\n\n\nBias Category\nSpecific Biases\nDescription\nManifestation in Human-AI Teams\nMitigation Strategies\nKey Studies\nImpact on Decision Quality\n\n\n\n\nInformation Processing\n- Confirmation bias- Availability bias- Anchoring effect\nSystematic errors in how information is gathered and weighted\n- Over-reliance on AI-provided information- Selective attention to AI suggestions\n- Structured information review- Multiple perspective consideration- Cognitive forcing functions\n- Cheung et al. (2024)- Hagendorff et al. (2023)- Bucinca et al. (2021)\nHigh - Affects core decision processes\n\n\nAI-Specific\n- Automation bias- Algorithm aversion- AI authority bias\nBiases specific to human interaction with AI systems\n- Uncritical acceptance of AI suggestions- Systematic distrust of AI\n- Training in AI capabilities- Transparency in AI reasoning- Balanced reliance strategies\n- Westphal et al. (2023)- Narayanan et al. (2023)- Tsirtsis et al. (2024)\nHigh - Directly impacts AI utilization\n\n\nSocial-Cognitive\n- Group polarization- Conformity bias- Status quo bias\nBiases emerging from social dynamics\n- Amplification of group biases by AI- Resistance to AI-suggested changes\n- Diverse perspective inclusion- Structured disagreement- Devil’s advocate approaches\n- Chuang et al. (2024)- Nishida et al. (2024)- Tessler et al. (2024)\nMedium - Affects group dynamics\n\n\nDecision Context\n- Time pressure bias- Complexity aversion- Risk perception bias\nBiases related to decision environment\n- Rushed integration of AI input- Oversimplification of AI suggestions\n- Time management strategies- Complexity reduction tools- Risk assessment frameworks\n- Swaroop et al. (2024)- Stadler et al. (2024)- Bhatia et al. (2024)\nMedium - Context dependent"
  },
  {
    "objectID": "tables.html#table-3-research-methods-in-human-ai-group-decision-making",
    "href": "tables.html#table-3-research-methods-in-human-ai-group-decision-making",
    "title": "Tables",
    "section": "Table 3: Research Methods in Human-AI Group Decision Making",
    "text": "Table 3: Research Methods in Human-AI Group Decision Making\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMethod Type\nStudy Design\nSample Characteristics\nTask Types\nKey Measures\nAnalysis Approaches\nStrengths\nLimitations\nRepresentative Studies\n\n\n\n\nLaboratory Experiments\n- Controlled conditions- Between/within subjects- Factorial designs\n- Small groups (2-5)- Student/general population- N=100-300 typically\n- Classification- Problem solving- Resource allocation- Group consensus\n- Decision accuracy- Response times- Process measures- User satisfaction\n- ANOVA/MANOVA- Regression analysis- Path modeling- Mediation analysis\n- High internal validity- Precise measurement- Causal inference- Process tracking\n- Limited ecological validity- Simplified tasks- Short timeframes- Selection bias\n- Bucinca et al. (2021)- Chiang et al. (2024)- Kumar et al. (2024)\n\n\nField Studies\n- Natural settings- Longitudinal designs- Case studies\n- Professional teams- Varied group sizes- Domain experts\n- Medical decisions- Project management- Emergency response- Strategic planning\n- Team performance- System usage- Communication patterns- Organizational outcomes\n- Mixed methods- Time series analysis- Network analysis- Content analysis\n- High external validity- Real outcomes- Rich context- Long-term effects\n- Less control- Multiple confounds- Small samples- Resource intensive\n- Bienefeld et al. (2023)- Yang et al. (2024)- Ma et al. (2024)\n\n\nOnline Experiments\n- Web-based platforms- Remote participation- Large-scale designs\n- Large samples- Diverse demographics- N&gt;500 typical\n- Text analysis- Opinion formation- Collective intelligence- Decision aggregation\n- Group agreement- Information flow- Participant behavior- System interaction\n- Computational modeling- Text analytics- Network analysis- Machine learning\n- Large samples- Ecological validity- Diverse participants- Cost effective\n- Less control- Attention issues- Technical constraints- Participation quality\n- Tessler et al. (2024)- Chuang et al. (2024)- Radivojevic et al. (2024)\n\n\nSimulation Studies\n- Agent-based models- System dynamics- Monte Carlo methods\n- Computational agents- Scalable populations- Parameter spaces\n- Strategic interaction- Resource distribution- Network evolution- System optimization\n- Emergent patterns- System efficiency- Optimization metrics- Stability measures\n- Agent-based modeling- System dynamics- Network simulation- Optimization analysis\n- Systematic variation- Large scale testing- Parameter control- Theory development\n- Simplified behavior- Limited realism- Parameter sensitivity- Validation challenges\n- Marjieh et al. (2024)- Gao et al. (2024)- Lu et al. (2024)"
  },
  {
    "objectID": "tables.html#table-1-ai-types-in-decision-making-tasks",
    "href": "tables.html#table-1-ai-types-in-decision-making-tasks",
    "title": "Tables",
    "section": "Table 1: AI Types in Decision-Making Tasks",
    "text": "Table 1: AI Types in Decision-Making Tasks\n\n\n\n\n\n\n\n\n\n\nAI Type\nKey Capabilities\nLimitations\nBest Application Contexts\nExample Studies\n\n\n\n\nRule-Based Systems\nExplicitly programmed logic, predictable behavior, high transparency\nBrittle, difficult to adapt to novel situations, limited learning capacity\nWell-defined tasks with clear rules, automated decision-making in stable environments\n[Find examples if applicable - potentially older systems]\n\n\nMachine Learning (ML)\nLearns patterns from data, adaptable, can handle complex relationships\nRequires large datasets, potential for bias, can be opaque (“black box”)\nPrediction, classification, personalized recommendations, tasks with complex patterns\n[Many examples throughout the chapter; could list a few representative ones from different domains]\n\n\nDeep Learning (DL)\nLearns hierarchical representations, excels in complex pattern recognition\nComputationally intensive, requires even larger datasets, interpretability challenges\nImage recognition, natural language processing, complex data analysis\n[Examples of DL models used in specific studies in the chapter]\n\n\nLarge Language Models (LLMs)\nNatural language understanding and generation, context-aware reasoning\nProne to hallucinations, bias amplification, can be computationally expensive\nFacilitating discussions, summarizing information, creative writing, generating explanations, text-based interactions\nChiang et al. (2024); Tessler et al. (2024); Burton et al. (2024); many others\n\n\nReinforcement Learning (RL)\nLearns through trial and error, optimizes for long-term goals, adaptable to dynamic environments\nCan be unstable, requires careful reward design, exploration-exploitation trade-off\nAutonomous agents, game playing, robotics, personalized learning, sequential decision-making tasks\nCallaway et al. (2022); Marjieh et al. (2024) [Focus on RL-specific studies if any are discussed]\n\n\nHybrid Approaches\nCombines strengths of different AI types (e.g., LLMs with rule-based systems or DL)\nIntegration complexity, potential for conflicts between different AI components\nMulti-faceted decision-making tasks requiring both adaptability and precision, complex tasks requiring multiple AI capabilities\nWu et al. (2024), Gao et al. (2024) [Focus on those that clearly combine multiple AI approaches]"
  },
  {
    "objectID": "tables.html#table-2-cognitive-biases-in-human-ai-decision-making-4",
    "href": "tables.html#table-2-cognitive-biases-in-human-ai-decision-making-4",
    "title": "Tables",
    "section": "Table 2: Cognitive Biases in Human-AI Decision Making",
    "text": "Table 2: Cognitive Biases in Human-AI Decision Making\n\n\n\n\n\n\n\n\n\n\nBias\nDescription\nImpact on Decision-Making\nMitigation Strategies\nExample Studies\n\n\n\n\nConfirmation Bias\nFavoring information that confirms existing beliefs, potentially ignoring contradictory evidence.\nReinforces pre-existing beliefs, leading to suboptimal decisions.\nActively seeking out diverse perspectives, Devil’s advocate techniques, structured analytical techniques.\n[General examples from social psychology literature; perhaps connect to specific AI contexts if relevant in your chapter]\n\n\nAnchoring Bias\nOver-reliance on the first piece of information encountered (the “anchor”), even if irrelevant.\nSkews estimates and judgments toward the anchor.\nProviding multiple anchors, encouraging critical evaluation of initial information, delayed feedback.\nHagendorff et al. (2023); Nguyen (2024)\n\n\nAvailability Heuristic\nOverestimating the likelihood of events that are easily recalled (e.g., due to recent exposure).\nLeads to biased risk assessments and prioritization of readily available information.\nEncouraging systematic information search, providing base-rate information, structured risk assessment frameworks.\nMacmillan-Scott & Musolesi (2024); Suri et al. (2024)\n\n\nHindsight Bias\nBelieving, after an event, that one would have accurately predicted it.\nDistorts evaluation of past decisions and learning from experience.\nProspective documentation of predictions, counterfactual reasoning exercises.\n[Examples from decision-making literature; perhaps discuss implications for AI evaluation or training]\n\n\nFraming Effect\nBeing influenced by the way information is presented (e.g., gains vs. losses).\nLeads to inconsistent choices depending on framing, even if outcomes are equivalent.\nPresenting information in multiple frames, highlighting underlying probabilities and outcomes.\nSuri et al. (2024); Nguyen (2024)\n\n\nOverreliance on AI\nExcessive dependence on AI recommendations, even when incorrect or inappropriate.\nDiminishes human critical thinking and oversight.\nCognitive forcing functions (Buçinca et al., 2021), transparency and explainability, calibrated trust.\nBuçinca et al. (2021); Many studies on trust and reliance in the chapter\n\n\nAlgorithm Aversion\nRejecting AI recommendations due to distrust or lack of understanding.\nLeads to underutilization of potentially beneficial AI assistance.\nIncreased transparency, user control over AI, demonstrably successful AI performance.\n[Studies on algorithm aversion; discuss if mentioned in your chapter]\n\n\nAutomation Bias\nOver-trusting automated systems and neglecting to critically evaluate their outputs.\nIncreases the likelihood of accepting incorrect or incomplete AI recommendations.\nTraining on limitations of AI, clear communication of uncertainty, human-in-the-loop systems.\n[Studies on automation bias in different domains]"
  },
  {
    "objectID": "tables.html#table-3-research-methods-in-human-ai-group-decision-making-1",
    "href": "tables.html#table-3-research-methods-in-human-ai-group-decision-making-1",
    "title": "Tables",
    "section": "Table 3: Research Methods in Human-AI Group Decision-Making",
    "text": "Table 3: Research Methods in Human-AI Group Decision-Making\n\n\n\n\n\n\n\n\n\n\n\n\nResearch Method\nSample Characteristics\nTask Types\nKey Measures\nExample Studies\nStrengths\nLimitations\n\n\n\n\nLaboratory Experiments\nSmall groups (2-4), often student participants; typical N = 100-300\nSimplified tasks (e.g., classification, judgment, resource allocation)\nDecision accuracy, response time, trust ratings, reliance on AI, eye-tracking\nChiang et al. (2024); Buçinca et al. (2021)\nHigh control, causal inference, precise measurement of behavior\nLimited ecological validity, often simplified tasks, potential for demand characteristics\n\n\nField Studies\nProfessional teams, real organizational settings, varied group sizes\nComplex, real-world tasks (e.g., medical diagnosis, emergency response, project planning)\nTeam performance, communication patterns, system adoption, qualitative observations\nBienefeld et al. (2023); Yang et al. (2024)\nHigh ecological validity, real-world outcomes\nLess experimental control, smaller sample sizes, difficult to isolate specific AI effects\n\n\nOnline Experiments\nLarge participant pools, diverse demographics, often N &gt; 500\nVaried tasks (e.g., text analysis, opinion formation, consensus building, games)\nGroup agreement, information sharing, bias measures, survey responses\nTessler et al. (2024); Chuang et al. (2024); Sidji et al. (2024)\nLarge samples, cost-effective, access to diverse populations\nLess control over participation, limited interaction depth, potential for self-selection bias\n\n\nSimulation Studies\nComputational agents, mixed human-AI teams, scalable N\nSimulated environments, strategic games, resource distribution, network formation\nEmergent patterns, system optimization, efficiency metrics, agent behavior\nMarjieh et al. (2024); Gao & Zhang (2024)\nSystematic variation, large-scale testing, exploration of complex dynamics\nMay oversimplify human behavior, limited external validity\n\n\nObservational Studies\nNatural groups, longitudinal data, varied contexts\nReal-world collaborative activities (e.g., collaborative writing, decision support usage, team coordination)\nUsage patterns, adaptation over time, natural behaviors, communication analysis\nRadivojevic et al. (2024); Ma et al. (2024)\nNatural behavior, temporal dynamics, rich qualitative data\nNo experimental manipulation, selection effects, difficulty in establishing causality\n\n\nCase Studies\nIn-depth examination of specific instances or examples.\nVaried, often complex and real-world scenarios.\nQualitative data, detailed descriptions of processes and outcomes.\nMany examples throughout chapter; select a few representative cases.\nRich contextual information, exploration of unique phenomena\nLimited generalizability, potential for researcher bias"
  },
  {
    "objectID": "tables.html#table-1-ai-types-in-decision-making-tasks-1",
    "href": "tables.html#table-1-ai-types-in-decision-making-tasks-1",
    "title": "Tables",
    "section": "Table 1: AI Types in Decision-Making Tasks",
    "text": "Table 1: AI Types in Decision-Making Tasks\n\n\n\n\n\n\n\n\n\n\nAI Type\nKey Capabilities\nLimitations\nBest Application Contexts\nExample Studies\n\n\n\n\nRule-Based Systems\nExplicitly programmed logic, predictable behavior, high transparency, computationally efficient\nBrittle, difficult to adapt to novel situations, limited learning capacity\nWell-defined tasks with clear rules, automated decision-making in stable environments, situations where explainability is paramount\n[Provide specific examples if possible; classic expert systems might fit here]\n\n\nMachine Learning (ML)\nLearns patterns from data, adaptable, can handle complex relationships, can discover non-obvious patterns\nRequires large datasets, potential for bias, can be opaque (“black box”), can require significant computational resources depending on complexity\nPrediction, classification, personalized recommendations, tasks with complex patterns where data is abundant\n[Many examples throughout the chapter; could list a few representative ones from different domains, e.g., medical diagnosis, fraud detection]\n\n\nDeep Learning (DL)\nLearns hierarchical representations, excels in complex pattern recognition, can process unstructured data (images, text, audio)\nComputationally intensive, requires even larger datasets, interpretability challenges, “black box” nature can be problematic\nImage recognition, natural language processing, complex data analysis, tasks with high dimensionality\n[Examples of DL models used in specific studies in the chapter, e.g., image-based medical diagnosis systems]\n\n\nLarge Language Models (LLMs)\nNatural language understanding and generation, context-aware reasoning, knowledge retrieval, creative text generation\nProne to hallucinations, bias amplification, can be computationally expensive, limited reasoning abilities in some contexts, can be difficult to control\nFacilitating discussions, summarizing information, creative writing, generating explanations, text-based interactions, tasks requiring natural language understanding\nChiang et al. (2024); Tessler et al. (2024); Burton et al. (2024); many others\n\n\nReinforcement Learning (RL)\nLearns through trial and error, optimizes for long-term goals, adaptable to dynamic environments, can learn optimal strategies in complex situations\nCan be unstable, requires careful reward design, exploration-exploitation trade-off, computationally expensive, can be difficult to interpret\nAutonomous agents, game playing, robotics, personalized learning, sequential decision-making tasks, resource allocation problems\nCallaway et al. (2022); Marjieh et al. (2024) [Focus on RL-specific studies if any are discussed]\n\n\nHybrid Approaches\nCombines strengths of different AI types (e.g., LLMs with rule-based systems or DL), can leverage specialized AI modules for specific sub-tasks\nIntegration complexity, potential for conflicts between different AI components, requires careful design and coordination\nMulti-faceted decision-making tasks requiring both adaptability and precision, complex tasks requiring multiple AI capabilities, tasks where different AI strengths can be combined synergistically\nWu et al. (2024), Gao et al. (2024) [Focus on those that clearly combine multiple AI approaches]"
  },
  {
    "objectID": "tables.html#table-2-cognitive-biases-in-human-ai-decision-making-5",
    "href": "tables.html#table-2-cognitive-biases-in-human-ai-decision-making-5",
    "title": "Tables",
    "section": "Table 2: Cognitive Biases in Human-AI Decision Making",
    "text": "Table 2: Cognitive Biases in Human-AI Decision Making\n\n\n\n\n\n\n\n\n\n\nBias\nDescription\nImpact on Decision-Making\nMitigation Strategies\nExample Studies\n\n\n\n\nConfirmation Bias\nFavoring information that confirms existing beliefs, potentially ignoring contradictory evidence.\nReinforces pre-existing beliefs, leading to suboptimal or biased decisions.\nActively seeking out diverse perspectives, Devil’s advocate techniques, structured analytical techniques, critical thinking training\nNickerson (1998) [Provide specific examples in AI contexts from your chapter, if any]\n\n\nAnchoring Bias\nOver-reliance on the first piece of information encountered (the “anchor”), even if irrelevant.\nSkews estimates and judgments toward the anchor, hindering objective evaluation.\nProviding multiple anchors, encouraging critical evaluation of initial information, delayed feedback, using ranges instead of point estimates\nHagendorff et al. (2023); Nguyen (2024)\n\n\nAvailability Heuristic\nOverestimating the likelihood of events that are easily recalled (e.g., due to recent exposure or vividness).\nLeads to biased risk assessments and prioritization of readily available information, even if not representative.\nEncouraging systematic information search, providing base-rate information, considering less salient but relevant information, structured risk assessment frameworks\nTversky & Kahneman (1973); [Connect to AI contexts if relevant in your chapter]\n\n\nHindsight Bias\nBelieving, after an event, that one would have accurately predicted it.\nDistorts evaluation of past decisions, hinders learning from experience, leads to overconfidence.\nProspective documentation of predictions, counterfactual reasoning exercises, reflecting on the limitations of one’s own knowledge\nFischhoff (1975); [Discuss implications for AI evaluation or training if relevant]\n\n\nFraming Effect\nBeing influenced by the way information is presented (e.g., gains vs. losses).\nLeads to inconsistent choices depending on framing, even if outcomes are logically equivalent.\nPresenting information in multiple frames, highlighting underlying probabilities and outcomes, focusing on objective consequences rather than subjective framing\nTversky & Kahneman (1981); Suri et al. (2024)\n\n\nOverreliance on AI\nExcessive dependence on AI recommendations, even when incorrect or inappropriate.\nDiminishes human critical thinking and oversight, can lead to errors and propagation of AI biases.\nCognitive forcing functions (Buçinca et al., 2021), transparency and explainability, calibrated trust, training on AI limitations, clear communication of uncertainty\nBuçinca et al. (2021); Many studies on trust and reliance in the chapter\n\n\nAlgorithm Aversion\nRejecting AI recommendations due to distrust or lack of understanding, even when AI performance is demonstrably better.\nLeads to underutilization of potentially beneficial AI assistance, hinders optimal human-AI collaboration.\nIncreased transparency and explainability, user control over AI, demonstrably successful AI performance, education about AI capabilities and limitations\nDietvorst et al. (2015); [Studies on algorithm aversion; discuss if mentioned in your chapter]\n\n\nAutomation Bias\nOver-trusting automated systems and neglecting to critically evaluate their outputs.\nIncreases the likelihood of accepting incorrect or incomplete AI recommendations, especially in complex or time-pressured situations.\nTraining on limitations of AI, clear communication of uncertainty, human-in-the-loop systems, redundant checks and balances, promoting vigilance and critical evaluation\n[Studies on automation bias in different domains, e.g., Parasuraman & Manzey (2010)]"
  },
  {
    "objectID": "tables.html#table-3-research-methods-in-human-ai-group-decision-making-2",
    "href": "tables.html#table-3-research-methods-in-human-ai-group-decision-making-2",
    "title": "Tables",
    "section": "Table 3: Research Methods in Human-AI Group Decision-Making",
    "text": "Table 3: Research Methods in Human-AI Group Decision-Making\n\n\n\n\n\n\n\n\n\n\n\n\nResearch Method\nSample Characteristics\nTask Types\nKey Measures\nExample Studies\nStrengths\nLimitations\n\n\n\n\nLaboratory Experiments\nSmall groups (2-4), often student participants; typical N = 100-300\nSimplified, controlled tasks (e.g., classification, judgment, resource allocation, logic puzzles)\nDecision accuracy, response time, trust ratings, reliance on AI, eye-tracking, think-aloud protocols, physiological measures\nChiang et al. (2024); Buçinca et al. (2021); Swaroop et al. (2024)\nHigh control, causal inference, precise measurement of behavior\nLimited ecological validity, often simplified tasks, potential for demand characteristics, limited generalizability\n\n\nField Studies\nProfessional teams, real organizational settings, varied group sizes\nComplex, real-world tasks (e.g., medical diagnosis, emergency response, project planning, incident response)\nTeam performance, communication patterns, system adoption, qualitative observations (interviews, focus groups), workflow analysis\nBienefeld et al. (2023); Yang et al. (2024)\nHigh ecological validity, real-world outcomes\nLess experimental control, smaller sample sizes, difficult to isolate specific AI effects, potential for confounding variables\n\n\nOnline Experiments\nLarge participant pools, diverse demographics, often N &gt; 500\nVaried tasks (e.g., text analysis, opinion formation, consensus building, games, surveys, simulated work tasks)\nGroup agreement, information sharing, bias measures, survey responses, behavioral logs, communication content analysis\nTessler et al. (2024); Chuang et al. (2024); Sidji et al. (2024); Nishida et al. (2024)\nLarge samples, cost-effective, access to diverse populations, can study large-scale dynamics\nLess control over participation, limited interaction depth, potential for self-selection bias, difficulty ensuring data quality\n\n\nSimulation Studies\nComputational agents, mixed human-AI teams, scalable N\nSimulated environments, strategic games (e.g., negotiation games), resource distribution, network formation\nEmergent patterns, system optimization, efficiency metrics, agent behavior, communication patterns, social dynamics\nMarjieh et al. (2024); Gao et al. (2024); Abdelnabi et al. (2023)\nSystematic variation, large-scale testing, exploration of complex dynamics, can isolate specific mechanisms\nMay oversimplify human behavior, limited external validity, assumptions of the simulation model can influence results\n\n\nObservational Studies\nNatural groups, longitudinal data, varied contexts\nReal-world collaborative activities (e.g., collaborative writing, decision support usage, team coordination, online discussions)\nUsage patterns, adaptation over time, natural behaviors, communication analysis, qualitative observations\nRadivojevic et al. (2024); Ma et al. (2024)\nNatural behavior, temporal dynamics, rich qualitative data, can study evolving interactions\nNo experimental manipulation, selection effects, difficulty in establishing causality, observer bias\n\n\nCase Studies\nIn-depth examination of specific instances or examples.\nVaried, often complex and real-world scenarios.\nQualitative data, detailed descriptions of processes and outcomes, analysis of specific events or interactions\nMany examples throughout chapter; select a few representative cases.\nRich contextual information, exploration of unique phenomena, can generate hypotheses\nLimited generalizability, potential for researcher bias, difficult to replicate"
  },
  {
    "objectID": "v1.html",
    "href": "v1.html",
    "title": "AI and Group Decision Making: An Information Processing Perspective",
    "section": "",
    "text": "(Steyvers & Kumar, 2024)\n(Lai et al., 2023)\n(Burton et al., 2024)\n(Rastogi et al., 2023)\n\n…."
  },
  {
    "objectID": "v1.html#introduction",
    "href": "v1.html#introduction",
    "title": "AI and Group Decision Making: An Information Processing Perspective",
    "section": "",
    "text": "(Steyvers & Kumar, 2024)\n(Lai et al., 2023)\n(Burton et al., 2024)\n(Rastogi et al., 2023)\n\n…."
  },
  {
    "objectID": "v1.html#inputs",
    "href": "v1.html#inputs",
    "title": "AI and Group Decision Making: An Information Processing Perspective",
    "section": "Inputs",
    "text": "Inputs\n..\n\nGroup Member Roles\nDeciding how best to assign team members to roles is crucial in group decision-making, particularly when learning who is best suited for what role within a team. Marjieh et al. (2024) explore how humans allocate tasks within teams comprising both human and AI agents to maximize overall performance. The central theme of their research is understanding the mechanisms by which individuals discern and act upon their own strengths and those of their team members in a dynamic task allocation setting. In their experimental paradigm, participants had to repeatedly allocate three different types of tasks (visual, auditory, and lexical tasks) between themselves and two AI agents. Unbeknownst to participants, each AI agent was configured to have high competence (70% success rate) in one task type but low competence (15% success rate) in others.\n\nRecent advances in large language models have dramatically expanded the potential roles of AI in group decision-making, enabling AI agents to move beyond simple advisory functions to serve as mediators, devil’s advocates, and active discussion participants\nChiang et al. (2024) investigated the potential of Large Language Models (LLMs) to act as devil’s advocates in AI-assisted group decision-making - in the hopes of fostering more critical engagement with AI assistance. In their experimental task, participants were first individually trained on the relationship between defendant profiles and recidivism. For each defendant, participants were also shown the prediction of a reccomendation AI model (RiskComp). Participants were then sorted into groups of three, where they reviewed and discussed novel defendant profiles, before making a group recidivism assessment. In the group stage, the reccomendations from the RiskComp model were biased against a subset of the defendants (black defendants with low prior crime counts). Of interest was whether the inclusion of an LLM-based devil’s advocate in the group discussions could help mitigate the bias introduced by the RiskComp AI model (note that the LLM devils advocate and RiskComp AI are separate AI models). The experimental manipulation consisted of four variants of an LLM-based devil’s advocate using, varying both the target of objection (challenging either RiskComp recommendations or majority group opinions) and the level of interactivity (static one-time comments versus dynamic engagement throughout the discussions). Their findings revealed that the dynamic devil’s advocate led to higher decision accuracy and improved discernment of when to trust the RiskComp model’s advice.\n\n(Marjieh et al., 2024)\n(Kumar et al., 2024)\n(Lu et al., 2024)\n(McNeese et al., 2023)"
  },
  {
    "objectID": "v1.html#information-processing",
    "href": "v1.html#information-processing",
    "title": "AI and Group Decision Making: An Information Processing Perspective",
    "section": "Information Processing",
    "text": "Information Processing\n\nInformation Search\n\n(Gao & Zhang, 2024)\n\n…\n\n\nCommunication; information sharing\nTransactive memory systems (TMS) represent a critical aspect of group cognition, referring to the shared understanding within a group regarding the distribution of knowledge and expertise among its members (Wegner, 1987). A well-functioning TMS enables team members not only to know who possesses specific knowledge but also to access and share this distributed expertise efficiently.\nBienefeld et al. (2023) conducted an observational study to examine the role of transactive memory systems and speaking-up behaviors in human-AI teams within an intensive care unit (ICU) setting. In this study, ICU physicians and nurses, divded into groups of four, who collaborated with an AI agent named “Autovent.” Autovent is an auto-adaptive ventilator system that autonomously manages patient ventilation by processing continuous, individualized data streams. Participants, all with a minimum of six months’ experience using Autovent, engaged in simulated clinical scenarios that required diagnosing and treating critically ill patients. Using behavioral coding of video recordings, the researchers analyzed how team members accessed information from both human teammates and the AI system, investigating how these human-human and human-ai interactions related to subsequent behaviors like hypothesis generation and speaking up with concerns. The researchers found that in higher-performing teams, accessing knowledge from the AI agent was positively correlated with developing new hypotheses and increased speaking-up behavior. Conversely, accessing information from human team members was negatively associated with these behaviors, regardless of team performance. These results suggest that AI systems may serve as unique knowledge repositories that help teams overcome some of the social barriers that typically inhibit information sharing and voice behaviors in purely human teams.\nBastola et al. (2024) further explored the potential of AI-mediated communication by examining how an LLM-based Smart Reply (LSR) system could impact collaborative performance in professional settings. They developed a system utilizing ChatGPT to generate context-aware, personalized responses during workplace interactions, aiming to reduce the cognitive effort required for message composition in multitasking scenarios. In their study, participants engaged in a cognitively demanding Dual N-back task while managing scheduling activities via Google Calendar and responding to simulated co-workers on Slack. The findings indicated that the use of the LSR system not only improved work performance—evidenced by higher accuracy in the N-back task—but also increased messaging efficiency and reduced cognitive load, as participants could more readily focus on primary tasks without the distraction of composing responses. However, it is important to note that participants expressed concerns about the appropriateness and accuracy of AI-generated messages, as well as issues related to trust and privacy. Thus, while AI-mediated communication tools like the LSR system may facilitate information sharing and alleviate cognitive demands in collaborative work, these benefits must be balanced against potential user experience challenges to fully realize their potential advantages.\n\n(Yang et al., 2024)\n(Ma et al., 2024)\n(Radivojevic et al., 2024)\n(Sidji et al., 2024)\n(Nishida et al., 2024)\n(Chuang et al., 2024)\n\n\n\nShared Mental Models\n\n(Collins et al., 2024)\n\n…\n\n\nCognitive Load\nBuçinca et al. (2021) examined how interface design might influence cognitive engagement with AI recommendations through what they term “cognitive forcing functions.” Drawing on dual-process theory, they implemented three distinct interface interventions (e.g., requiring explicit requests for AI input, mandating initial independent decisions, introducing temporal delays) designed to disrupt automatic processing and promote more analytical engagement with AI suggestions. Their findings demonstrated that while these interventions successfully reduced overreliance on incorrect AI recommendations, they also increased perceived cognitive load and decreased user satisfaction. Of particular methodological interest was their systematic investigation of individual differences in cognitive motivation: participants with high Need for Cognition (NFC) showed substantially greater benefits from these interventions, suggesting that the effectiveness of such cognitive load manipulations may be moderated by individual differences in information processing preferences."
  },
  {
    "objectID": "v1.html#decision-making-output",
    "href": "v1.html#decision-making-output",
    "title": "AI and Group Decision Making: An Information Processing Perspective",
    "section": "Decision-Making Output",
    "text": "Decision-Making Output\n..\n\nConsensus Formation\nTessler et al. (2024) investigated the potential of AI in facilitating consensus formation through their development of the “Habermas Machine” (HM), an LLM-based system fine-tuned to mediate human deliberation. The HM system receives input statements from individual participants, and attempts to generate consensus statements which will maximize group endorsement. The findings revealed that the AI-generated group statements were consistently preferred over comparison statements written by human mediators. Participants rated the AI-mediated statements higher in terms of informativeness, clarity, and lack of bias. This suggests that AI can effectively capture the collective sentiment of a group and articulate it in a way that resonates with its members. Notably, the researchers also verified that the HM system reliably incorporated minority opinions into the consensus statements, preventing dominance by majority perspectives. These results were replicated in a virtual citizens’ assembly with a demographically representative sample of the UK population. The AI-mediated process again resulted in high-quality group statements and facilitated consensus among participants on contentious issues.\n\n\nDecision Accuracy and Confidence\n\n(Becker et al., 2022)\n…"
  },
  {
    "objectID": "v1.html#trust-risk-and-reliance",
    "href": "v1.html#trust-risk-and-reliance",
    "title": "AI and Group Decision Making: An Information Processing Perspective",
    "section": "Trust, Risk and Reliance",
    "text": "Trust, Risk and Reliance\n\nTrust in AI\n\nWestphal et al. (2023)\n(Koehl & Vangsness, 2023)\n(Banerjee et al., 2024)\n\n\n\nReliance\n\n(Narayanan et al., 2023)\n\nRecent work has begun examining how people attribute responsibility in human-AI collaborative contexts where control is shared and actions are interdependent (Tsirtsis et al., 2024). Their study employs a stylized semi-autonomous driving simulation where participants observe how a ‘human agent’ and an ‘AI agent’ collaborate to reach a destination within a time limit. In their setup, the human and AI agents shared control of a vehicle, with each agent having partial and differing knowledge of the environment (i.e., the AI knew about traffic conditions but not road closures, while humans knew about closures but not traffic). Participants observe illustrated simulations of a variety of commute scenarios, and then make judgements about how responsible each agent was for the commute outcome (reaching the destination on time, or not). The study reveals that participants’ responsibility judgments are influenced by factors such as the unexpectedness of an agent’s action, counterfactual simulations of alternative actions, and the actual contribution of each agent to the task outcome.\n\n\nUtilization\nRecent work by Buçinca et al. (2021) presents an innovative approach to addressing overreliance on AI systems through interface design rather than explanation quality. Their study evaluated three “cognitive forcing functions” - interface elements designed to disrupt quick, heuristic processing of AI recommendations. Although these interventions significantly reduced overreliance on incorrect AI recommendations, an important trade-off emerged: interfaces that most effectively prevented overreliance were also rated as most complex and least preferred by users. Moreover, their analysis revealed potential equity concerns, as the interventions provided substantially greater benefits to individuals with high Need for Cognition. These findings suggest that while interface design can effectively modulate AI utilization patterns, careful consideration must be given to both user experience and potential intervention-generated inequalities.\n\n(Cui & Yasseri, 2024)\n(Stadler et al., 2024)\n\n\n\nRisk\n\n(Bhatia, 2024)\n(Zhu et al., 2024)"
  },
  {
    "objectID": "v1.html#tables",
    "href": "v1.html#tables",
    "title": "AI and Group Decision Making: An Information Processing Perspective",
    "section": "Tables",
    "text": "Tables\n\n\n\n\n\nTable 1: Research methods in human-AI decision making\n\n\n\n\n\n\n\n\n\n\n\nResearch Method\nTask Types\nKey Measures\nExample Studies\n\n\n\n\nLaboratory Experiments\nSimplified, controlled tasks (e.g., classification, judgment, resource allocation, logic puzzles)\nDecision accuracy, response time, trust ratings, reliance on AI, eye-tracking, think-aloud protocols, physiological measures\nChiang et al. (2024); Buçinca et al. (2021); Swaroop et al. (2024)\n\n\nField Studies\nComplex, real-world tasks (e.g., medical diagnosis, emergency response, project planning, incident response)\nTeam performance, communication patterns, system adoption, qualitative observations (interviews, focus groups), workflow analysis\nBienefeld et al. (2023); Yang et al. (2024)\n\n\nOnline Experiments\nVaried tasks (e.g., text analysis, opinion formation, consensus building, games, surveys, simulated work tasks)\nGroup agreement, information sharing, bias measures, survey responses, behavioral logs, communication content analysis\nTessler et al. (2024); Chuang et al. (2024); Sidji et al. (2024); Nishida et al. (2024)\n\n\nObservational Studies\nReal-world collaborative activities (e.g., collaborative writing, decision support usage, team coordination, online discussions)\nUsage patterns, adaptation over time, natural behaviors, communication analysis, qualitative observations\nRadivojevic et al. (2024); Ma et al. (2024)\n\n\n\n\n\n\n\n\n\nTable 2: Types of AI used in decision-making tasks\n\n\n\n\n\n\n\n\n\n\n\nAI Type\nKey Capabilities\nPrimary Applications\nRepresentative Studies\n\n\n\n\nLarge Language Models\nNatural language understanding; Open-ended dialogue; Context adaptation; Knowledge synthesis; Abstract reasoning\nGroup facilitation; Information synthesis; Creative ideation; Deliberation support\nTessler et al. (2024); Chiang et al. (2024); Collins et al. (2024); Du et al. (2024)\n\n\nDomain-Specific AI\nExpert knowledge; Precise calculations; Consistent performance; Real-time processing; Specialized optimization\nMedical diagnosis; Resource allocation; Risk assessment; Pattern detection\nBienefeld et al. (2023); Marjieh et al. (2024); Yang et al. (2024)\n\n\nHybrid Systems\nCombined generalist/specialist capabilities; Integrated expertise; Adaptive reasoning; Multi-modal processing\nComplex decision-making; Strategic planning; Knowledge management\nBurton et al. (2024); Ma et al. (2024); Gao et al. (2024)\n\n\nAutonomous Agents\nIndependent operation; Goal-directed behavior; Environmental interaction; Continuous learning\nTeam collaboration; Process automation; Distributed tasks\nMcNeese et al. (2023); Guo et al. (2024); Zhang et al. (2024)\n\n\nRecommender Systems\nPersonalization; Pattern recognition; Preference learning; Information filtering\nInformation curation; Decision support; Resource suggestions\nStadler et al. (2024); Kumar et al. (2024); Swaroop et al. (2024)"
  },
  {
    "objectID": "v1.html#references",
    "href": "v1.html#references",
    "title": "AI and Group Decision Making: An Information Processing Perspective",
    "section": "References",
    "text": "References\n\n\nBanerjee, D., Teso, S., Sayin, B., & Passerini, A. (2024). Learning To Guide Human Decision Makers With Vision-Language Models (arXiv:2403.16501). arXiv. https://arxiv.org/abs/2403.16501\n\n\nBastola, A., Wang, H., Hembree, J., Yadav, P., Gong, Z., Dixon, E., Razi, A., & McNeese, N. (2024). LLM-based Smart Reply (LSR): Enhancing Collaborative Performance with ChatGPT-mediated Smart Reply System (arXiv:2306.11980). arXiv. https://arxiv.org/abs/2306.11980\n\n\nBecker, F., Skirzyński, J., van Opheusden, B., & Lieder, F. (2022). Boosting Human Decision-making with AI-Generated Decision Aids. Computational Brain & Behavior, 5(4), 467–490. https://doi.org/10.1007/s42113-022-00149-y\n\n\nBhatia, S. (2024). Exploring variability in risk taking with large language models. Journal of Experimental Psychology: General, 153(7), 1838–1860. https://doi.org/10.1037/xge0001607\n\n\nBienefeld, N., Kolbe, M., Camen, G., Huser, D., & Buehler, P. K. (2023). Human-AI teaming: Leveraging transactive memory and speaking up for enhanced team effectiveness. Frontiers in Psychology, 14. https://doi.org/10.3389/fpsyg.2023.1208019\n\n\nBuçinca, Z., Malaya, M. B., & Gajos, K. Z. (2021). To Trust or to Think: Cognitive Forcing Functions Can Reduce Overreliance on AI in AI-assisted Decision-making. Proceedings of the ACM on Human-Computer Interaction, 5(CSCW1), 1–21. https://doi.org/10.1145/3449287\n\n\nBurton, J. W., Lopez-Lopez, E., Hechtlinger, S., Rahwan, Z., Aeschbach, S., Bakker, M. A., Becker, J. A., Berditchevskaia, A., Berger, J., Brinkmann, L., Flek, L., Herzog, S. M., Huang, S., Kapoor, S., Narayanan, A., Nussberger, A.-M., Yasseri, T., Nickl, P., Almaatouq, A., … Hertwig, R. (2024). How large language models can reshape collective intelligence. Nature Human Behaviour, 1–13. https://doi.org/10.1038/s41562-024-01959-9\n\n\nChiang, C.-W., Lu, Z., Li, Z., & Yin, M. (2024). Enhancing AI-Assisted Group Decision Making through LLM-Powered Devil’s Advocate. Proceedings of the 29th International Conference on Intelligent User Interfaces, 103–119. https://doi.org/10.1145/3640543.3645199\n\n\nChuang, Y.-S., Harlalka, N., Suresh, S., Goyal, A., Hawkins, R., Yang, S., Shah, D., Hu, J., & Rogers, T. T. (2024). The Wisdom of Partisan Crowds: Comparing Collective Intelligence in Humans and LLM-based Agents.\n\n\nCollins, K. M., Sucholutsky, I., Bhatt, U., Chandra, K., Wong, L., Lee, M., Zhang, C. E., Zhi-Xuan, T., Ho, M., Mansinghka, V., Weller, A., Tenenbaum, J. B., & Griffiths, T. L. (2024). Building machines that learn and think with people. Nature Human Behaviour, 8(10), 1851–1863. https://doi.org/10.1038/s41562-024-01991-9\n\n\nCui, H., & Yasseri, T. (2024). AI-enhanced collective intelligence. Patterns, 5(11), 101074. https://doi.org/10.1016/j.patter.2024.101074\n\n\nGao, H., & Zhang, Y. (2024). Memory Sharing for Large Language Model based Agents (arXiv:2404.09982). arXiv. https://arxiv.org/abs/2404.09982\n\n\nKoehl, D., & Vangsness, L. (2023). Measuring Latent Trust Patterns in Large Language Models in the Context of Human-AI Teaming. Proceedings of the Human Factors and Ergonomics Society Annual Meeting, 67. https://doi.org/10.1177/21695067231192869\n\n\nKumar, A., Tham, R.-H. M., & Steyvers, M. (2024). Assessing the Impact of Differing Perspectives in Advice-Taking Behavior. https://doi.org/10.31234/osf.io/seqjr\n\n\nLai, V., Chen, C., Smith-Renner, A., Liao, Q. V., & Tan, C. (2023). Towards a Science of Human-AI Decision Making: An Overview of Design Space in Empirical Human-Subject Studies. 2023 ACM Conference on Fairness, Accountability, and Transparency, 1369–1385. https://doi.org/10.1145/3593013.3594087\n\n\nLu, Z., Amin Mahmoo, S. H., Li, Z., & Yin, M. (2024). Mix and Match: Characterizing Heterogeneous Human Behavior in AI-assisted Decision Making. Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, 12, 95–104. https://doi.org/10.1609/hcomp.v12i1.31604\n\n\nMa, S., Chen, Q., Wang, X., Zheng, C., Peng, Z., Yin, M., & Ma, X. (2024). Towards Human-AI Deliberation: Design and Evaluation of LLM-Empowered Deliberative AI for AI-Assisted Decision-Making (arXiv:2403.16812). arXiv. https://arxiv.org/abs/2403.16812\n\n\nMarjieh, R., Gokhale, A., Bullo, F., & Griffiths, T. L. (2024). Task Allocation in Teams as a Multi-Armed Bandit.\n\n\nMcNeese, N. J., Flathmann, C., O’Neill, T. A., & Salas, E. (2023). Stepping out of the shadow of human-human teaming: Crafting a unique identity for human-autonomy teams. Computers in Human Behavior, 148, 107874. https://doi.org/10.1016/j.chb.2023.107874\n\n\nNarayanan, S., Yu, G., Ho, C.-J., & Yin, M. (2023). How does Value Similarity affect Human Reliance in AI-Assisted Ethical Decision Making? Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society, 49–57. https://doi.org/10.1145/3600211.3604709\n\n\nNishida, Y., Shimojo, S., & Hayashi, Y. (2024). Conversational Agent Dynamics with Minority Opinion and Cognitive Conflict in Small-Group Decision-Making. Japanese Psychological Research. https://doi.org/10.1111/jpr.12552\n\n\nRadivojevic, K., Clark, N., & Brenner, P. (2024). LLMs Among Us: Generative AI Participating in Digital Discourse. Proceedings of the AAAI Symposium Series, 3(1), 209–218. https://doi.org/10.1609/aaaiss.v3i1.31202\n\n\nRastogi, C., Leqi, L., Holstein, K., & Heidari, H. (2023). A Taxonomy of Human and ML Strengths in Decision-Making to Investigate Human-ML Complementarity. Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, 11, 127–139. https://doi.org/10.1609/hcomp.v11i1.27554\n\n\nSidji, M., Smith, W., & Rogerson, M. J. (2024). Human-AI Collaboration in Cooperative Games: A Study of Playing Codenames with an LLM Assistant. Proc. ACM Hum.-Comput. Interact., 8(CHI PLAY), 316:1–316:25. https://doi.org/10.1145/3677081\n\n\nStadler, M., Bannert, M., & Sailer, M. (2024). Cognitive ease at a cost: LLMs reduce mental effort but compromise depth in student scientific inquiry. Computers in Human Behavior, 160, 108386. https://doi.org/10.1016/j.chb.2024.108386\n\n\nSteyvers, M., & Kumar, A. (2024). Three Challenges for AI-Assisted Decision-Making. Perspectives on Psychological Science, 19(5), 722–734. https://doi.org/10.1177/17456916231181102\n\n\nTessler, M. H., Bakker, M. A., Jarrett, D., Sheahan, H., Chadwick, M. J., Koster, R., Evans, G., Campbell-Gillingham, L., Collins, T., Parkes, D. C., Botvinick, M., & Summerfield, C. (2024). AI can help humans find common ground in democratic deliberation. Science, 386(6719), eadq2852. https://doi.org/10.1126/science.adq2852\n\n\nTsirtsis, S., Rodriguez, M. G., & Gerstenberg, T. (2024). Towards a computational model of responsibility judgments in sequential human-AI collaboration. https://doi.org/10.31234/osf.io/m4yad\n\n\nWegner, D. M. (1987). Transactive Memory: A Contemporary Analysis of the Group Mind. In B. Mullen & G. R. Goethals (Eds.), Theories of Group Behavior (pp. 185–208). Springer. https://doi.org/10.1007/978-1-4612-4634-3_9\n\n\nWestphal, M., Vössing, M., Satzger, G., Yom-Tov, G. B., & Rafaeli, A. (2023). Decision control and explanations in human-AI collaboration: Improving user perceptions and compliance. Computers in Human Behavior, 144, 107714. https://doi.org/10.1016/j.chb.2023.107714\n\n\nYang, Z., Xu, X., Yao, B., Rogers, E., Zhang, S., Intille, S., Shara, N., Gao, G. G., & Wang, D. (2024). Talk2Care: An LLM-based Voice Assistant for Communication between Healthcare Providers and Older Adults. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 8(2), 1–35. https://doi.org/10.1145/3659625\n\n\nZhu, J.-Q., Yan, H., & Griffiths, T. L. (2024). Language Models Trained to do Arithmetic Predict Human Risky and Intertemporal Choice (arXiv:2405.19313). arXiv. https://arxiv.org/abs/2405.19313"
  }
]