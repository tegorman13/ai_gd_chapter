<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.13">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Thomas E. Gorman">
<meta name="author" content="Torsten Reimer">
<meta name="dcterms.date" content="2025-02-03">

<title>AI and Group Decision Making: An Information Processing Perspective – GD Chapter</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-66ab7fd5e73b7f0a764e0d49b3e29ab1.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-7a0e04ec82617701cd34937292aa414e.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-6bd9cfa162949bde0a231f530c97869d.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark-7f2ec24aee834e62cd2de1bda3d4642b.min.css" rel="prefetch" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="site_libs/quarto-contrib/fontawesome6-0.1.0/all.css" rel="stylesheet">
<link href="site_libs/quarto-contrib/fontawesome6-0.1.0/latex-fontsize.css" rel="stylesheet">
<script src="site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="Assets/style.css">
</head>

<body>

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#inputs-resources-and-factors-shaping-ai-assisted-group-decision-making" id="toc-inputs-resources-and-factors-shaping-ai-assisted-group-decision-making" class="nav-link" data-scroll-target="#inputs-resources-and-factors-shaping-ai-assisted-group-decision-making">Inputs: Resources and Factors Shaping AI-Assisted Group Decision Making</a>
  <ul class="collapse">
  <li><a href="#member-characteristics-and-roles" id="toc-member-characteristics-and-roles" class="nav-link" data-scroll-target="#member-characteristics-and-roles">Member Characteristics and Roles</a></li>
  <li><a href="#initial-trust-expertise-assumptions-and-biases" id="toc-initial-trust-expertise-assumptions-and-biases" class="nav-link" data-scroll-target="#initial-trust-expertise-assumptions-and-biases">Initial Trust, Expertise Assumptions, and Biases</a></li>
  <li><a href="#future-research-directions-inputs" id="toc-future-research-directions-inputs" class="nav-link" data-scroll-target="#future-research-directions-inputs">Future Research Directions (Inputs)</a></li>
  </ul></li>
  <li><a href="#information-processing" id="toc-information-processing" class="nav-link" data-scroll-target="#information-processing">Information Processing</a>
  <ul class="collapse">
  <li><a href="#information-search" id="toc-information-search" class="nav-link" data-scroll-target="#information-search">Information Search</a></li>
  <li><a href="#communication-and-information-sharing" id="toc-communication-and-information-sharing" class="nav-link" data-scroll-target="#communication-and-information-sharing">Communication and information sharing</a></li>
  <li><a href="#information-integration-and-consensus-formation" id="toc-information-integration-and-consensus-formation" class="nav-link" data-scroll-target="#information-integration-and-consensus-formation">Information Integration and Consensus Formation</a></li>
  <li><a href="#future-research-directions-processing" id="toc-future-research-directions-processing" class="nav-link" data-scroll-target="#future-research-directions-processing">Future Research Directions (Processing)</a></li>
  </ul></li>
  <li><a href="#output" id="toc-output" class="nav-link" data-scroll-target="#output">Output</a>
  <ul class="collapse">
  <li><a href="#improved-decision-quality-and-complementarity" id="toc-improved-decision-quality-and-complementarity" class="nav-link" data-scroll-target="#improved-decision-quality-and-complementarity">Improved Decision Quality and Complementarity</a></li>
  <li><a href="#trust-and-reliance" id="toc-trust-and-reliance" class="nav-link" data-scroll-target="#trust-and-reliance">Trust and Reliance</a></li>
  <li><a href="#responsibility" id="toc-responsibility" class="nav-link" data-scroll-target="#responsibility">Responsibility</a></li>
  <li><a href="#future-research-directions-output" id="toc-future-research-directions-output" class="nav-link" data-scroll-target="#future-research-directions-output">Future Research Directions (Output)</a></li>
  <li><a href="#discussion" id="toc-discussion" class="nav-link" data-scroll-target="#discussion">Discussion</a></li>
  <li><a href="#future-directions" id="toc-future-directions" class="nav-link" data-scroll-target="#future-directions">Future Directions</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/tegorman13/ai_gd_chapter/blob/main/working.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/tegorman13/ai_gd_chapter/edit/main/working.qmd" class="toc-action"><i class="bi empty"></i>Edit this page</a></li><li><a href="https://github.com/tegorman13/ai_gd_chapter/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div><div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="gd_chapter_v4.md"><i class="bi bi-file-code"></i>Github (GFM)</a></li><li><a href="gd_chapter_v4_hikmah.docx"><i class="bi bi-file-word"></i>MS Word (hikmah-manuscript)</a></li><li><a href="gd_chapter_v4.pdf"><i class="bi bi-file-pdf"></i>PDF (hikmah-manuscript)</a></li><li><a href="gd_ai_chapter_v4.docx"><i class="bi bi-file-word"></i>MS Word</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content column-page-left" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">AI and Group Decision Making: An Information Processing Perspective</h1>
</div>


<div class="quarto-title-meta-author column-page-left">
  <div class="quarto-title-meta-heading">Authors</div>
  <div class="quarto-title-meta-heading">Affiliations</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author"><a href="https://tegorman13.github.io/">Thomas E. Gorman</a> <a href="mailto:tegorman@purdue.edu" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> <a href="https://orcid.org/0000-0001-5366-5442" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            <a href="https://web.ics.purdue.edu/~treimer/">
            Communication and Cognition Lab, Purdue University, USA
            </a>
          </p>
        <p class="affiliation">
            <a href="https://cla.purdue.edu/about/college-initiatives/research-academy/">
            College of Liberal Arts Research Academy
            </a>
          </p>
      </div>
    <div class="quarto-title-meta-contents">
    <p class="author"><a href="https://web.ics.purdue.edu/~treimer/">Torsten Reimer</a> <a href="mailto:treimer@purdue.edu" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> <a href="https://orcid.org/0000-0002-7419-0076" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            <a href="https://cla.purdue.edu/communication/">
            Communication and Cognition Lab, Purdue University, USA
            </a>
          </p>
      </div>
  </div>

<div class="quarto-title-meta column-page-left">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">February 3, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Artificial Intelligence (AI) is becoming a central component of group decision-making processes across a range of domains. From healthcare to finance, education to policymaking, AI systems are being integrated into group decision-making processes, offering new avenues for enhancing efficiency, accuracy, and innovation <span class="citation" data-cites="burtonHowLargeLanguage2024 carterIntegratingArtificialIntelligence2024 banihaniAIDecisionmakingProcess2024">(<a href="#ref-banihaniAIDecisionmakingProcess2024" role="doc-biblioref">BaniHani et al., 2024</a>; <a href="#ref-burtonHowLargeLanguage2024" role="doc-biblioref">Burton et al., 2024</a>; <a href="#ref-carterIntegratingArtificialIntelligence2024" role="doc-biblioref">Carter &amp; Wynne, 2024</a>)</span>. This growing collaboration between humans and AI brings forth both significant opportunities and pressing challenges. On one hand, AI systems offer the potential to enhance information processing efficiency, improve decision accuracy, and streamline communication within teams. On the other hand, the complexities inherent in human-AI interactions—such as issues of trust and over-reliance, susceptibility to cognitive biases, erosion of critical thinking skills, lack of transparency in AI algorithms, and ethical concerns regarding accountability and fairness.</p>
<p>The use of AI in group settings has evolved from basic decision-support tools to more sophisticated roles, such as collaborative partners capable of generating novel insights. Large language models (LLMs), for instance, can facilitate collective intelligence by synthesizing information, generating alternative solutions, and even mediating group discussions​​. However, the extent to which AI enhances group performance remains context-dependent. Recent meta-analyses reveal that human-AI collaboration can lead to either augmentation of individual performance or to performance decrements <span class="citation" data-cites="vaccaroWhenCombinationsHumans2024">(<a href="#ref-vaccaroWhenCombinationsHumans2024" role="doc-biblioref">Vaccaro et al., 2024</a>)</span>, depending on the task and interaction design​​.</p>
<p>To navigate these complexities, this chapter adopts the information processing framework as a lens for examining AI-assisted group decision-making <span class="citation" data-cites="hinszEmergingConceptualizationGroups1997">(<a href="#ref-hinszEmergingConceptualizationGroups1997" role="doc-biblioref">Hinsz et al., 1997</a>)</span>. This framework provides a structured method to analyze how AI systems interact with human cognitive processes at each stage of decision-making. By dissecting the inputs (information acquisition and sharing), the processing mechanisms (interpretation and integration of information), and the outputs (decisions and actions), we can gain insights into the opportunities and challenges presented by AI integration.</p>
<p>Key questions we will seek to address within this framework::</p>
<ul>
<li><p><strong>Inputs</strong>: How does AI influence the way groups search for, gather, and share information? For example, AI can augment information search through advanced data retrieval but may also introduce biases based on the algorithms’ training data.</p></li>
<li><p><strong>Processing</strong>: In what ways do AI systems affect the interpretation and integration of information within the group? AI can facilitate complex data analysis but might obscure the reasoning process through opaque algorithms, impacting the group’s shared understanding.</p></li>
<li><p><strong>Outputs</strong>: How do AI recommendations influence the group’s final decisions and actions? The reliance on AI outputs raises questions about trust, accountability, and the potential diminishment of human agency.</p></li>
</ul>
<p>By examining these questions through the lens of the information processing framework, we can better understand the complex interplay between humans and AI in group decision-making contexts and identify strategies for optimizing the benefits of AI-assisted collaboration while mitigating its risks.</p>
<div style="page-break-after: always;"></div>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><i class="fa-regular fa-lightbulb" aria-label="lightbulb"></i> <strong>Box 1: Glossary of Terms</strong></p>
<p><strong>Complementarity.</strong> In the context of human-AI teams, complementarity refers to the synergistic integration of human and artificial intelligence, leveraging the unique strengths of each to achieve performance outcomes that exceed those attainable by either humans or AI systems operating in isolation (Steyvers et al., 2022). Effective complementarity involves a balanced division of labor and mutual enhancement of capabilities.</p>
<p><strong>Information Processing Framework.</strong> A theoretical framework in cognitive psychology that conceptualizes the human mind, and by extension, groups, as systems that acquire, process, store, retrieve, and transmit information, akin to computational systems (Hinsz et al., 1997). This framework provides a structured lens for analyzing decision-making as a sequence of stages, from inputs to outputs.</p>
<p><strong>Large Language Model (LLM).</strong> A sophisticated type of artificial intelligence algorithm characterized by its use of deep learning techniques and training on massive datasets to enable the understanding, generation, and prediction of human language. LLMs, such as GPT-4, are foundational to many contemporary AI applications in decision support and communication.</p>
<p><strong>Overreliance.</strong> A cognitive bias characterized by excessive trust in and dependence on AI recommendations, often leading to the uncritical acceptance of AI outputs, even when they are flawed or suboptimal. Overreliance can undermine human vigilance and critical evaluation in AI-assisted decision-making contexts.</p>
<p><strong>Transactive Memory Systems (TMS).</strong> A system of distributed knowledge within a group, where members develop specialized knowledge and rely on each other for access to that knowledge.</p>
</div>
</div>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="Assets/ip3.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Information processing framework illustrating the key inputs, processing mechanisms, and outputs in human-AI group decision-making."><img src="Assets/ip3.png" class="img-fluid figure-img" style="width:105.0%" alt="Information processing framework illustrating the key inputs, processing mechanisms, and outputs in human-AI group decision-making."></a></p>
<figcaption>Information processing framework illustrating the key inputs, processing mechanisms, and outputs in human-AI group decision-making.</figcaption>
</figure>
</div>
</section>
<section id="inputs-resources-and-factors-shaping-ai-assisted-group-decision-making" class="level2">
<h2 class="anchored" data-anchor-id="inputs-resources-and-factors-shaping-ai-assisted-group-decision-making">Inputs: Resources and Factors Shaping AI-Assisted Group Decision Making</h2>
<p>The input stage is characterized by a multifaceted array of resources and factors, each contributing uniquely to the subsequent processing and ultimate decision outputs. These inputs can be broadly categorized into human member characteristics, AI system attributes, task and contextual factors, and initial trust, expertise assumptions, and biases, each playing a critical role in shaping the dynamics of human-AI collaboration.</p>
<section id="member-characteristics-and-roles" class="level3">
<h3 class="anchored" data-anchor-id="member-characteristics-and-roles">Member Characteristics and Roles</h3>
<p><strong>Roles and Functionality of AI</strong>.The roles and functionality assigned to AI systems significantly impact group dynamics and decision-making processes <span class="citation" data-cites="berrettaDefiningHumanAITeaming2023 carterIntegratingArtificialIntelligence2024 duanUnderstandingProcessesTrust2025 guoDecisionTheoreticFramework2024 bennettHumanPerformanceCompetitive2023 nomuraCollaborativeBrainstormingHumans2024">(<a href="#ref-bennettHumanPerformanceCompetitive2023" role="doc-biblioref">Bennett et al., 2023</a>; <a href="#ref-berrettaDefiningHumanAITeaming2023" role="doc-biblioref">Berretta et al., 2023</a>; <a href="#ref-carterIntegratingArtificialIntelligence2024" role="doc-biblioref">Carter &amp; Wynne, 2024</a>; <a href="#ref-duanUnderstandingProcessesTrust2025" role="doc-biblioref">Duan et al., 2025</a>; <a href="#ref-guoDecisionTheoreticFramework2024" role="doc-biblioref">Guo et al., 2024</a>; <a href="#ref-nomuraCollaborativeBrainstormingHumans2024" role="doc-biblioref">Nomura et al., 2024</a>)</span>. AI can act as an advisor, providing recommendations and insights; a peer collaborator, actively participating in discussions; a devil’s advocate, challenging the group’s assumptions; a mediator, facilitating consensus formation; or even a manager, coordinating tasks and assigning roles. The role of the AI will affect how humans interact with it and how much responsibility is assigned to the AI in the decision-making process.</p>
<p>For instance, an AI acting as an advisor might provide information and suggestions, which group members then evaluate and integrate into their decision-making process. In contrast, an AI acting as a peer collaborator might actively engage in discussions, contributing its own opinions and analyses. A devil’s advocate AI could challenge the group’s consensus, promoting critical evaluation and potentially reducing groupthink. A mediator AI might help to synthesize diverse perspectives and facilitate agreement, as demonstrated by the Habermas Machine <span class="citation" data-cites="tesslerAICanHelp2024">(<a href="#ref-tesslerAICanHelp2024" role="doc-biblioref">Tessler et al., 2024</a>)</span>. The proactivity or reactiveness of the AI also influences its role and impact <span class="citation" data-cites="diebelWhenAIBasedAgents2025">(<a href="#ref-diebelWhenAIBasedAgents2025" role="doc-biblioref">Diebel et al., 2025</a>)</span>. A proactive AI might initiate suggestions or interventions, while a reactive AI would only respond to user prompts. Proactive AI can enhance efficiency but may also reduce user control and satisfaction, particularly if the AI’s actions are perceived as intrusive or misaligned with user needs.</p>
<p>Deciding how best to assign team members to roles is crucial in group decision-making, particularly when learning who is best suited for what role within a team. <span class="citation" data-cites="marjiehTaskAllocationTeams2024">Marjieh, Gokhale, et al. (<a href="#ref-marjiehTaskAllocationTeams2024" role="doc-biblioref">2024</a>)</span> explore how humans allocate tasks within teams comprising both human and AI agents to maximize overall performance. The central theme of their research is understanding the mechanisms by which individuals discern and act upon their own strengths and those of their team members in a dynamic task allocation setting. In their experimental paradigm, participants had to repeatedly allocate three different types of tasks (visual, auditory, and lexical tasks) between themselves and two AI agents. Unbeknownst to participants, each AI agent was configured to have high competence (70% success rate) in one task type but low competence (15% success rate) in others.</p>
<p>Building upon this, <span class="citation" data-cites="mcneeseSteppingOutShadow2023">McNeese et al. (<a href="#ref-mcneeseSteppingOutShadow2023" role="doc-biblioref">2023</a>)</span> argue that human-autonomy teams (HATs) should be recognized as distinct from traditional human teams. They emphasize that HATs should not strive to replicate human-human team dynamics but instead should leverage the unique capabilities of AI agents. The authors propose several research trajectories to advance our understanding of HATs, including exploring diverse teaming models, redefining roles for AI teammates, expanding communication modalities, focusing on AI behavior design, developing specialized training, and emphasizing teamwork in AI design. These insights highlight the necessity of adjusting our approaches to team composition and role assignment when AI agents are involved, ensuring that both human and AI strengths are optimized in the decision-making process.</p>
<p>Recent advances in large language models have dramatically expanded the potential roles of AI in group decision-making, enabling AI agents to move beyond simple advisory functions to serve as mediators, devil’s advocates, and active discussion participants. <span class="citation" data-cites="chiangEnhancingAIAssistedGroup2024">Chiang et al. (<a href="#ref-chiangEnhancingAIAssistedGroup2024" role="doc-biblioref">2024</a>)</span> investigated the potential of Large Language Models (LLMs) to act as devil’s advocates in AI-assisted group decision-making - in the hopes of fostering more critical engagement with AI assistance. In their experimental task, participants were first individually trained on the relationship between defendant profiles and recidivism. For each defendant, participants were also shown the prediction of a recommendation AI model (RiskComp). Participants were then sorted into groups of three, where they reviewed and discussed novel defendant profiles, before making a group recidivism assessment. In the group stage, the recommendations from the RiskComp model were biased against a subset of the defendants (black defendants with low prior crime counts). Of interest was whether the inclusion of an LLM-based devil’s advocate in the group discussions could help mitigate the bias introduced by the RiskComp AI model (note that the LLM devils advocate and RiskComp AI are separate AI models). The experimental manipulation consisted of four variants of an LLM-based devil’s advocate using, varying both the target of objection (challenging either RiskComp recommendations or majority group opinions) and the level of interactivity (static one-time comments versus dynamic engagement throughout the discussions). Their findings revealed that the dynamic devil’s advocate led to higher decision accuracy and improved discernment of when to trust the RiskComp model’s advice.</p>
<p><strong>Human Member Characteristics.</strong> The composition of human teams, in terms of expertise, cognitive styles, and diversity, represents a foundational input layer. For instance, the expertise and knowledge that individual members bring to a team are crucial determinants of the quality of information available for processing <span class="citation" data-cites="aggarwalSelfbeliefsTransactiveMemory2023">(<a href="#ref-aggarwalSelfbeliefsTransactiveMemory2023" role="doc-biblioref">Aggarwal et al., 2023</a>)</span>. Furthermore, cognitive diversity, encompassing varied approaches to problem-solving and information processing, can enrich the group’s cognitive resources, potentially enhancing its ability to tackle complex problems <span class="citation" data-cites="aggarwalSelfbeliefsTransactiveMemory2023">(<a href="#ref-aggarwalSelfbeliefsTransactiveMemory2023" role="doc-biblioref">Aggarwal et al., 2023</a>)</span>. However, the benefits of diversity are not unqualified, as factors such as team longevity and task complexity can moderate the diversity-performance relationship <span class="citation" data-cites="wallrichRelationshipTeamDiversity2024">(<a href="#ref-wallrichRelationshipTeamDiversity2024" role="doc-biblioref">Wallrich et al., 2024</a>)</span>. In addition to expertise and cognitive styles, the roles assigned to team members, particularly in hierarchical structures, shape how information is accessed, shared, and utilized within the group <span class="citation" data-cites="narayananHowDoesValue2023 marjiehLargeLanguageModels2024">(<a href="#ref-marjiehLargeLanguageModels2024" role="doc-biblioref">Marjieh, Sucholutsky, et al., 2024</a>; <a href="#ref-narayananHowDoesValue2023" role="doc-biblioref">S. Narayanan et al., 2023</a>)</span>. Moreover, demographic diversity, while having a statistically significant but practically small overall effect on team performance, can interact with contextual factors to influence group dynamics and outcomes <span class="citation" data-cites="wallrichRelationshipTeamDiversity2024">(<a href="#ref-wallrichRelationshipTeamDiversity2024" role="doc-biblioref">Wallrich et al., 2024</a>)</span>. These varied human characteristics collectively form a crucial input layer, setting the stage for how groups interact with and leverage AI in decision-making processes.</p>
<p><strong>Demographics and Individual Differences</strong>. Demographic factors such as age, gender, and education, as well as individual differences in personality traits, digital affinity, and cultural background, can moderate human-AI interaction <span class="citation" data-cites="diebelWhenAIBasedAgents2025 gerlichAIToolsSociety2025 roeslerNumericVsVerbal2024">(<a href="#ref-diebelWhenAIBasedAgents2025" role="doc-biblioref">Diebel et al., 2025</a>; <a href="#ref-gerlichAIToolsSociety2025" role="doc-biblioref">Gerlich, 2025</a>; <a href="#ref-roeslerNumericVsVerbal2024" role="doc-biblioref">Roesler et al., 2024</a>)</span>. For instance, younger individuals or those with higher digital affinity may be more comfortable integrating AI into their decision-making processes. <span class="citation" data-cites="diebelWhenAIBasedAgents2025">Diebel et al. (<a href="#ref-diebelWhenAIBasedAgents2025" role="doc-biblioref">2025</a>)</span> found that individuals with higher AI knowledge experienced a greater loss of competence-based self-esteem when interacting with proactive AI, indicating that prior experience with AI can shape user perceptions. Cultural background can also play a role, influencing attitudes towards authority, technology, and collaboration <span class="citation" data-cites="chugunovaWeItInterdisciplinary2022">(<a href="#ref-chugunovaWeItInterdisciplinary2022" role="doc-biblioref">Chugunova &amp; Sele, 2022</a>)</span>.</p>
<p><strong>Task and Contextual Factors.</strong> The nature of the decision-making task itself and the broader context within which it is embedded form another essential layer of inputs. Task complexity, for instance, significantly influences the type of information processing required and the potential benefits of AI assistance <span class="citation" data-cites="hamadaWisdomCrowdsCollective2020 eignerDeterminantsLLMassistedDecisionMaking2024">(<a href="#ref-eignerDeterminantsLLMassistedDecisionMaking2024" role="doc-biblioref">Eigner &amp; Händler, 2024</a>; <a href="#ref-hamadaWisdomCrowdsCollective2020" role="doc-biblioref">Hamada et al., 2020</a>)</span>. Complex tasks may necessitate more sophisticated AI support to manage information overload and enhance analytical capabilities. The decision environment, encompassing factors such as time pressure and risk levels, also shapes the input requirements and the dynamics of human-AI interaction <span class="citation" data-cites="zhangInvestigatingAITeammate2023">(<a href="#ref-zhangInvestigatingAITeammate2023" role="doc-biblioref">R. Zhang et al., 2023</a>)</span>. For example, time pressure may alter reliance on AI assistance, as individuals adapt their decision-making strategies to balance speed and accuracy <span class="citation" data-cites="swaroopAccuracyTimeTradeoffsAIAssisted2024">(<a href="#ref-swaroopAccuracyTimeTradeoffsAIAssisted2024" role="doc-biblioref">Swaroop et al., 2024</a>)</span>. Moreover, the specific decision-making setting, whether in healthcare, finance, or policy, introduces unique contextual factors that influence the relevance and effectiveness of AI inputs <span class="citation" data-cites="narayananHowDoesValue2023">(<a href="#ref-narayananHowDoesValue2023" role="doc-biblioref">S. Narayanan et al., 2023</a>)</span>. These task and contextual factors, therefore, represent a critical input layer, moderating the interplay between human and AI contributions.</p>
</section>
<section id="initial-trust-expertise-assumptions-and-biases" class="level3">
<h3 class="anchored" data-anchor-id="initial-trust-expertise-assumptions-and-biases">Initial Trust, Expertise Assumptions, and Biases</h3>
<p>Trust and confidence in AI are dynamic and multifaceted, as highlighted by Hancock et al.&nbsp;(2011), who conducted a meta-analysis showing that trust in automation is influenced by a variety of human, robot, and environmental factors. Li et al.&nbsp;(2025) further explored the dynamic nature of trust, revealing that human self-confidence tends to align with AI confidence, which can affect decision-making strategies. Vodrahalli et al.&nbsp;(2022) emphasized the importance of calibrated trust, where humans accurately assess the reliability of AI advice. The calibration of trust is also seen as critical, as shown by Cecil et al.&nbsp;(2024), who found that explanations did not mitigate the negative impacts of incorrect AI advice, suggesting a complex relationship between trust, reliance, and the perceived reliability of AI.</p>
<ul>
<li>Trust Baselines: Pre-existing attitudes toward AI, perceived reliability, anthropomorphism <span class="citation" data-cites="cuiAIenhancedCollectiveIntelligence2024">(<a href="#ref-cuiAIenhancedCollectiveIntelligence2024" role="doc-biblioref">Cui &amp; Yasseri, 2024</a>)</span></li>
<li>Bias and Training Data: Potential for AI to introduce or amplify biases from training corpora <span class="citation" data-cites="cecilExplainabilityDoesNot2024 bhatiaExploringVariabilityRisk2024">(<a href="#ref-bhatiaExploringVariabilityRisk2024" role="doc-biblioref">Bhatia, 2024</a>; <a href="#ref-cecilExplainabilityDoesNot2024" role="doc-biblioref">Cecil et al., 2024</a>)</span></li>
</ul>
</section>
<section id="future-research-directions-inputs" class="level3">
<h3 class="anchored" data-anchor-id="future-research-directions-inputs">Future Research Directions (Inputs)</h3>
<p>Future research should address several gaps in our understanding of inputs in human-AI group decision-making.</p>
<ol type="1">
<li><strong>Interplay of Input Categories:</strong> A more detailed exploration of the interplay between different input categories is warranted. This includes investigating how human characteristics interact with specific AI system attributes within varying contextual demands. For instance, how does the level of human expertise moderate the impact of AI transparency on trust and reliance? How do different communication modalities influence the integration of AI advice in groups with varying cognitive styles?</li>
<li><strong>Longitudinal Studies:</strong> There is a need for longitudinal studies to investigate the long-term effects of input factors on the evolution of human-AI team dynamics and decision-making strategies over time. How do initial trust, expertise assumptions, and biases change with repeated interactions? How do groups adapt their communication patterns and shared mental models as they gain experience working with AI systems?</li>
<li><strong>Dynamic Role Adjustment:</strong> Research should explore dynamic role adjustment in human-AI teams, where roles are not fixed but can change based on the task demands and the performance of individual team members. This includes developing metrics to evaluate the effectiveness of AI-augmented team structures. Such metrics should go beyond traditional measures of team performance and consider factors like trust, communication quality, and the development of shared mental models.</li>
<li><strong>Personalized AI Systems:</strong> More research is needed on how to design AI systems that can adapt to the cognitive styles and abilities of individual human team members. This includes developing personalized AI interfaces that provide tailored explanations, adjust the level of interactivity, and offer appropriate cognitive support.</li>
</ol>
</section>
</section>
<section id="information-processing" class="level2">
<h2 class="anchored" data-anchor-id="information-processing">Information Processing</h2>
<p>The processing stage reveals the complex mechanisms through which humans and AI interact and transform information into collective decisions.</p>
<section id="information-search" class="level3">
<h3 class="anchored" data-anchor-id="information-search">Information Search</h3>
<p>The information search stage of decision-making, once reliant on human capacity to locate and synthesize data, has been transformed by the advent of artificial intelligence (AI), particularly Large Language Models (LLMs). This section explores how AI reshapes information search, augmenting both data retrieval and synthesis, and fostering idea generation and creative discovery.</p>
<section id="ai-assisted-data-retrieval-and-synthesis" class="level4">
<h4 class="anchored" data-anchor-id="ai-assisted-data-retrieval-and-synthesis">AI-Assisted Data Retrieval and Synthesis</h4>
<p>LLMs significantly enhance the efficiency and comprehensiveness of information gathering, enabling access to a broader knowledge base and deeper insights <span class="citation" data-cites="bouscheryAugmentingHumanInnovation2023">(<a href="#ref-bouscheryAugmentingHumanInnovation2023" role="doc-biblioref">Bouschery et al., 2023</a>)</span>. These models process vast datasets, identifying connections and patterns beyond human capacity. Furthermore, individual differences, such as computational thinking skills, influence how users interact with LLMs, with those possessing higher creativity and algorithmic thinking more effectively leveraging AI-generated content for deeper engagement within a specific information landscape <span class="citation" data-cites="floresInformationForagingHumanChatGPT2024">(<a href="#ref-floresInformationForagingHumanChatGPT2024" role="doc-biblioref">Flores et al., 2024</a>)</span>. Programmers, for example, navigate between traditional web search and generative AI tools, strategically selecting between them based on factors like task familiarity and goal clarity, demonstrating the synergistic use of both resources <span class="citation" data-cites="yenSearchGenExploring2024">(<a href="#ref-yenSearchGenExploring2024" role="doc-biblioref">Yen et al., 2024</a>)</span>. DiscipLink, for instance, uses LLMs to generate exploratory questions across disciplines, automatically expand queries with field-specific terminology, and extract themes from retrieved papers, effectively bridging knowledge gaps in interdisciplinary research <span class="citation" data-cites="zhengDiscipLinkUnfoldingInterdisciplinary2024">(<a href="#ref-zhengDiscipLinkUnfoldingInterdisciplinary2024" role="doc-biblioref">Zheng et al., 2024</a>)</span>. Moreover, AI facilitates advanced techniques like retrieval-augmented generation (RAG), allowing LLMs to access and process real-time information, enhancing the accuracy and relevance of their output <span class="citation" data-cites="siCanLLMsGenerate2024 wangSocialRAGRetrievingGroup2024">(<a href="#ref-siCanLLMsGenerate2024" role="doc-biblioref">Si et al., 2024</a>; <a href="#ref-wangSocialRAGRetrievingGroup2024" role="doc-biblioref">Wang et al., 2024</a>)</span>. This capability empowers decision-makers with synthesized insights from diverse sources, crucial for informed choices across various fields, from scientific research to policy analysis <span class="citation" data-cites="burtonHowLargeLanguage2024">(<a href="#ref-burtonHowLargeLanguage2024" role="doc-biblioref">Burton et al., 2024</a>)</span>.</p>
<p>LLM-based search tools offer natural language interfaces, streamlining complex queries and providing detailed responses, often leading to increased efficiency and user satisfaction <span class="citation" data-cites="spathariotiComparingTraditionalLLMbased2023">(<a href="#ref-spathariotiComparingTraditionalLLMbased2023" role="doc-biblioref">Spatharioti et al., 2023</a>)</span>. However, this ease of use can also lead to overreliance on potentially inaccurate information and decreased critical evaluation, particularly when presented conversationally <span class="citation" data-cites="anderlConversationalPresentationMode2024">(<a href="#ref-anderlConversationalPresentationMode2024" role="doc-biblioref">Anderl et al., 2024</a>)</span>. This can contribute to confirmation bias and the formation of “generative echo chambers,” limiting exposure to diverse perspectives <span class="citation" data-cites="sharmaGenerativeEchoChamber2024">(<a href="#ref-sharmaGenerativeEchoChamber2024" role="doc-biblioref">Sharma et al., 2024</a>)</span>. Furthermore, while LLMs can reduce cognitive load during information seeking, this may come at the cost of deeper learning and engagement with the material, leading to less sophisticated reasoning and argumentation <span class="citation" data-cites="stadlerCognitiveEaseCost2024">(<a href="#ref-stadlerCognitiveEaseCost2024" role="doc-biblioref">Stadler et al., 2024</a>)</span>. Therefore, careful design and implementation are crucial to mitigate these risks and leverage the full potential of LLMs for enhanced information retrieval and synthesis.</p>
</section>
<section id="ai-in-idea-generation-and-creative-discovery" class="level4">
<h4 class="anchored" data-anchor-id="ai-in-idea-generation-and-creative-discovery">AI in Idea Generation and Creative Discovery</h4>
<p>AI’s role extends beyond data retrieval to fostering creative discovery. LLMs act as catalysts, offering alternative perspectives, challenging assumptions, and proposing unexpected connections <span class="citation" data-cites="bouscheryAugmentingHumanInnovation2023">(<a href="#ref-bouscheryAugmentingHumanInnovation2023" role="doc-biblioref">Bouschery et al., 2023</a>)</span>. In structured tasks like semantic search, AI agents enhance group performance by selectively sharing information, amplifying collective intelligence <span class="citation" data-cites="ueshimaDiscoveringNovelSocial2024">(<a href="#ref-ueshimaDiscoveringNovelSocial2024" role="doc-biblioref">Ueshima &amp; Takikawa, 2024</a>)</span>. Studies comparing human and AI-generated ideas reveal a nuanced picture: while LLMs excel at generating ideas with higher average quality (e.g., purchase intent) and even surpassing human experts in novelty <span class="citation" data-cites="joostenComparingIdeationQuality2024 meinckeUsingLargeLanguage2024 siCanLLMsGenerate2024">(<a href="#ref-joostenComparingIdeationQuality2024" role="doc-biblioref">Joosten et al., 2024</a>; <a href="#ref-meinckeUsingLargeLanguage2024" role="doc-biblioref">Meincke et al., 2024</a>; <a href="#ref-siCanLLMsGenerate2024" role="doc-biblioref">Si et al., 2024</a>)</span>, they may exhibit lower feasibility <span class="citation" data-cites="joostenComparingIdeationQuality2024">(<a href="#ref-joostenComparingIdeationQuality2024" role="doc-biblioref">Joosten et al., 2024</a>)</span> and reduced diversity <span class="citation" data-cites="meinckeUsingLargeLanguage2024">(<a href="#ref-meinckeUsingLargeLanguage2024" role="doc-biblioref">Meincke et al., 2024</a>)</span>. This highlights the importance of strategic prompt engineering, as demonstrated by <span class="citation" data-cites="boussiouxCrowdlessFutureGenerative2024">Boussioux et al. (<a href="#ref-boussiouxCrowdlessFutureGenerative2024" role="doc-biblioref">2024</a>)</span>, who found that human-guided prompts—specifically differentiated search (i.e., prompts designed to encourage diverse and varied responses) — enhanced the novelty of LLM-generated solutions while maintaining high value.</p>
<p>The type of AI interaction also significantly influences human creativity. <span class="citation" data-cites="ashkinazeHowAIIdeas2024">Ashkinaze et al. (<a href="#ref-ashkinazeHowAIIdeas2024" role="doc-biblioref">2024</a>)</span> found that exposure to AI-generated ideas increased the diversity of collective ideas without affecting individual creativity. In contrast, <span class="citation" data-cites="kumarHumanCreativityAge2024">Kumar et al. (<a href="#ref-kumarHumanCreativityAge2024" role="doc-biblioref">2024</a>)</span> observed that while providing direct answers had minimal negative impact, exposure to LLM-generated strategies decreased both originality and creative flexibility in subsequent unassisted tasks.</p>
</section>
</section>
<section id="communication-and-information-sharing" class="level3">
<h3 class="anchored" data-anchor-id="communication-and-information-sharing">Communication and information sharing</h3>
<p><strong>AI-Mediated Communication Systems</strong>. AI systems, such as LLM-based Smart Reply (LSR) systems, can mediate communication and information sharing within human-AI teams <span class="citation" data-cites="bastolaLLMbasedSmartReply2024 fortunatiMovingAheadHumanMachine2021 gomezcaballeroHumanAICollaborationNot2024">(<a href="#ref-bastolaLLMbasedSmartReply2024" role="doc-biblioref">Bastola et al., 2024</a>; <a href="#ref-fortunatiMovingAheadHumanMachine2021" role="doc-biblioref">Fortunati &amp; Edwards, 2021</a>; <a href="#ref-gomezcaballeroHumanAICollaborationNot2024" role="doc-biblioref">Gomez Caballero et al., 2024</a>)</span>. The key is to use AI to ensure more efficient information exchange between team members, and also to reduce cognitive load during repetitive tasks. However, AI systems need to be carefully designed to maintain user experience and be respectful of human biases, preferences and agency.</p>
<p><strong>Transactive Memory and Knowledge Sharing</strong>. AI can facilitate transactive memory systems in human-AI teams, acting as knowledge repositories and influencing information access and sharing <span class="citation" data-cites="bienefeldHumanAITeamingLeveraging2023 gaoAligningLLMAgents2024 yanCommunicationTransactiveMemory2021">(<a href="#ref-bienefeldHumanAITeamingLeveraging2023" role="doc-biblioref">Bienefeld et al., 2023</a>; <a href="#ref-gaoAligningLLMAgents2024" role="doc-biblioref">Gao et al., 2024</a>; <a href="#ref-yanCommunicationTransactiveMemory2021" role="doc-biblioref">Yan et al., 2021</a>)</span>. This is not unlike how human transactive memory systems work <span class="citation" data-cites="bienefeldHumanAITeamingLeveraging2023">(<a href="#ref-bienefeldHumanAITeamingLeveraging2023" role="doc-biblioref">Bienefeld et al., 2023</a>)</span>, but the dynamics and limitations of human-AI TMSs are yet to be fully understood, especially in cases where AI may not fully understand the full context or task environment.</p>
<p><strong>Agent Communication Strategies</strong>. The way in which AI agents communicate, including whether they are proactive, sociable, responsive and clear, can affect the flow of information in the group and the trust between team members <span class="citation" data-cites="duanUnderstandingProcessesTrust2025 bennettHumanPerformanceCompetitive2023 nishidaConversationalAgentDynamics2024">(<a href="#ref-bennettHumanPerformanceCompetitive2023" role="doc-biblioref">Bennett et al., 2023</a>; <a href="#ref-duanUnderstandingProcessesTrust2025" role="doc-biblioref">Duan et al., 2025</a>; <a href="#ref-nishidaConversationalAgentDynamics2024" role="doc-biblioref">Nishida et al., 2024</a>)</span>. Different communication styles may be preferable in different contexts, and the human perception of AI communication strategies play a critical role in shaping human expectations for AI teammates <span class="citation" data-cites="zhangCounteractsTestingStereotypical2023">(<a href="#ref-zhangCounteractsTestingStereotypical2023" role="doc-biblioref">D. Zhang et al., 2023</a>)</span>.</p>
<p>Transactive memory systems (TMS) represent a critical aspect of group cognition, referring to the shared understanding within a group regarding the distribution of knowledge and expertise among its members <span class="citation" data-cites="wegnerTransactiveMemoryContemporary1987 yanCommunicationTransactiveMemory2021">(<a href="#ref-wegnerTransactiveMemoryContemporary1987" role="doc-biblioref">Wegner, 1987</a>; <a href="#ref-yanCommunicationTransactiveMemory2021" role="doc-biblioref">Yan et al., 2021</a>)</span>. A well-functioning TMS enables team members not only to know who possesses specific knowledge but also to access and share this distributed expertise efficiently.</p>
<p><span class="citation" data-cites="bienefeldHumanAITeamingLeveraging2023">Bienefeld et al. (<a href="#ref-bienefeldHumanAITeamingLeveraging2023" role="doc-biblioref">2023</a>)</span> conducted an observational study to examine the role of transactive memory systems and speaking-up behaviors in human-AI teams within an intensive care unit (ICU) setting. In this study, ICU physicians and nurses, divided into groups of four, who collaborated with an AI agent named “Autovent.” Autovent is an auto-adaptive ventilator system that autonomously manages patient ventilation by processing continuous, individualized data streams. Participants, all with a minimum of six months’ experience using Autovent, engaged in simulated clinical scenarios that required diagnosing and treating critically ill patients. Using behavioral coding of video recordings, the researchers analyzed how team members accessed information from both human teammates and the AI system, investigating how these human-human and human-ai interactions related to subsequent behaviors like hypothesis generation and speaking up with concerns. The researchers found that in higher-performing teams, accessing knowledge from the AI agent was positively correlated with developing new hypotheses and increased speaking-up behavior. Conversely, accessing information from human team members was negatively associated with these behaviors, regardless of team performance. These results suggest that AI systems may serve as unique knowledge repositories that help teams overcome some of the social barriers that typically inhibit information sharing and voice behaviors in purely human teams.</p>
<p><span class="citation" data-cites="bastolaLLMbasedSmartReply2024">Bastola et al. (<a href="#ref-bastolaLLMbasedSmartReply2024" role="doc-biblioref">2024</a>)</span> further explored the potential of AI-mediated communication by examining how an LLM-based Smart Reply (LSR) system could impact collaborative performance in professional settings. They developed a system utilizing ChatGPT to generate context-aware, personalized responses during workplace interactions, aiming to reduce the cognitive effort required for message composition in multitasking scenarios. In their study, participants engaged in a cognitively demanding Dual N-back task while managing scheduling activities via Google Calendar and responding to simulated co-workers on Slack. The findings indicated that the use of the LSR system not only improved work performance—evidenced by higher accuracy in the N-back task—but also increased messaging efficiency and reduced cognitive load, as participants could more readily focus on primary tasks without the distraction of composing responses. However, it is important to note that participants expressed concerns about the appropriateness and accuracy of AI-generated messages, as well as issues related to trust and privacy. Thus, while AI-mediated communication tools like the LSR system may facilitate information sharing and alleviate cognitive demands in collaborative work, these benefits must be balanced against potential user experience challenges to fully realize their potential advantages.</p>
<ul>
<li><p><span class="citation" data-cites="yangTalk2CareLLMbasedVoice2024">(<a href="#ref-yangTalk2CareLLMbasedVoice2024" role="doc-biblioref">Yang et al., 2024</a>)</span></p></li>
<li><p><span class="citation" data-cites="maHumanAIDeliberationDesign2024">(<a href="#ref-maHumanAIDeliberationDesign2024" role="doc-biblioref">Ma et al., 2024</a>)</span></p></li>
<li><p><span class="citation" data-cites="radivojevicLLMsUsGenerative2024">(<a href="#ref-radivojevicLLMsUsGenerative2024" role="doc-biblioref">Radivojevic et al., 2024</a>)</span></p></li>
<li><p><span class="citation" data-cites="sidjiHumanAICollaborationCooperative2024">(<a href="#ref-sidjiHumanAICollaborationCooperative2024" role="doc-biblioref">Sidji et al., 2024</a>)</span></p></li>
<li><p><span class="citation" data-cites="nishidaConversationalAgentDynamics2024">(<a href="#ref-nishidaConversationalAgentDynamics2024" role="doc-biblioref">Nishida et al., 2024</a>)</span></p></li>
<li><p><span class="citation" data-cites="chuangWisdomPartisanCrowds2024">(<a href="#ref-chuangWisdomPartisanCrowds2024" role="doc-biblioref">Chuang et al., 2024</a>)</span></p></li>
<li><p>AI-Mediated Communication: Smart replies, devil’s advocacy, and AI as a discussion facilitator (Chiang et al., 2024; Duan et al., 2025)</p></li>
<li><p>chat-based collaboration (ChatCollab, Klieger et al., 2024)</p></li>
</ul>
</section>
<section id="information-integration-and-consensus-formation" class="level3">
<h3 class="anchored" data-anchor-id="information-integration-and-consensus-formation">Information Integration and Consensus Formation</h3>
<p><strong>Influence of AI on Deliberation and Argumentation</strong>. AI systems, particularly LLMs, can influence group deliberation, argumentation, and consensus formation. AI can mediate discussions by synthesizing information, presenting different perspectives, and facilitating more balanced conversations <span class="citation" data-cites="argyleLeveragingAIDemocratic2023 chiangEnhancingAIAssistedGroup2024 duLargeLanguageModels2024 tesslerAICanHelp2024">(<a href="#ref-argyleLeveragingAIDemocratic2023" role="doc-biblioref">Argyle et al., 2023</a>; <a href="#ref-chiangEnhancingAIAssistedGroup2024" role="doc-biblioref">Chiang et al., 2024</a>; <a href="#ref-duLargeLanguageModels2024" role="doc-biblioref">Du et al., 2024</a>; <a href="#ref-tesslerAICanHelp2024" role="doc-biblioref">Tessler et al., 2024</a>)</span>. Moreover, AI can act as a ’devil’s advocate <span class="citation" data-cites="chiangEnhancingAIAssistedGroup2024">(<a href="#ref-chiangEnhancingAIAssistedGroup2024" role="doc-biblioref">Chiang et al., 2024</a>)</span>, which enhances the likelihood that a team considers alternatives, and does not prematurely converge on a single idea.</p>
<p><span class="citation" data-cites="tesslerAICanHelp2024">Tessler et al. (<a href="#ref-tesslerAICanHelp2024" role="doc-biblioref">2024</a>)</span> investigated the potential of AI in facilitating consensus formation through their development of the “Habermas Machine” (HM), an LLM-based system fine-tuned to mediate human deliberation. The HM system receives input statements from individual participants, and attempts to generate consensus statements which will maximize group endorsement. The findings revealed that the AI-generated group statements were consistently preferred over comparison statements written by human mediators. Participants rated the AI-mediated statements higher in terms of informativeness, clarity, and lack of bias. This suggests that AI can effectively capture the collective sentiment of a group and articulate it in a way that resonates with its members. Notably, the researchers also verified that the HM system reliably incorporated minority opinions into the consensus statements, preventing dominance by majority perspectives. These results were replicated in a virtual citizens’ assembly with a demographically representative sample of the UK population. The AI-mediated process again resulted in high-quality group statements and facilitated consensus among participants on contentious issues.</p>
<p><strong>Shared Mental Models in Human-AI Teams</strong>. The formation and maintenance of shared mental models (SMMs) in human-AI teams are significantly impacted by team structure <span class="citation" data-cites="collinsBuildingMachinesThat2024 narayananInfluenceHumanAITeam2024">(<a href="#ref-collinsBuildingMachinesThat2024" role="doc-biblioref">Collins et al., 2024</a>; <a href="#ref-narayananInfluenceHumanAITeam2024" role="doc-biblioref">R. Narayanan &amp; Feigh, 2024</a>)</span>. In hierarchical human-AI teams, for example, SMMs are often distributed across the team, rather than held by all members, which requires more sophisticated approaches to communication. A key to successful human-AI teams is ensuring there is a common understanding of team member roles, goals and limitations, and in particular the role of any AI systems.</p>
<p><strong>Cognitive Load and Cognitive Forcing Functions</strong>. Cognitive load impacts human decision-making and the way in which they engage with AI tools <span class="citation" data-cites="bucincaTrustThinkCognitive2021 gerlichAIToolsSociety2025 westphalDecisionControlExplanations2023">(<a href="#ref-bucincaTrustThinkCognitive2021" role="doc-biblioref">Buçinca et al., 2021</a>; <a href="#ref-gerlichAIToolsSociety2025" role="doc-biblioref">Gerlich, 2025</a>; <a href="#ref-westphalDecisionControlExplanations2023" role="doc-biblioref">Westphal et al., 2023</a>)</span>. AI systems may reduce cognitive load, but can also impair critical thinking skills, and must be carefully designed to support users. Cognitive forcing functions <span class="citation" data-cites="bucincaTrustThinkCognitive2021">(<a href="#ref-bucincaTrustThinkCognitive2021" role="doc-biblioref">Buçinca et al., 2021</a>)</span>, or interactions designed to encourage analytical thinking and deeper processing, can promote more effective human-AI interactions. However, there can be trade-offs between effectiveness and usability <span class="citation" data-cites="bucincaContrastiveExplanationsThat2024 westbyCollectiveIntelligenceHumanAI2023">(<a href="#ref-bucincaContrastiveExplanationsThat2024" role="doc-biblioref">Buçinca et al., 2024</a>; <a href="#ref-westbyCollectiveIntelligenceHumanAI2023" role="doc-biblioref">Westby &amp; Riedl, 2023</a>)</span> that must be considered.</p>
<p><span class="citation" data-cites="bucincaTrustThinkCognitive2021">Buçinca et al. (<a href="#ref-bucincaTrustThinkCognitive2021" role="doc-biblioref">2021</a>)</span> examined how interface design might influence cognitive engagement with AI recommendations through what they term “cognitive forcing functions.” Drawing on dual-process theory, they implemented three distinct interface interventions (e.g., requiring explicit requests for AI input, mandating initial independent decisions, introducing temporal delays) designed to disrupt automatic processing and promote more analytical engagement with AI suggestions. Their findings demonstrated that while these interventions successfully reduced overreliance on incorrect AI recommendations, they also increased perceived cognitive load and decreased user satisfaction. Of particular methodological interest was their systematic investigation of individual differences in cognitive motivation: participants with high Need for Cognition (NFC) showed substantially greater benefits from these interventions, suggesting that the effectiveness of such cognitive load manipulations may be moderated by individual differences in information processing preferences.</p>
<ul>
<li>Cognitive offloading effects <span class="citation" data-cites="stadlerCognitiveEaseCost2024">(<a href="#ref-stadlerCognitiveEaseCost2024" role="doc-biblioref">Stadler et al., 2024</a>)</span></li>
</ul>
<p><strong>Human-AI Complementarity Mechanisms</strong>. The conditions under which human and AI capabilities complement each other are essential to study. This includes understanding aspects such as confidence weighting and error correction. By modeling how people weigh information, and how AI can effectively supplement this process, new strategies for promoting more accurate and reliable collective decision-making can be developed. One promising approach for promoting this complementarity involves developing Bayesian models of human-AI interaction <span class="citation" data-cites="steyversBayesianModelingHuman2022">(<a href="#ref-steyversBayesianModelingHuman2022" role="doc-biblioref">Steyvers et al., 2022</a>)</span>.</p>
</section>
<section id="future-research-directions-processing" class="level3">
<h3 class="anchored" data-anchor-id="future-research-directions-processing">Future Research Directions (Processing)</h3>
<p>Future research should address several gaps in our understanding of cognitive processing in human-AI teams.</p>
<ol type="1">
<li><strong>Dynamic Interplay:</strong> A deeper understanding of the dynamic interplay between different processing mechanisms is needed. For instance, investigating how AI-mediated communication shapes shared mental models and how cognitive load influences information integration.</li>
<li><strong>Emotions and Social Factors:</strong> The role of emotions and social factors in human-AI information processing should be explored. How do factors like trust, rapport, and social identity influence the way humans interact with and rely on AI teammates?</li>
<li>Research on how to measure and visualize shared mental models in human-AI teams, how to design AI systems that actively contribute to the development of shared understanding, how to adapt AI behavior based on the evolving mental models of human teammates.</li>
</ol>
</section>
</section>
<section id="output" class="level2">
<h2 class="anchored" data-anchor-id="output">Output</h2>
<p>The output stage of the IPO framework examines the decisions, actions, and consequences resulting from AI-assisted group decision-making. This section analyzes various dimensions of decision outputs, including accuracy, trust, user satisfaction, and broader group-level outcomes.</p>
<section id="improved-decision-quality-and-complementarity" class="level3">
<h3 class="anchored" data-anchor-id="improved-decision-quality-and-complementarity">Improved Decision Quality and Complementarity</h3>
<p>One of the primary goals of integrating AI into group decision-making is to enhance decision quality, and research has shown promising results in this area. AI augmentation has been demonstrated to improve human decision-making across various tasks <span class="citation" data-cites="beckerBoostingHumanDecisionmaking2022">(<a href="#ref-beckerBoostingHumanDecisionmaking2022" role="doc-biblioref">Becker et al., 2022</a>)</span>. In complex domains, such as medical diagnostics, human-AI collectives have been shown to produce more accurate differential diagnoses than either human-only or AI-only groups, highlighting the potential for synergistic gains <span class="citation" data-cites="zollerHumanAICollectivesProduce2024">(<a href="#ref-zollerHumanAICollectivesProduce2024" role="doc-biblioref">Zöller et al., 2024</a>)</span>. The exposure to superhuman AI, as in the game of Go, can also enhance human decision-making by encouraging the exploration of novel strategies, thereby increasing overall performance and innovation <span class="citation" data-cites="shinSuperhumanArtificialIntelligence2023">(<a href="#ref-shinSuperhumanArtificialIntelligence2023" role="doc-biblioref">Shin et al., 2023</a>)</span>.</p>
<p><strong>Factors Influencing Decision Quality.</strong> Several factors influence the quality of decisions in AI-assisted contexts. One critical factor is the accuracy of the AI’s recommendations <span class="citation" data-cites="yinUnderstandingEffectAccuracy2019">(<a href="#ref-yinUnderstandingEffectAccuracy2019" role="doc-biblioref">Yin et al., 2019</a>)</span>. However, accuracy alone is not sufficient; the way AI confidence is communicated also plays a crucial role. Humans do not simply accept or reject AI advice but rather adjust their reliance based on a complex interplay of factors, including perceived AI accuracy, human confidence, and decision context <span class="citation" data-cites="steyversBayesianModelingHuman2022">(<a href="#ref-steyversBayesianModelingHuman2022" role="doc-biblioref">Steyvers et al., 2022</a>)</span>. The alignment of confidence between team members also influences how individuals process and weight the information they have at hand and make decisions based on that information <span class="citation" data-cites="liConfidenceAlignsExploring2025">(<a href="#ref-liConfidenceAlignsExploring2025" role="doc-biblioref">Li et al., 2025</a>)</span>. The alignment of confidence could also impact polarization, as individuals with high self-confidence might reject AI recommendations, while those with lower self-confidence might over-rely, potentially leading to inconsistent reliance and team performance <span class="citation" data-cites="liConfidenceAlignsExploring2025">(<a href="#ref-liConfidenceAlignsExploring2025" role="doc-biblioref">Li et al., 2025</a>)</span>. AI systems optimized for teamwork, rather than just accuracy, can lead to better overall performance <span class="citation" data-cites="bansalDoesWholeExceed2021">(<a href="#ref-bansalDoesWholeExceed2021" role="doc-biblioref">Bansal et al., 2021</a>)</span>. This optimization includes factors like explainability and adaptability, which directly relate to the concepts of complementarity and reliance. The confidence level displayed by an AI and the provided explanation impact the trust level of human decision-makers <span class="citation" data-cites="zhangEffectConfidenceExplanation2020">(<a href="#ref-zhangEffectConfidenceExplanation2020" role="doc-biblioref">Y. Zhang et al., 2020</a>)</span>. When people do not understand the basis of AI recommendations (or believe the AI to be a black box), they are less likely to trust it and therefore less likely to rely on the AI even when it would improve decision quality <span class="citation" data-cites="westphalDecisionControlExplanations2023">(<a href="#ref-westphalDecisionControlExplanations2023" role="doc-biblioref">Westphal et al., 2023</a>)</span>. In this way, lack of explainability presents a risk to decision quality.</p>
<p><strong>Complementarity.</strong> Research indicates that human-AI teams, under certain conditions, can achieve superior decision quality compared to either human-only or AI-only groups, demonstrating the potential for human-AI complementarity <span class="citation" data-cites="zollerHumanAICollectivesProduce2024 steyversBayesianModelingHuman2022">(<a href="#ref-steyversBayesianModelingHuman2022" role="doc-biblioref">Steyvers et al., 2022</a>; <a href="#ref-zollerHumanAICollectivesProduce2024" role="doc-biblioref">Zöller et al., 2024</a>)</span>. This complementarity arises from leveraging the distinct strengths of humans and machines: AI systems excel at processing large datasets and identifying patterns, while humans contribute contextual understanding, ethical considerations, and creative problem-solving abilities <span class="citation" data-cites="canonicoCollectivelyIntelligentTeams2019 carterIntegratingArtificialIntelligence2024">(<a href="#ref-canonicoCollectivelyIntelligentTeams2019" role="doc-biblioref">Canonico et al., 2019</a>; <a href="#ref-carterIntegratingArtificialIntelligence2024" role="doc-biblioref">Carter &amp; Wynne, 2024</a>)</span>. For instance, in medical diagnostics, hybrid human-AI collectives have shown greater accuracy in differential diagnoses due to the diverse error profiles of humans and LLMs <span class="citation" data-cites="zollerHumanAICollectivesProduce2024">(<a href="#ref-zollerHumanAICollectivesProduce2024" role="doc-biblioref">Zöller et al., 2024</a>)</span>. However, achieving this complementarity requires careful consideration of task allocation and team structure <span class="citation" data-cites="zhangYouCompleteMe2022 marjiehLargeLanguageModels2024">(<a href="#ref-marjiehLargeLanguageModels2024" role="doc-biblioref">Marjieh, Sucholutsky, et al., 2024</a>; <a href="#ref-zhangYouCompleteMe2022" role="doc-biblioref">Q. Zhang et al., 2022</a>)</span>. Bayesian modeling approaches have been developed to formally analyze and optimize human-AI complementarity, demonstrating that hybrid combinations can outperform either component alone under specific conditions <span class="citation" data-cites="lemusEmpiricalInvestigationReliance2022 steyversBayesianModelingHuman2022">(<a href="#ref-lemusEmpiricalInvestigationReliance2022" role="doc-biblioref">Lemus et al., 2022</a>; <a href="#ref-steyversBayesianModelingHuman2022" role="doc-biblioref">Steyvers et al., 2022</a>)</span>.</p>
<p>The integration of AI into group decision-making holds significant promise for leveraging the complementary strengths of humans and machines. <span class="citation" data-cites="rastogiTaxonomyHumanML2023">Rastogi et al. (<a href="#ref-rastogiTaxonomyHumanML2023" role="doc-biblioref">2023</a>)</span> proposed a taxonomy to characterize differences in human and machine decision-making, providing a framework for understanding how to combine their unique capabilities optimally. This taxonomy highlights areas where AI can augment human decision processes, such as handling large data sets or identifying patterns beyond human perceptual abilities. <span class="citation" data-cites="beckerBoostingHumanDecisionmaking2022">Becker et al. (<a href="#ref-beckerBoostingHumanDecisionmaking2022" role="doc-biblioref">2022</a>)</span> demonstrated that AI-generated decision aids, when presented as interpretable procedural instructions, can significantly improve human decision-making by promoting more resource-rational strategies. In complex domains, <span class="citation" data-cites="shinSuperhumanArtificialIntelligence2023">Shin et al. (<a href="#ref-shinSuperhumanArtificialIntelligence2023" role="doc-biblioref">2023</a>)</span> showed that exposure to superhuman AI, as in the game of Go, can enhance human decision-making by encouraging the exploration of novel strategies, thereby increasing overall performance and innovation. However, the effectiveness of human-AI collaboration depends on the dynamics of the interaction.</p>
<p>Recent research has revealed complex trade-offs in human-AI team performance that depend heavily on task structure and collaboration dynamics. <span class="citation" data-cites="bennettHumanPerformanceCompetitive2023">Bennett et al. (<a href="#ref-bennettHumanPerformanceCompetitive2023" role="doc-biblioref">2023</a>)</span> found that while both human-human and human-AI teams experienced performance costs relative to theoretical benchmarks, human-human teams showed particular advantages in collaborative versus competitive conditions—an effect that diminished when humans worked with AI partners. This aligns with findings from <span class="citation" data-cites="liangAdaptingAlgorithmHow2022">Liang et al. (<a href="#ref-liangAdaptingAlgorithmHow2022" role="doc-biblioref">2022</a>)</span> showing that humans can learn to selectively rely on AI assistance based on task difficulty, but often require explicit feedback and training to optimize this collaboration. The reduced collaborative advantage in human-AI teams appears to stem from difficulties in developing shared mental models and coordinating actions effectively, suggesting that current AI systems may lack crucial capabilities for fluid team interaction.</p>
<ul>
<li>Performance Gains and Limits: Complementarity vs.&nbsp;overreliance <span class="citation" data-cites="liangAdaptingAlgorithmHow2022">(<a href="#ref-liangAdaptingAlgorithmHow2022" role="doc-biblioref">Liang et al., 2022</a>)</span></li>
</ul>
</section>
<section id="trust-and-reliance" class="level3">
<h3 class="anchored" data-anchor-id="trust-and-reliance">Trust and Reliance</h3>
<p>Trust and confidence are crucial determinants of how individuals and groups interact with AI systems during decision-making processes. Research indicates that individuals’ confidence levels significantly influence their propensity to seek and utilize AI-generated advice. <span class="citation" data-cites="pescetelliRoleDecisionConfidence2021">Pescetelli &amp; Yeung (<a href="#ref-pescetelliRoleDecisionConfidence2021" role="doc-biblioref">2021</a>)</span> found that people are more likely to seek advice when their confidence in their own decisions is low; however, they often deviate from optimal Bayesian integration when incorporating this advice, sometimes relying on heuristic strategies instead. <span class="citation" data-cites="carlebachFlexibleUseConfidence2023">Carlebach &amp; Yeung (<a href="#ref-carlebachFlexibleUseConfidence2023" role="doc-biblioref">2023</a>)</span> further demonstrated that the relationship between confidence and advice-seeking is context-dependent. When the reliability of an AI advisor is unknown, individuals may paradoxically seek advice even when they are confident, using their own confidence as a feedback mechanism to learn about the advisor’s quality. <span class="citation" data-cites="liangAdaptingAlgorithmHow2022">Liang et al. (<a href="#ref-liangAdaptingAlgorithmHow2022" role="doc-biblioref">2022</a>)</span> explored how explicit performance comparisons between humans and AI affect reliance on decision aids, revealing that individuals adapt their use of AI recommendations based on task difficulty and perceived accuracy differences. <span class="citation" data-cites="steyversCalibrationGapModel2024">Steyvers et al. (<a href="#ref-steyversCalibrationGapModel2024" role="doc-biblioref">2024</a>)</span> demonstrated that humans tend to overestimate AI system accuracy when presented with default explanations, particularly when those explanations are lengthy. However, this miscalibration can be mitigated by explicitly communicating the AI’s uncertainty levels.</p>
<p>In group settings, trust in AI systems significantly influences how teams interact and make decisions. <span class="citation" data-cites="cuiAIenhancedCollectiveIntelligence2024">Cui &amp; Yasseri (<a href="#ref-cuiAIenhancedCollectiveIntelligence2024" role="doc-biblioref">2024</a>)</span> emphasize that trust between humans and AI is crucial for achieving collective intelligence in teams. They argue that factors such as the perceived competence, benevolence, and integrity of AI systems shape this trust, mirroring the dynamics of trust in human relationships. The level of anthropomorphism in AI agents can also affect trust; while human-like features may initially enhance trust, this effect can wane if the AI’s performance does not meet team expectations. <span class="citation" data-cites="zvelebilovaCollectiveAttentionHumanAI2024">Zvelebilova et al. (<a href="#ref-zvelebilovaCollectiveAttentionHumanAI2024" role="doc-biblioref">2024</a>)</span> further demonstrate that even when teams do not fully trust an AI assistant or consider it a genuine team member, the AI can still significantly influence team discourse and collective attention. Their study found that teams adopted terminology introduced by the AI, indicating an automatic integration of AI input despite doubts about its reliability. This suggests that AI systems can impact group cognition and coordination regardless of explicit trust levels. These findings highlight the complexity of trust in AI within team environments, where both the design of AI agents and their subtle influences on team dynamics must be carefully managed to enhance collaborative outcomes.</p>
<ul>
<li><p>Contagion Effects of (Dis)Trust: Spread of distrust or overconfidence within human–AI teams <span class="citation" data-cites="duanUnderstandingProcessesTrust2025">(<a href="#ref-duanUnderstandingProcessesTrust2025" role="doc-biblioref">Duan et al., 2025</a>)</span></p></li>
<li><p>Uncalibrated vs.&nbsp;Well-Calibrated AI: Conditions under which uncalibrated AI might paradoxically enhance outcomes <span class="citation" data-cites="vodrahalliUncalibratedModelsCan2022">(<a href="#ref-vodrahalliUncalibratedModelsCan2022" role="doc-biblioref">Vodrahalli et al., 2022</a>)</span></p></li>
</ul>
<p><strong>Satisfaction, Acceptance, and Group Dynamics.</strong> Beyond objective measures of decision quality, the subjective experiences of group members, including satisfaction, acceptance, and shifts in group dynamics, are crucial outputs in AI-assisted decision-making. User satisfaction with AI systems is significantly influenced by factors such as interface design, perceived cognitive load, and the type of AI assistance provided <span class="citation" data-cites="bucincaTrustThinkCognitive2021 rebholzConversationalUserInterfaces2024">(<a href="#ref-bucincaTrustThinkCognitive2021" role="doc-biblioref">Buçinca et al., 2021</a>; <a href="#ref-rebholzConversationalUserInterfaces2024" role="doc-biblioref">Rebholz et al., 2024</a>)</span>. For instance, while cognitive forcing functions can improve decision accuracy, they may also decrease user satisfaction due to increased perceived complexity <span class="citation" data-cites="bucincaTrustThinkCognitive2021">(<a href="#ref-bucincaTrustThinkCognitive2021" role="doc-biblioref">Buçinca et al., 2021</a>)</span>. Acceptance of AI in group settings is contingent on various factors, including the perceived usefulness of AI, trust in its recommendations, and alignment with human values <span class="citation" data-cites="narayananHowDoesValue2023">(<a href="#ref-narayananHowDoesValue2023" role="doc-biblioref">S. Narayanan et al., 2023</a>)</span>. Furthermore, AI integration can reshape group dynamics, potentially influencing group polarization and the emergence of echo chambers or polarization phenomena <span class="citation" data-cites="cheungLargeLanguageModels2024">(<a href="#ref-cheungLargeLanguageModels2024" role="doc-biblioref">Cheung et al., 2024</a>)</span>. While AI can facilitate consensus formation and bridge diverse viewpoints <span class="citation" data-cites="tesslerAICanHelp2024">(<a href="#ref-tesslerAICanHelp2024" role="doc-biblioref">Tessler et al., 2024</a>)</span> it also carries the risk of amplifying existing biases and limiting exposure to diverse perspectives <span class="citation" data-cites="cheungLargeLanguageModels2024 sharmaGenerativeEchoChamber2024">(<a href="#ref-cheungLargeLanguageModels2024" role="doc-biblioref">Cheung et al., 2024</a>; <a href="#ref-sharmaGenerativeEchoChamber2024" role="doc-biblioref">Sharma et al., 2024</a>)</span>.</p>
<p>Recent work by <span class="citation" data-cites="bucincaTrustThinkCognitive2021">Buçinca et al. (<a href="#ref-bucincaTrustThinkCognitive2021" role="doc-biblioref">2021</a>)</span> presents an innovative approach to addressing overreliance on AI systems through interface design rather than explanation quality. Their study evaluated three “cognitive forcing functions” - interface elements designed to disrupt quick, heuristic processing of AI recommendations. Although these interventions significantly reduced overreliance on incorrect AI recommendations, an important trade-off emerged: interfaces that most effectively prevented overreliance were also rated as most complex and least preferred by users. Moreover, their analysis revealed potential equity concerns, as the interventions provided substantially greater benefits to individuals with high Need for Cognition. These findings suggest that while interface design can effectively modulate AI utilization patterns, careful consideration must be given to both user experience and potential intervention-generated inequalities.</p>
</section>
<section id="responsibility" class="level3">
<h3 class="anchored" data-anchor-id="responsibility">Responsibility</h3>
<ul>
<li><span class="citation" data-cites="narayananHowDoesValue2023">(<a href="#ref-narayananHowDoesValue2023" role="doc-biblioref">S. Narayanan et al., 2023</a>)</span></li>
</ul>
<p>Recent work has begun examining how people attribute responsibility in human-AI collaborative contexts where control is shared and actions are interdependent <span class="citation" data-cites="tsirtsisComputationalModelResponsibility2024">(<a href="#ref-tsirtsisComputationalModelResponsibility2024" role="doc-biblioref">Tsirtsis et al., 2024</a>)</span>. Their study employs a stylized semi-autonomous driving simulation where participants observe how a ‘human agent’ and an ‘AI agent’ collaborate to reach a destination within a time limit. In their setup, the human and AI agents shared control of a vehicle, with each agent having partial and differing knowledge of the environment (i.e., the AI knew about traffic conditions but not road closures, while humans knew about closures but not traffic). Participants observe illustrated simulations of a variety of commute scenarios, and then make judgements about how responsible each agent was for the commute outcome (reaching the destination on time, or not). The study reveals that participants’ responsibility judgments are influenced by factors such as the unexpectedness of an agent’s action, counterfactual simulations of alternative actions, and the actual contribution of each agent to the task outcome.</p>
</section>
<section id="future-research-directions-output" class="level3">
<h3 class="anchored" data-anchor-id="future-research-directions-output">Future Research Directions (Output)</h3>
<ul>
<li>How can we develop better metrics and methods for evaluating decision quality in human-AI teams, beyond simple accuracy measures?</li>
<li>Developing methods for measuring and monitoring reliance in real-time?</li>
<li>Developing interventions that promote appropriate trust calibration</li>
</ul>
</section>
<section id="discussion" class="level3">
<h3 class="anchored" data-anchor-id="discussion">Discussion</h3>
</section>
<section id="future-directions" class="level3">
<h3 class="anchored" data-anchor-id="future-directions">Future Directions</h3>
<ul>
<li>Designing systems that can actively promote constructive dialogue, facilitate perspective-taking, and encourage the integration of diverse viewpoints in group decision-making</li>
<li>Opportunities for real-time AI moderation (Han et al., 2024)</li>
<li>Evolving norms around AI accountability (Smith et al., 2025)</li>
<li>potential negative impacts of AI on human skills, critical thinking, and agency (Gerlich, 2025; H. Kumar et al., 2024)</li>
<li>Effects of AI on group extremity and the emergence of echo chambers or polarization phenomena (van Swol et al., 2023).</li>
<li>privacy, accountability, and fairness in AI-assisted group decision making (Cui &amp; Yasseri, 2024; Barredo Arrieta et al., 2020).</li>
</ul>
</section>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-aggarwalSelfbeliefsTransactiveMemory2023" class="csl-entry" role="listitem">
Aggarwal, I., Cuconato, G., Ateş, N. Y., &amp; Meslec, N. (2023). Self-beliefs, <span>Transactive Memory Systems</span>, and <span>Collective Identification</span> in <span>Teams</span>: <span>Articulating</span> the <span>Socio-Cognitive Underpinnings</span> of <span>COHUMAIN</span>. <em>Topics in Cognitive Science</em>, 1–31. <a href="https://doi.org/10.1111/tops.12681">https://doi.org/10.1111/tops.12681</a>
</div>
<div id="ref-anderlConversationalPresentationMode2024" class="csl-entry" role="listitem">
Anderl, C., Klein, S. H., Sarigül, B., Schneider, F. M., Han, J., Fiedler, P. L., &amp; Utz, S. (2024). Conversational presentation mode increases credibility judgements during information search with <span>ChatGPT</span>. <em>Scientific Reports</em>, <em>14</em>(1), 17127. <a href="https://doi.org/10.1038/s41598-024-67829-6">https://doi.org/10.1038/s41598-024-67829-6</a>
</div>
<div id="ref-argyleLeveragingAIDemocratic2023" class="csl-entry" role="listitem">
Argyle, L. P., Bail, C. A., Busby, E. C., Gubler, J. R., Howe, T., Rytting, C., Sorensen, T., &amp; Wingate, D. (2023). Leveraging <span>AI</span> for democratic discourse: <span>Chat</span> interventions can improve online political conversations at scale. <em>Proceedings of the National Academy of Sciences</em>, <em>120</em>(41), e2311627120. <a href="https://doi.org/10.1073/pnas.2311627120">https://doi.org/10.1073/pnas.2311627120</a>
</div>
<div id="ref-ashkinazeHowAIIdeas2024" class="csl-entry" role="listitem">
Ashkinaze, J., Mendelsohn, J., Qiwei, L., Budak, C., &amp; Gilbert, E. (2024). <em>How <span>AI Ideas Affect</span> the <span>Creativity</span>, <span>Diversity</span>, and <span>Evolution</span> of <span>Human Ideas</span>: <span>Evidence From</span> a <span>Large</span>, <span>Dynamic Experiment</span></em> (arXiv:2401.13481). arXiv. <a href="https://doi.org/10.48550/arXiv.2401.13481">https://doi.org/10.48550/arXiv.2401.13481</a>
</div>
<div id="ref-banihaniAIDecisionmakingProcess2024" class="csl-entry" role="listitem">
BaniHani, I., Alawadi, S., &amp; Elmrayyan, N. (2024). <span>AI</span> and the decision-making process: A literature review in healthcare, financial, and technology sectors. <em>Journal of Decision Systems</em>, 1–11. <a href="https://doi.org/10.1080/12460125.2024.2349425">https://doi.org/10.1080/12460125.2024.2349425</a>
</div>
<div id="ref-bansalDoesWholeExceed2021" class="csl-entry" role="listitem">
Bansal, G., Wu, T., Zhou, J., Fok, R., Nushi, B., Kamar, E., Ribeiro, M. T., &amp; Weld, D. (2021). Does the <span>Whole Exceed</span> its <span>Parts</span>? <span>The Effect</span> of <span>AI Explanations</span> on <span>Complementary Team Performance</span>. <em>Proceedings of the 2021 <span>CHI Conference</span> on <span>Human Factors</span> in <span>Computing Systems</span></em>, 1–16. <a href="https://doi.org/10.1145/3411764.3445717">https://doi.org/10.1145/3411764.3445717</a>
</div>
<div id="ref-bastolaLLMbasedSmartReply2024" class="csl-entry" role="listitem">
Bastola, A., Wang, H., Hembree, J., Yadav, P., Gong, Z., Dixon, E., Razi, A., &amp; McNeese, N. (2024). <em><span class="nocase">LLM-based Smart Reply</span> (<span>LSR</span>): <span>Enhancing Collaborative Performance</span> with <span class="nocase">ChatGPT-mediated Smart Reply System</span></em> (arXiv:2306.11980). arXiv. <a href="https://arxiv.org/abs/2306.11980">https://arxiv.org/abs/2306.11980</a>
</div>
<div id="ref-beckerBoostingHumanDecisionmaking2022" class="csl-entry" role="listitem">
Becker, F., Skirzyński, J., van Opheusden, B., &amp; Lieder, F. (2022). Boosting <span class="nocase">Human Decision-making</span> with <span>AI-Generated Decision Aids</span>. <em>Computational Brain &amp; Behavior</em>, <em>5</em>(4), 467–490. <a href="https://doi.org/10.1007/s42113-022-00149-y">https://doi.org/10.1007/s42113-022-00149-y</a>
</div>
<div id="ref-bennettHumanPerformanceCompetitive2023" class="csl-entry" role="listitem">
Bennett, M. S., Hedley, L., Love, J., Houpt, J. W., Brown, S. D., &amp; Eidels, A. (2023). Human <span>Performance</span> in <span>Competitive</span> and <span>Collaborative Human</span>–<span>Machine Teams</span>. <em>Topics in Cognitive Science</em>, 1–25. <a href="https://doi.org/10.1111/tops.12683">https://doi.org/10.1111/tops.12683</a>
</div>
<div id="ref-berrettaDefiningHumanAITeaming2023" class="csl-entry" role="listitem">
Berretta, S., Tausch, A., Ontrup, G., Gilles, B., Peifer, C., &amp; Kluge, A. (2023). Defining human-<span>AI</span> teaming the human-centered way: A scoping review and network analysis. <em>Frontiers in Artificial Intelligence</em>, <em>6</em>. <a href="https://doi.org/10.3389/frai.2023.1250725">https://doi.org/10.3389/frai.2023.1250725</a>
</div>
<div id="ref-bhatiaExploringVariabilityRisk2024" class="csl-entry" role="listitem">
Bhatia, S. (2024). Exploring variability in risk taking with large language models. <em>Journal of Experimental Psychology: General</em>, <em>153</em>(7), 1838–1860. <a href="https://doi.org/10.1037/xge0001607">https://doi.org/10.1037/xge0001607</a>
</div>
<div id="ref-bienefeldHumanAITeamingLeveraging2023" class="csl-entry" role="listitem">
Bienefeld, N., Kolbe, M., Camen, G., Huser, D., &amp; Buehler, P. K. (2023). Human-<span>AI</span> teaming: Leveraging transactive memory and speaking up for enhanced team effectiveness. <em>Frontiers in Psychology</em>, <em>14</em>. <a href="https://doi.org/10.3389/fpsyg.2023.1208019">https://doi.org/10.3389/fpsyg.2023.1208019</a>
</div>
<div id="ref-bouscheryAugmentingHumanInnovation2023" class="csl-entry" role="listitem">
Bouschery, S. G., Blazevic, V., &amp; Piller, F. T. (2023). Augmenting human innovation teams with artificial intelligence: <span>Exploring</span> transformer-based language models. <em>Journal of Product Innovation Management</em>, <em>40</em>(2), 139–153. <a href="https://doi.org/10.1111/jpim.12656">https://doi.org/10.1111/jpim.12656</a>
</div>
<div id="ref-boussiouxCrowdlessFutureGenerative2024" class="csl-entry" role="listitem">
Boussioux, L., Lane, J. N., Zhang, M., Jacimovic, V., &amp; Lakhani, K. R. (2024). The <span>Crowdless Future</span>? <span>Generative AI</span> and <span>Creative Problem-Solving</span>. <em>Organization Science</em>, <em>35</em>(5), 1589–1607. <a href="https://doi.org/10.1287/orsc.2023.18430">https://doi.org/10.1287/orsc.2023.18430</a>
</div>
<div id="ref-bucincaTrustThinkCognitive2021" class="csl-entry" role="listitem">
Buçinca, Z., Malaya, M. B., &amp; Gajos, K. Z. (2021). To <span>Trust</span> or to <span>Think</span>: <span>Cognitive Forcing Functions Can Reduce Overreliance</span> on <span>AI</span> in <span class="nocase">AI-assisted Decision-making</span>. <em>Proceedings of the ACM on Human-Computer Interaction</em>, <em>5</em>(CSCW1), 1–21. <a href="https://doi.org/10.1145/3449287">https://doi.org/10.1145/3449287</a>
</div>
<div id="ref-bucincaContrastiveExplanationsThat2024" class="csl-entry" role="listitem">
Buçinca, Z., Swaroop, S., Paluch, A. E., Doshi-Velez, F., &amp; Gajos, K. Z. (2024). <em>Contrastive <span>Explanations That Anticipate Human Misconceptions Can Improve Human Decision-Making Skills</span></em> (arXiv:2410.04253). arXiv. <a href="https://arxiv.org/abs/2410.04253">https://arxiv.org/abs/2410.04253</a>
</div>
<div id="ref-burtonHowLargeLanguage2024" class="csl-entry" role="listitem">
Burton, J. W., Lopez-Lopez, E., Hechtlinger, S., Rahwan, Z., Aeschbach, S., Bakker, M. A., Becker, J. A., Berditchevskaia, A., Berger, J., Brinkmann, L., Flek, L., Herzog, S. M., Huang, S., Kapoor, S., Narayanan, A., Nussberger, A.-M., Yasseri, T., Nickl, P., Almaatouq, A., … Hertwig, R. (2024). How large language models can reshape collective intelligence. <em>Nature Human Behaviour</em>, 1–13. <a href="https://doi.org/10.1038/s41562-024-01959-9">https://doi.org/10.1038/s41562-024-01959-9</a>
</div>
<div id="ref-canonicoCollectivelyIntelligentTeams2019" class="csl-entry" role="listitem">
Canonico, L. B., Flathmann, C., &amp; McNeese, N. (2019). Collectively <span>Intelligent Teams</span>: <span>Integrating Team Cognition</span>, <span>Collective Intelligence</span>, and <span>AI</span> for <span>Future Teaming</span>. <em>Proceedings of the Human Factors and Ergonomics Society Annual Meeting</em>, <em>63</em>(1), 1466–1470. <a href="https://doi.org/10.1177/1071181319631278">https://doi.org/10.1177/1071181319631278</a>
</div>
<div id="ref-carlebachFlexibleUseConfidence2023" class="csl-entry" role="listitem">
Carlebach, N., &amp; Yeung, N. (2023). Flexible use of confidence to guide advice requests. <em>Cognition</em>, <em>230</em>, 105264. <a href="https://doi.org/10.1016/j.cognition.2022.105264">https://doi.org/10.1016/j.cognition.2022.105264</a>
</div>
<div id="ref-carterIntegratingArtificialIntelligence2024" class="csl-entry" role="listitem">
Carter, W., &amp; Wynne, K. T. (2024). Integrating artificial intelligence into team decision-making: <span>Toward</span> a theory of <span>AI</span>–human team effectiveness. <em>European Management Review</em>. <a href="https://doi.org/10.1111/emre.12685">https://doi.org/10.1111/emre.12685</a>
</div>
<div id="ref-cecilExplainabilityDoesNot2024" class="csl-entry" role="listitem">
Cecil, J., Lermer, E., Hudecek, M. F. C., Sauer, J., &amp; Gaube, S. (2024). Explainability does not mitigate the negative impact of incorrect <span>AI</span> advice in a personnel selection task. <em>Scientific Reports</em>, <em>14</em>(1), 9736. <a href="https://doi.org/10.1038/s41598-024-60220-5">https://doi.org/10.1038/s41598-024-60220-5</a>
</div>
<div id="ref-cheungLargeLanguageModels2024" class="csl-entry" role="listitem">
Cheung, V., Maier, M., &amp; Lieder, F. (2024). <em>Large <span>Language Models Amplify Human Biases</span> in <span>Moral Decision-Making</span></em>. <a href="https://doi.org/10.31234/osf.io/aj46b">https://doi.org/10.31234/osf.io/aj46b</a>
</div>
<div id="ref-chiangEnhancingAIAssistedGroup2024" class="csl-entry" role="listitem">
Chiang, C.-W., Lu, Z., Li, Z., &amp; Yin, M. (2024). Enhancing <span>AI-Assisted Group Decision Making</span> through <span>LLM-Powered Devil</span>’s <span>Advocate</span>. <em>Proceedings of the 29th <span>International Conference</span> on <span>Intelligent User Interfaces</span></em>, 103–119. <a href="https://doi.org/10.1145/3640543.3645199">https://doi.org/10.1145/3640543.3645199</a>
</div>
<div id="ref-chuangWisdomPartisanCrowds2024" class="csl-entry" role="listitem">
Chuang, Y.-S., Harlalka, N., Suresh, S., Goyal, A., Hawkins, R., Yang, S., Shah, D., Hu, J., &amp; Rogers, T. T. (2024). <em>The <span>Wisdom</span> of <span>Partisan Crowds</span>: <span>Comparing Collective Intelligence</span> in <span>Humans</span> and <span class="nocase">LLM-based Agents</span></em>.
</div>
<div id="ref-chugunovaWeItInterdisciplinary2022" class="csl-entry" role="listitem">
Chugunova, M., &amp; Sele, D. (2022). We and <span>It</span>: <span>An</span> interdisciplinary review of the experimental evidence on how humans interact with machines. <em>Journal of Behavioral and Experimental Economics</em>, <em>99</em>, 101897. <a href="https://doi.org/10.1016/j.socec.2022.101897">https://doi.org/10.1016/j.socec.2022.101897</a>
</div>
<div id="ref-collinsBuildingMachinesThat2024" class="csl-entry" role="listitem">
Collins, K. M., Sucholutsky, I., Bhatt, U., Chandra, K., Wong, L., Lee, M., Zhang, C. E., Zhi-Xuan, T., Ho, M., Mansinghka, V., Weller, A., Tenenbaum, J. B., &amp; Griffiths, T. L. (2024). <em>Building <span>Machines</span> that <span>Learn</span> and <span>Think</span> with <span>People</span></em> (arXiv:2408.03943). arXiv. <a href="https://arxiv.org/abs/2408.03943">https://arxiv.org/abs/2408.03943</a>
</div>
<div id="ref-cuiAIenhancedCollectiveIntelligence2024" class="csl-entry" role="listitem">
Cui, H., &amp; Yasseri, T. (2024). <span class="nocase">AI-enhanced</span> collective intelligence. <em>Patterns</em>, <em>5</em>(11), 101074. <a href="https://doi.org/10.1016/j.patter.2024.101074">https://doi.org/10.1016/j.patter.2024.101074</a>
</div>
<div id="ref-diebelWhenAIBasedAgents2025" class="csl-entry" role="listitem">
Diebel, C., Goutier, M., Adam, M., &amp; Benlian, A. (2025). When <span>AI-Based Agents Are Proactive</span>: <span>Implications</span> for <span>Competence</span> and <span>System Satisfaction</span> in <span>Human</span>–<span>AI Collaboration</span>. <em>Business &amp; Information Systems Engineering</em>, 1–20. <a href="https://doi.org/10.1007/s12599-024-00918-y">https://doi.org/10.1007/s12599-024-00918-y</a>
</div>
<div id="ref-duLargeLanguageModels2024" class="csl-entry" role="listitem">
Du, Y., Rajivan, P., &amp; Gonzalez, C. C. (2024). Large <span>Language Models</span> for <span>Collective Problem-Solving</span>: <span>Insights</span> into <span>Group Consensus Decision-Making</span>. <em>Proceedings of the <span>Annual Meeting</span> of the <span>Cognitive Science Society</span></em>, <em>46</em>.
</div>
<div id="ref-duanUnderstandingProcessesTrust2025" class="csl-entry" role="listitem">
Duan, W., Zhou, S., Scalia, M. J., Freeman, G., Gorman, J., Tolston, M., McNeese, N. J., &amp; Funke, G. (2025). Understanding the processes of trust and distrust contagion in <span>Human-AI Teams</span>: <span>A</span> qualitative approach. <em>Computers in Human Behavior</em>, 108560. <a href="https://doi.org/10.1016/j.chb.2025.108560">https://doi.org/10.1016/j.chb.2025.108560</a>
</div>
<div id="ref-eignerDeterminantsLLMassistedDecisionMaking2024" class="csl-entry" role="listitem">
Eigner, E., &amp; Händler, T. (2024). <em>Determinants of <span class="nocase">LLM-assisted Decision-Making</span></em> (arXiv:2402.17385). arXiv. <a href="https://arxiv.org/abs/2402.17385">https://arxiv.org/abs/2402.17385</a>
</div>
<div id="ref-floresInformationForagingHumanChatGPT2024" class="csl-entry" role="listitem">
Flores, P., Rong, G., &amp; Cowley, B. (2024). Information foraging in human-<span>ChatGPT</span> interactions: Factors of computational thinking dissociate exploration and exploitation. <em>Proceedings of the <span>Annual Meeting</span> of the <span>Cognitive Science Society</span></em>, <em>46</em>.
</div>
<div id="ref-fortunatiMovingAheadHumanMachine2021" class="csl-entry" role="listitem">
Fortunati, L., &amp; Edwards, A. (2021). Moving <span>Ahead With Human-Machine Communication</span>. <em>Human-Machine Communication</em>, <em>2</em>, 7–28. <a href="https://doi.org/10.30658/hmc.2.1">https://doi.org/10.30658/hmc.2.1</a>
</div>
<div id="ref-gaoAligningLLMAgents2024" class="csl-entry" role="listitem">
Gao, G., Taymanov, A., Salinas, E., Mineiro, P., &amp; Misra, D. (2024). <em>Aligning <span>LLM Agents</span> by <span>Learning Latent Preference</span> from <span>User Edits</span></em> (arXiv:2404.15269). arXiv. <a href="https://arxiv.org/abs/2404.15269">https://arxiv.org/abs/2404.15269</a>
</div>
<div id="ref-gerlichAIToolsSociety2025" class="csl-entry" role="listitem">
Gerlich, M. (2025). <span>AI Tools</span> in <span>Society</span>: <span>Impacts</span> on <span>Cognitive Offloading</span> and the <span>Future</span> of <span>Critical Thinking</span>. <em>Societies</em>, <em>15</em>(1), 6. <a href="https://doi.org/10.3390/soc15010006">https://doi.org/10.3390/soc15010006</a>
</div>
<div id="ref-gomezcaballeroHumanAICollaborationNot2024" class="csl-entry" role="listitem">
Gomez Caballero, C., Cho, S. M., Ke, S., Huang, C.-M., &amp; Unberath, M. (2024). Human-<span>AI</span> collaboration is not very collaborative yet: <span>A</span> taxonomy of interaction patterns in <span class="nocase">AI-assisted</span> decision making from a systematic review. <em>Frontiers in Computer Science</em>, <em>6</em>. <a href="https://doi.org/10.3389/fcomp.2024.1521066">https://doi.org/10.3389/fcomp.2024.1521066</a>
</div>
<div id="ref-guoDecisionTheoreticFramework2024" class="csl-entry" role="listitem">
Guo, Z., Wu, Y., Hartline, J. D., &amp; Hullman, J. (2024). A <span>Decision Theoretic Framework</span> for <span>Measuring AI Reliance</span>. <em>The 2024 <span>ACM Conference</span> on <span>Fairness</span>, <span>Accountability</span>, and <span>Transparency</span></em>, 221–236. <a href="https://doi.org/10.1145/3630106.3658901">https://doi.org/10.1145/3630106.3658901</a>
</div>
<div id="ref-hamadaWisdomCrowdsCollective2020" class="csl-entry" role="listitem">
Hamada, D., Nakayama, M., &amp; Saiki, J. (2020). Wisdom of crowds and collective decision-making in a survival situation with complex information integration. <em>Cognitive Research: Principles and Implications</em>, <em>5</em>(1), 48. <a href="https://doi.org/10.1186/s41235-020-00248-z">https://doi.org/10.1186/s41235-020-00248-z</a>
</div>
<div id="ref-hinszEmergingConceptualizationGroups1997" class="csl-entry" role="listitem">
Hinsz, V. B., Tindale, R. S., &amp; Vollrath, D. A. (1997). The emerging conceptualization of groups as information processors. <em>Psychological Bulletin</em>, <em>121</em>(1), 43–64. <a href="https://doi.org/10.1037/0033-2909.121.1.43">https://doi.org/10.1037/0033-2909.121.1.43</a>
</div>
<div id="ref-joostenComparingIdeationQuality2024" class="csl-entry" role="listitem">
Joosten, J., Bilgram, V., Hahn, A., &amp; Totzek, D. (2024). Comparing the <span>Ideation Quality</span> of <span>Humans With Generative Artificial Intelligence</span>. <em>IEEE Engineering Management Review</em>, <em>52</em>(2), 153–164. <a href="https://doi.org/10.1109/EMR.2024.3353338">https://doi.org/10.1109/EMR.2024.3353338</a>
</div>
<div id="ref-kumarHumanCreativityAge2024" class="csl-entry" role="listitem">
Kumar, H., Vincentius, J., Jordan, E., &amp; Anderson, A. (2024). <em>Human <span>Creativity</span> in the <span>Age</span> of <span>LLMs</span>: <span>Randomized Experiments</span> on <span>Divergent</span> and <span>Convergent Thinking</span></em> (arXiv:2410.03703). arXiv. <a href="https://arxiv.org/abs/2410.03703">https://arxiv.org/abs/2410.03703</a>
</div>
<div id="ref-lemusEmpiricalInvestigationReliance2022" class="csl-entry" role="listitem">
Lemus, H. T., Kumar, A., &amp; Steyvers, M. (2022). <em>An <span>Empirical Investigation</span> of <span>Reliance</span> on <span>AI-Assistance</span> in a <span>Noisy-Image Classification Task</span></em>. 13.
</div>
<div id="ref-liConfidenceAlignsExploring2025" class="csl-entry" role="listitem">
Li, J., Yang, Y., Liao, Q. V., Zhang, J., &amp; Lee, Y.-C. (2025). <em>As <span>Confidence Aligns</span>: <span>Exploring</span> the <span>Effect</span> of <span>AI Confidence</span> on <span class="nocase">Human Self-confidence</span> in <span>Human-AI Decision Making</span></em> (arXiv:2501.12868). arXiv. <a href="https://doi.org/10.48550/arXiv.2501.12868">https://doi.org/10.48550/arXiv.2501.12868</a>
</div>
<div id="ref-liangAdaptingAlgorithmHow2022" class="csl-entry" role="listitem">
Liang, G., Sloane, J. F., Donkin, C., &amp; Newell, B. R. (2022). Adapting to the algorithm: How accuracy comparisons promote the use of a decision aid. <em>Cognitive Research: Principles and Implications</em>, <em>7</em>(1), 14. <a href="https://doi.org/10.1186/s41235-022-00364-y">https://doi.org/10.1186/s41235-022-00364-y</a>
</div>
<div id="ref-maHumanAIDeliberationDesign2024" class="csl-entry" role="listitem">
Ma, S., Chen, Q., Wang, X., Zheng, C., Peng, Z., Yin, M., &amp; Ma, X. (2024). <em>Towards <span>Human-AI Deliberation</span>: <span>Design</span> and <span>Evaluation</span> of <span>LLM-Empowered Deliberative AI</span> for <span>AI-Assisted Decision-Making</span></em> (arXiv:2403.16812). arXiv. <a href="https://arxiv.org/abs/2403.16812">https://arxiv.org/abs/2403.16812</a>
</div>
<div id="ref-marjiehTaskAllocationTeams2024" class="csl-entry" role="listitem">
Marjieh, R., Gokhale, A., Bullo, F., &amp; Griffiths, T. L. (2024). <em>Task <span>Allocation</span> in <span>Teams</span> as a <span>Multi-Armed Bandit</span></em>.
</div>
<div id="ref-marjiehLargeLanguageModels2024" class="csl-entry" role="listitem">
Marjieh, R., Sucholutsky, I., van Rijn, P., Jacoby, N., &amp; Griffiths, T. L. (2024). Large language models predict human sensory judgments across six modalities. <em>Scientific Reports</em>, <em>14</em>(1), 21445. <a href="https://doi.org/10.1038/s41598-024-72071-1">https://doi.org/10.1038/s41598-024-72071-1</a>
</div>
<div id="ref-mcneeseSteppingOutShadow2023" class="csl-entry" role="listitem">
McNeese, N. J., Flathmann, C., O’Neill, T. A., &amp; Salas, E. (2023). Stepping out of the shadow of human-human teaming: <span>Crafting</span> a unique identity for human-autonomy teams. <em>Computers in Human Behavior</em>, <em>148</em>, 107874. <a href="https://doi.org/10.1016/j.chb.2023.107874">https://doi.org/10.1016/j.chb.2023.107874</a>
</div>
<div id="ref-meinckeUsingLargeLanguage2024" class="csl-entry" role="listitem">
Meincke, L., Girotra, K., Nave, G., Terwiesch, C., &amp; Ulrich, K. T. (2024). <em>Using <span>Large Language Models</span> for <span>Idea Generation</span> in <span>Innovation</span></em>. <a href="https://doi.org/10.2139/ssrn.4526071">https://doi.org/10.2139/ssrn.4526071</a>
</div>
<div id="ref-narayananInfluenceHumanAITeam2024" class="csl-entry" role="listitem">
Narayanan, R., &amp; Feigh, K. (2024). <em>Influence of <span>Human-AI Team Structuring</span> on <span>Shared Mental Models</span> for <span>Collaborative Decision Making</span></em>.
</div>
<div id="ref-narayananHowDoesValue2023" class="csl-entry" role="listitem">
Narayanan, S., Yu, G., Ho, C.-J., &amp; Yin, M. (2023). How does <span>Value Similarity</span> affect <span>Human Reliance</span> in <span>AI-Assisted Ethical Decision Making</span>? <em>Proceedings of the 2023 <span>AAAI</span>/<span>ACM Conference</span> on <span>AI</span>, <span>Ethics</span>, and <span>Society</span></em>, 49–57. <a href="https://doi.org/10.1145/3600211.3604709">https://doi.org/10.1145/3600211.3604709</a>
</div>
<div id="ref-nishidaConversationalAgentDynamics2024" class="csl-entry" role="listitem">
Nishida, Y., Shimojo, S., &amp; Hayashi, Y. (2024). Conversational <span>Agent Dynamics</span> with <span>Minority Opinion</span> and <span>Cognitive Conflict</span> in <span>Small-Group Decision-Making</span>. <em>Japanese Psychological Research</em>. <a href="https://doi.org/10.1111/jpr.12552">https://doi.org/10.1111/jpr.12552</a>
</div>
<div id="ref-nomuraCollaborativeBrainstormingHumans2024" class="csl-entry" role="listitem">
Nomura, M., Ito, T., &amp; Ding, S. (2024). Towards <span class="nocase">Collaborative Brain-storming</span> among <span>Humans</span> and <span>AI Agents</span>: <span>An Implementation</span> of the <span class="nocase">IBIS-based Brainstorming Support System</span> with <span>Multiple AI Agents</span>. <em>Proceedings of the <span>ACM Collective Intelligence Conference</span></em>, 1–9. <a href="https://doi.org/10.1145/3643562.3672609">https://doi.org/10.1145/3643562.3672609</a>
</div>
<div id="ref-pescetelliRoleDecisionConfidence2021" class="csl-entry" role="listitem">
Pescetelli, N., &amp; Yeung, N. (2021). The role of decision confidence in advice-taking and trust formation. <em>Journal of Experimental Psychology: General</em>, <em>150</em>(3), 507–526. <a href="https://doi.org/10.1037/xge0000960">https://doi.org/10.1037/xge0000960</a>
</div>
<div id="ref-radivojevicLLMsUsGenerative2024" class="csl-entry" role="listitem">
Radivojevic, K., Clark, N., &amp; Brenner, P. (2024). <span>LLMs Among Us</span>: <span>Generative AI Participating</span> in <span>Digital Discourse</span>. <em>Proceedings of the AAAI Symposium Series</em>, <em>3</em>(1), 209–218. <a href="https://doi.org/10.1609/aaaiss.v3i1.31202">https://doi.org/10.1609/aaaiss.v3i1.31202</a>
</div>
<div id="ref-rastogiTaxonomyHumanML2023" class="csl-entry" role="listitem">
Rastogi, C., Leqi, L., Holstein, K., &amp; Heidari, H. (2023). A <span>Taxonomy</span> of <span>Human</span> and <span>ML Strengths</span> in <span>Decision-Making</span> to <span>Investigate Human-ML Complementarity</span>. <em>Proceedings of the AAAI Conference on Human Computation and Crowdsourcing</em>, <em>11</em>, 127–139. <a href="https://doi.org/10.1609/hcomp.v11i1.27554">https://doi.org/10.1609/hcomp.v11i1.27554</a>
</div>
<div id="ref-rebholzConversationalUserInterfaces2024" class="csl-entry" role="listitem">
Rebholz, T. R., Koop, A., &amp; Hütter, M. (2024). Conversational <span>User Interfaces</span>: <span>Explanations</span> and <span>Interactivity Positively Influence Advice Taking</span> from <span>Generative Artificial Intelligence</span>. <em>Technology, Mind, and Behavior</em>. <a href="https://doi.org/10.1037/tmb0000136">https://doi.org/10.1037/tmb0000136</a>
</div>
<div id="ref-roeslerNumericVsVerbal2024" class="csl-entry" role="listitem">
Roesler, E., Rieger, T., &amp; Langer, M. (2024). Numeric vs. Verbal information: <span>The</span> influence of information quantifiability in <span>Human-AI</span> vs. <span>Human-Human</span> decision support. <em>Computers in Human Behavior: Artificial Humans</em>, 100116. <a href="https://doi.org/10.1016/j.chbah.2024.100116">https://doi.org/10.1016/j.chbah.2024.100116</a>
</div>
<div id="ref-sharmaGenerativeEchoChamber2024" class="csl-entry" role="listitem">
Sharma, N., Liao, Q. V., &amp; Xiao, Z. (2024). Generative <span>Echo Chamber</span>? <span>Effect</span> of <span>LLM-Powered Search Systems</span> on <span>Diverse Information Seeking</span>. <em>Proceedings of the <span>CHI Conference</span> on <span>Human Factors</span> in <span>Computing Systems</span></em>, 1–17. <a href="https://doi.org/10.1145/3613904.3642459">https://doi.org/10.1145/3613904.3642459</a>
</div>
<div id="ref-shinSuperhumanArtificialIntelligence2023" class="csl-entry" role="listitem">
Shin, M., Kim, J., van Opheusden, B., &amp; Griffiths, T. L. (2023). Superhuman artificial intelligence can improve human decision-making by increasing novelty. <em>Proceedings of the National Academy of Sciences</em>, <em>120</em>(12), e2214840120. <a href="https://doi.org/10.1073/pnas.2214840120">https://doi.org/10.1073/pnas.2214840120</a>
</div>
<div id="ref-siCanLLMsGenerate2024" class="csl-entry" role="listitem">
Si, C., Yang, D., &amp; Hashimoto, T. (2024). <em>Can <span>LLMs Generate Novel Research Ideas</span>? <span>A Large-Scale Human Study</span> with 100+ <span>NLP Researchers</span></em> (arXiv:2409.04109). arXiv. <a href="https://doi.org/10.48550/arXiv.2409.04109">https://doi.org/10.48550/arXiv.2409.04109</a>
</div>
<div id="ref-sidjiHumanAICollaborationCooperative2024" class="csl-entry" role="listitem">
Sidji, M., Smith, W., &amp; Rogerson, M. J. (2024). Human-<span>AI Collaboration</span> in <span>Cooperative Games</span>: <span>A Study</span> of <span>Playing Codenames</span> with an <span>LLM Assistant</span>. <em>Proc. ACM Hum.-Comput. Interact.</em>, <em>8</em>(CHI PLAY), 316:1–316:25. <a href="https://doi.org/10.1145/3677081">https://doi.org/10.1145/3677081</a>
</div>
<div id="ref-spathariotiComparingTraditionalLLMbased2023" class="csl-entry" role="listitem">
Spatharioti, S. E., Rothschild, D. M., Goldstein, D. G., &amp; Hofman, J. M. (2023). <em>Comparing <span>Traditional</span> and <span class="nocase">LLM-based Search</span> for <span>Consumer Choice</span>: <span>A Randomized Experiment</span></em> (arXiv:2307.03744). arXiv. <a href="https://doi.org/10.48550/arXiv.2307.03744">https://doi.org/10.48550/arXiv.2307.03744</a>
</div>
<div id="ref-stadlerCognitiveEaseCost2024" class="csl-entry" role="listitem">
Stadler, M., Bannert, M., &amp; Sailer, M. (2024). Cognitive ease at a cost: <span>LLMs</span> reduce mental effort but compromise depth in student scientific inquiry. <em>Computers in Human Behavior</em>, <em>160</em>, 108386. <a href="https://doi.org/10.1016/j.chb.2024.108386">https://doi.org/10.1016/j.chb.2024.108386</a>
</div>
<div id="ref-steyversBayesianModelingHuman2022" class="csl-entry" role="listitem">
Steyvers, M., Tejeda, H., Kerrigan, G., &amp; Smyth, P. (2022). Bayesian modeling of human–<span>AI</span> complementarity. <em>Proceedings of the National Academy of Sciences</em>, <em>119</em>(11), e2111547119. <a href="https://doi.org/10.1073/pnas.2111547119">https://doi.org/10.1073/pnas.2111547119</a>
</div>
<div id="ref-steyversCalibrationGapModel2024" class="csl-entry" role="listitem">
Steyvers, M., Tejeda, H., Kumar, A., Belem, C., Karny, S., Hu, X., Mayer, L., &amp; Smyth, P. (2024). <em>The <span>Calibration Gap</span> between <span>Model</span> and <span>Human Confidence</span> in <span>Large Language Models</span></em> (arXiv:2401.13835). arXiv. <a href="https://arxiv.org/abs/2401.13835">https://arxiv.org/abs/2401.13835</a>
</div>
<div id="ref-swaroopAccuracyTimeTradeoffsAIAssisted2024" class="csl-entry" role="listitem">
Swaroop, S., Buçinca, Z., Gajos, K. Z., &amp; Doshi-Velez, F. (2024). Accuracy-<span>Time Tradeoffs</span> in <span>AI-Assisted Decision Making</span> under <span>Time Pressure</span>. <em>Proceedings of the 29th <span>International Conference</span> on <span>Intelligent User Interfaces</span></em>, 138–154. <a href="https://doi.org/10.1145/3640543.3645206">https://doi.org/10.1145/3640543.3645206</a>
</div>
<div id="ref-tesslerAICanHelp2024" class="csl-entry" role="listitem">
Tessler, M. H., Bakker, M. A., Jarrett, D., Sheahan, H., Chadwick, M. J., Koster, R., Evans, G., Campbell-Gillingham, L., Collins, T., Parkes, D. C., Botvinick, M., &amp; Summerfield, C. (2024). <span>AI</span> can help humans find common ground in democratic deliberation. <em>Science</em>, <em>386</em>(6719), eadq2852. <a href="https://doi.org/10.1126/science.adq2852">https://doi.org/10.1126/science.adq2852</a>
</div>
<div id="ref-tsirtsisComputationalModelResponsibility2024" class="csl-entry" role="listitem">
Tsirtsis, S., Rodriguez, M. G., &amp; Gerstenberg, T. (2024). <em>Towards a computational model of responsibility judgments in sequential human-<span>AI</span> collaboration</em>. <a href="https://doi.org/10.31234/osf.io/m4yad">https://doi.org/10.31234/osf.io/m4yad</a>
</div>
<div id="ref-ueshimaDiscoveringNovelSocial2024" class="csl-entry" role="listitem">
Ueshima, A., &amp; Takikawa, H. (2024). Discovering <span>Novel Social Preferences Using Simple Artificial Neural Networks</span>. <em>Collabra: Psychology</em>, <em>10</em>(1), 121234. <a href="https://doi.org/10.1525/collabra.121234">https://doi.org/10.1525/collabra.121234</a>
</div>
<div id="ref-vaccaroWhenCombinationsHumans2024" class="csl-entry" role="listitem">
Vaccaro, M., Almaatouq, A., &amp; Malone, T. (2024). When combinations of humans and <span>AI</span> are useful: <span>A</span> systematic review and meta-analysis. <em>Nature Human Behaviour</em>, 1–11. <a href="https://doi.org/10.1038/s41562-024-02024-1">https://doi.org/10.1038/s41562-024-02024-1</a>
</div>
<div id="ref-vodrahalliUncalibratedModelsCan2022" class="csl-entry" role="listitem">
Vodrahalli, K., Gerstenberg, T., &amp; Zou, J. (2022). Uncalibrated <span>Models Can Improve Human-AI Collaboration</span>. <em>Advances in <span>Neural Information Processing Systems</span></em>, <em>35</em>, 4004–4016.
</div>
<div id="ref-wallrichRelationshipTeamDiversity2024" class="csl-entry" role="listitem">
Wallrich, L., Opara, V., Wesołowska, M., Barnoth, D., &amp; Yousefi, S. (2024). The <span>Relationship Between Team Diversity</span> and <span>Team Performance</span>: <span>Reconciling Promise</span> and <span>Reality Through</span> a <span>Comprehensive Meta-Analysis Registered Report</span>. <em>Journal of Business and Psychology</em>, <em>39</em>(6), 1303–1354. <a href="https://doi.org/10.1007/s10869-024-09977-0">https://doi.org/10.1007/s10869-024-09977-0</a>
</div>
<div id="ref-wangSocialRAGRetrievingGroup2024" class="csl-entry" role="listitem">
Wang, R., Zhou, X., Qiu, L., Chang, J. C., Bragg, J., &amp; Zhang, A. X. (2024). <em>Social-<span>RAG</span>: <span>Retrieving</span> from <span>Group Interactions</span> to <span>Socially Ground Proactive AI Generation</span> to <span>Group Preferences</span></em> (arXiv:2411.02353). arXiv. <a href="https://doi.org/10.48550/arXiv.2411.02353">https://doi.org/10.48550/arXiv.2411.02353</a>
</div>
<div id="ref-wegnerTransactiveMemoryContemporary1987" class="csl-entry" role="listitem">
Wegner, D. M. (1987). Transactive <span>Memory</span>: <span>A Contemporary Analysis</span> of the <span>Group Mind</span>. In B. Mullen &amp; G. R. Goethals (Eds.), <em>Theories of <span>Group Behavior</span></em> (pp. 185–208). Springer. <a href="https://doi.org/10.1007/978-1-4612-4634-3_9">https://doi.org/10.1007/978-1-4612-4634-3_9</a>
</div>
<div id="ref-westbyCollectiveIntelligenceHumanAI2023" class="csl-entry" role="listitem">
Westby, S., &amp; Riedl, C. (2023). Collective <span>Intelligence</span> in <span>Human-AI Teams</span>: <span>A Bayesian Theory</span> of <span>Mind Approach</span>. <em>Proceedings of the AAAI Conference on Artificial Intelligence</em>, <em>37</em>(5), 6119–6127. <a href="https://doi.org/10.1609/aaai.v37i5.25755">https://doi.org/10.1609/aaai.v37i5.25755</a>
</div>
<div id="ref-westphalDecisionControlExplanations2023" class="csl-entry" role="listitem">
Westphal, M., Vössing, M., Satzger, G., Yom-Tov, G. B., &amp; Rafaeli, A. (2023). Decision control and explanations in human-<span>AI</span> collaboration: <span>Improving</span> user perceptions and compliance. <em>Computers in Human Behavior</em>, <em>144</em>, 107714. <a href="https://doi.org/10.1016/j.chb.2023.107714">https://doi.org/10.1016/j.chb.2023.107714</a>
</div>
<div id="ref-yanCommunicationTransactiveMemory2021" class="csl-entry" role="listitem">
Yan, B., Hollingshead, A. B., Alexander, K. S., Cruz, I., &amp; Shaikh, S. J. (2021). Communication in <span>Transactive Memory Systems</span>: <span>A Review</span> and <span>Multidimensional Network Perspective</span>. <em>Small Group Research</em>, <em>52</em>(1), 3–32. <a href="https://doi.org/10.1177/1046496420967764">https://doi.org/10.1177/1046496420967764</a>
</div>
<div id="ref-yangTalk2CareLLMbasedVoice2024" class="csl-entry" role="listitem">
Yang, Z., Xu, X., Yao, B., Rogers, E., Zhang, S., Intille, S., Shara, N., Gao, G. G., &amp; Wang, D. (2024). <span>Talk2Care</span>: <span class="nocase">An LLM-based Voice Assistant</span> for <span>Communication</span> between <span>Healthcare Providers</span> and <span>Older Adults</span>. <em>Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</em>, <em>8</em>(2), 1–35. <a href="https://doi.org/10.1145/3659625">https://doi.org/10.1145/3659625</a>
</div>
<div id="ref-yenSearchGenExploring2024" class="csl-entry" role="listitem">
Yen, R., Sultanum, N., &amp; Zhao, J. (2024). To <span>Search</span> or <span>To Gen</span>? <span>Exploring</span> the <span>Synergy</span> between <span>Generative AI</span> and <span>Web Search</span> in <span>Programming</span>. <em>Extended <span>Abstracts</span> of the <span>CHI Conference</span> on <span>Human Factors</span> in <span>Computing Systems</span></em>, 1–8. <a href="https://doi.org/10.1145/3613905.3650867">https://doi.org/10.1145/3613905.3650867</a>
</div>
<div id="ref-yinUnderstandingEffectAccuracy2019" class="csl-entry" role="listitem">
Yin, M., Wortman Vaughan, J., &amp; Wallach, H. (2019). Understanding the <span>Effect</span> of <span>Accuracy</span> on <span>Trust</span> in <span>Machine Learning Models</span>. <em>Proceedings of the 2019 <span>CHI Conference</span> on <span>Human Factors</span> in <span>Computing Systems</span></em>, 1–12. <a href="https://doi.org/10.1145/3290605.3300509">https://doi.org/10.1145/3290605.3300509</a>
</div>
<div id="ref-zhangCounteractsTestingStereotypical2023" class="csl-entry" role="listitem">
Zhang, D., Rayz, J., &amp; Pradhan, R. (2023). <em>Counteracts: <span>Testing Stereotypical Representation</span> in <span class="nocase">Pre-trained Language Models</span></em> (arXiv:2301.04347). arXiv. <a href="https://arxiv.org/abs/2301.04347">https://arxiv.org/abs/2301.04347</a>
</div>
<div id="ref-zhangYouCompleteMe2022" class="csl-entry" role="listitem">
Zhang, Q., Lee, M. L., &amp; Carter, S. (2022). You <span>Complete Me</span>: <span>Human-AI Teams</span> and <span>Complementary Expertise</span>. <em><span>CHI Conference</span> on <span>Human Factors</span> in <span>Computing Systems</span></em>, 1–28. <a href="https://doi.org/10.1145/3491102.3517791">https://doi.org/10.1145/3491102.3517791</a>
</div>
<div id="ref-zhangInvestigatingAITeammate2023" class="csl-entry" role="listitem">
Zhang, R., Duan, W., Flathmann, C., McNeese, N., Freeman, G., &amp; Williams, A. (2023). Investigating <span>AI Teammate Communication Strategies</span> and <span>Their Impact</span> in <span>Human-AI Teams</span> for <span>Effective Teamwork</span>. <em>Proceedings of the ACM on Human-Computer Interaction</em>, <em>7</em>(CSCW2), 1–31. <a href="https://doi.org/10.1145/3610072">https://doi.org/10.1145/3610072</a>
</div>
<div id="ref-zhangEffectConfidenceExplanation2020" class="csl-entry" role="listitem">
Zhang, Y., Liao, Q. V., &amp; Bellamy, R. K. E. (2020). Effect of confidence and explanation on accuracy and trust calibration in <span class="nocase">AI-assisted</span> decision making. <em>Proceedings of the 2020 <span>Conference</span> on <span>Fairness</span>, <span>Accountability</span>, and <span>Transparency</span></em>, 295–305. <a href="https://doi.org/10.1145/3351095.3372852">https://doi.org/10.1145/3351095.3372852</a>
</div>
<div id="ref-zhengDiscipLinkUnfoldingInterdisciplinary2024" class="csl-entry" role="listitem">
Zheng, C., Zhang, Y., Huang, Z., Shi, C., Xu, M., &amp; Ma, X. (2024). <span>DiscipLink</span>: <span>Unfolding Interdisciplinary Information Seeking Process</span> via <span>Human-AI Co-Exploration</span>. <em>Proceedings of the 37th <span>Annual ACM Symposium</span> on <span>User Interface Software</span> and <span>Technology</span></em>, 1–20. <a href="https://doi.org/10.1145/3654777.3676366">https://doi.org/10.1145/3654777.3676366</a>
</div>
<div id="ref-zollerHumanAICollectivesProduce2024" class="csl-entry" role="listitem">
Zöller, N., Berger, J., Lin, I., Fu, N., Komarneni, J., Barabucci, G., Laskowski, K., Shia, V., Harack, B., Chu, E. A., Trianni, V., Kurvers, R. H. J. M., &amp; Herzog, S. M. (2024). <em>Human-<span>AI</span> collectives produce the most accurate differential diagnoses</em> (arXiv:2406.14981). arXiv. <a href="https://arxiv.org/abs/2406.14981">https://arxiv.org/abs/2406.14981</a>
</div>
<div id="ref-zvelebilovaCollectiveAttentionHumanAI2024" class="csl-entry" role="listitem">
Zvelebilova, J., Savage, S., &amp; Riedl, C. (2024). <em>Collective <span>Attention</span> in <span>Human-AI Teams</span></em> (arXiv:2407.17489). arXiv. <a href="https://doi.org/10.48550/arXiv.2407.17489">https://doi.org/10.48550/arXiv.2407.17489</a>
</div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/tegorman13\.github\.io\/ai_gd_chapter\/working\.html");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/tegorman13/ai_gd_chapter/blob/main/working.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/tegorman13/ai_gd_chapter/edit/main/working.qmd" class="toc-action"><i class="bi empty"></i>Edit this page</a></li><li><a href="https://github.com/tegorman13/ai_gd_chapter/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div></div></footer><script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>